2025-01-26 01:10:34,129 Iteration 1
2025-01-26 02:50:27,335 (Self-play) Number of new training examples: 97840
2025-01-26 02:50:27,336 (Self-play) Number of total training examples: 97840
2025-01-26 02:50:27,336 (Self-play) Number of total unique state-reward pairs: 69830
2025-01-26 02:50:27,336 (Self-play) Model 1 wins: 1324. Draws: 73. Model 2 wins: 1103
2025-01-26 02:51:20,116 (Training) Epoch: 1. Total loss: 1.967. Value loss: 0.065. Policy accuracy: 0.178
2025-01-26 02:51:51,607 (Training) Epoch: 2. Total loss: 1.885. Value loss: 0.050. Policy accuracy: 0.227
2025-01-26 02:52:25,654 (Training) Epoch: 3. Total loss: 1.847. Value loss: 0.047. Policy accuracy: 0.258
2025-01-26 02:52:59,188 (Training) Epoch: 4. Total loss: 1.817. Value loss: 0.044. Policy accuracy: 0.282
2025-01-26 02:53:32,421 (Training) Epoch: 5. Total loss: 1.789. Value loss: 0.039. Policy accuracy: 0.299
2025-01-26 02:54:06,173 (Training) Epoch: 6. Total loss: 1.770. Value loss: 0.034. Policy accuracy: 0.307
2025-01-26 02:54:37,051 (Training) Epoch: 7. Total loss: 1.745. Value loss: 0.033. Policy accuracy: 0.320
2025-01-26 02:55:07,691 (Training) Epoch: 8. Total loss: 1.728. Value loss: 0.030. Policy accuracy: 0.326
2025-01-26 02:55:38,406 (Training) Epoch: 9. Total loss: 1.706. Value loss: 0.026. Policy accuracy: 0.334
2025-01-26 02:56:07,921 (Training) Epoch: 10. Total loss: 1.689. Value loss: 0.025. Policy accuracy: 0.341
2025-01-26 02:58:11,041 (Evaluation) Win rate: 0.7
2025-01-26 02:58:11,041 (Evaluation) Model 1 wins: 11. Draws: 1. Model 2 wins: 28
2025-01-26 02:58:11,046 (Evaluation) Accepting new model...
2025-01-26 02:58:11,099 

2025-01-26 02:58:56,605 (Experiment) Win rate (random): 0.925
2025-01-26 03:07:23,807 (Experiment) Win rate (alpha-beta pruning with depth 6): 0.55
2025-01-26 03:07:23,808 Iteration 2
2025-01-26 05:18:34,674 (Self-play) Number of new training examples: 213876
2025-01-26 05:18:34,674 (Self-play) Number of total training examples: 213876
2025-01-26 05:18:34,674 (Self-play) Number of total unique state-reward pairs: 152236
2025-01-26 05:18:34,674 (Self-play) Model 1 wins: 1290. Draws: 162. Model 2 wins: 1048
2025-01-26 05:20:08,001 (Training) Epoch: 1. Total loss: 1.690. Value loss: 0.060. Policy accuracy: 0.360
2025-01-26 05:21:06,078 (Training) Epoch: 2. Total loss: 1.663. Value loss: 0.053. Policy accuracy: 0.373
2025-01-26 05:22:05,226 (Training) Epoch: 3. Total loss: 1.635. Value loss: 0.047. Policy accuracy: 0.385
2025-01-26 05:23:04,384 (Training) Epoch: 4. Total loss: 1.616. Value loss: 0.043. Policy accuracy: 0.396
2025-01-26 05:24:03,576 (Training) Epoch: 5. Total loss: 1.597. Value loss: 0.040. Policy accuracy: 0.403
2025-01-26 05:25:02,522 (Training) Epoch: 6. Total loss: 1.577. Value loss: 0.038. Policy accuracy: 0.411
2025-01-26 05:26:01,534 (Training) Epoch: 7. Total loss: 1.563. Value loss: 0.037. Policy accuracy: 0.417
2025-01-26 05:27:00,876 (Training) Epoch: 8. Total loss: 1.542. Value loss: 0.035. Policy accuracy: 0.425
2025-01-26 05:27:59,972 (Training) Epoch: 9. Total loss: 1.533. Value loss: 0.034. Policy accuracy: 0.431
2025-01-26 05:28:59,369 (Training) Epoch: 10. Total loss: 1.512. Value loss: 0.033. Policy accuracy: 0.434
2025-01-26 05:31:04,174 (Evaluation) Win rate: 0.475
2025-01-26 05:31:04,175 (Evaluation) Model 1 wins: 18. Draws: 3. Model 2 wins: 19
2025-01-26 05:31:04,364 (Evaluation) Rejecting new model...
2025-01-26 05:31:04,641 

2025-01-26 05:31:50,317 (Experiment) Win rate (random): 1.0
2025-01-26 05:39:02,351 (Experiment) Win rate (alpha-beta pruning with depth 6): 0.55
2025-01-26 05:39:02,356 Iteration 3
2025-01-26 07:22:56,955 (Self-play) Number of new training examples: 328098
2025-01-26 07:22:56,955 (Self-play) Number of total training examples: 328098
2025-01-26 07:22:56,956 (Self-play) Number of total unique state-reward pairs: 230222
2025-01-26 07:22:56,956 (Self-play) Model 1 wins: 1305. Draws: 146. Model 2 wins: 1049
2025-01-26 07:25:20,315 (Training) Epoch: 1. Total loss: 1.684. Value loss: 0.067. Policy accuracy: 0.376
2025-01-26 07:26:51,283 (Training) Epoch: 2. Total loss: 1.647. Value loss: 0.060. Policy accuracy: 0.395
2025-01-26 07:28:22,917 (Training) Epoch: 3. Total loss: 1.627. Value loss: 0.054. Policy accuracy: 0.401
2025-01-26 07:29:54,306 (Training) Epoch: 4. Total loss: 1.608. Value loss: 0.050. Policy accuracy: 0.411
2025-01-26 07:31:26,094 (Training) Epoch: 5. Total loss: 1.593. Value loss: 0.046. Policy accuracy: 0.416
2025-01-26 07:32:57,744 (Training) Epoch: 6. Total loss: 1.577. Value loss: 0.044. Policy accuracy: 0.422
2025-01-26 07:34:29,204 (Training) Epoch: 7. Total loss: 1.561. Value loss: 0.042. Policy accuracy: 0.429
2025-01-26 07:36:01,070 (Training) Epoch: 8. Total loss: 1.544. Value loss: 0.040. Policy accuracy: 0.435
2025-01-26 07:37:33,034 (Training) Epoch: 9. Total loss: 1.526. Value loss: 0.038. Policy accuracy: 0.442
2025-01-26 07:39:04,568 (Training) Epoch: 10. Total loss: 1.515. Value loss: 0.037. Policy accuracy: 0.446
2025-01-26 07:41:28,069 (Evaluation) Win rate: 0.525
2025-01-26 07:41:28,069 (Evaluation) Model 1 wins: 13. Draws: 6. Model 2 wins: 21
2025-01-26 07:41:28,263 (Evaluation) Rejecting new model...
2025-01-26 07:41:28,316 

2025-01-26 07:42:10,543 (Experiment) Win rate (random): 1.0
2025-01-26 07:50:26,875 (Experiment) Win rate (alpha-beta pruning with depth 6): 0.475
2025-01-26 07:50:26,877 Iteration 4
2025-01-26 09:35:27,959 (Self-play) Number of new training examples: 442710
2025-01-26 09:35:27,959 (Self-play) Number of total training examples: 442710
2025-01-26 09:35:27,959 (Self-play) Number of total unique state-reward pairs: 307262
2025-01-26 09:35:27,959 (Self-play) Model 1 wins: 1272. Draws: 151. Model 2 wins: 1077
2025-01-26 09:38:41,260 (Training) Epoch: 1. Total loss: 1.678. Value loss: 0.071. Policy accuracy: 0.381
2025-01-26 09:40:44,363 (Training) Epoch: 2. Total loss: 1.642. Value loss: 0.064. Policy accuracy: 0.402
2025-01-26 09:42:47,903 (Training) Epoch: 3. Total loss: 1.619. Value loss: 0.059. Policy accuracy: 0.411
2025-01-26 09:44:51,451 (Training) Epoch: 4. Total loss: 1.602. Value loss: 0.054. Policy accuracy: 0.417
2025-01-26 09:46:55,279 (Training) Epoch: 5. Total loss: 1.587. Value loss: 0.050. Policy accuracy: 0.422
2025-01-26 09:48:59,153 (Training) Epoch: 6. Total loss: 1.572. Value loss: 0.048. Policy accuracy: 0.427
2025-01-26 09:51:02,999 (Training) Epoch: 7. Total loss: 1.556. Value loss: 0.046. Policy accuracy: 0.434
2025-01-26 09:53:06,876 (Training) Epoch: 8. Total loss: 1.542. Value loss: 0.044. Policy accuracy: 0.440
2025-01-26 09:55:10,777 (Training) Epoch: 9. Total loss: 1.529. Value loss: 0.042. Policy accuracy: 0.445
2025-01-26 09:57:14,813 (Training) Epoch: 10. Total loss: 1.515. Value loss: 0.041. Policy accuracy: 0.452
2025-01-26 09:59:34,014 (Evaluation) Win rate: 0.45
2025-01-26 09:59:34,014 (Evaluation) Model 1 wins: 14. Draws: 8. Model 2 wins: 18
2025-01-26 09:59:34,206 (Evaluation) Rejecting new model...
2025-01-26 09:59:34,278 

2025-01-26 10:00:13,744 (Experiment) Win rate (random): 0.975
2025-01-26 10:08:45,821 (Experiment) Win rate (alpha-beta pruning with depth 6): 0.3
2025-01-26 10:08:45,823 Iteration 5
2025-01-26 11:54:48,348 (Self-play) Number of new training examples: 557816
2025-01-26 11:54:48,349 (Self-play) Number of total training examples: 557816
2025-01-26 11:54:48,349 (Self-play) Number of total unique state-reward pairs: 383616
2025-01-26 11:54:48,349 (Self-play) Model 1 wins: 1317. Draws: 147. Model 2 wins: 1036
2025-01-26 11:59:02,594 (Training) Epoch: 1. Total loss: 1.675. Value loss: 0.073. Policy accuracy: 0.386
2025-01-26 12:01:38,474 (Training) Epoch: 2. Total loss: 1.636. Value loss: 0.067. Policy accuracy: 0.407
2025-01-26 12:04:14,661 (Training) Epoch: 3. Total loss: 1.614. Value loss: 0.062. Policy accuracy: 0.415
2025-01-26 12:06:50,781 (Training) Epoch: 4. Total loss: 1.598. Value loss: 0.058. Policy accuracy: 0.422
2025-01-26 12:09:27,182 (Training) Epoch: 5. Total loss: 1.584. Value loss: 0.054. Policy accuracy: 0.427
2025-01-26 12:12:04,010 (Training) Epoch: 6. Total loss: 1.568. Value loss: 0.051. Policy accuracy: 0.434
2025-01-26 12:14:40,447 (Training) Epoch: 7. Total loss: 1.556. Value loss: 0.049. Policy accuracy: 0.438
2025-01-26 12:17:17,090 (Training) Epoch: 8. Total loss: 1.541. Value loss: 0.047. Policy accuracy: 0.444
2025-01-26 12:19:53,793 (Training) Epoch: 9. Total loss: 1.526. Value loss: 0.046. Policy accuracy: 0.451
2025-01-26 12:22:30,468 (Training) Epoch: 10. Total loss: 1.513. Value loss: 0.044. Policy accuracy: 0.456
2025-01-26 12:24:53,472 (Evaluation) Win rate: 0.575
2025-01-26 12:24:53,472 (Evaluation) Model 1 wins: 10. Draws: 7. Model 2 wins: 23
2025-01-26 12:24:53,636 (Evaluation) Accepting new model...
2025-01-26 12:24:53,690 

2025-01-26 12:25:32,811 (Experiment) Win rate (random): 0.975
2025-01-26 12:33:06,323 (Experiment) Win rate (alpha-beta pruning with depth 6): 0.625
2025-01-26 12:33:06,325 Iteration 6
2025-01-26 14:23:35,341 (Self-play) Number of new training examples: 679516
2025-01-26 14:23:35,341 (Self-play) Number of total training examples: 679516
2025-01-26 14:23:35,341 (Self-play) Number of total unique state-reward pairs: 465802
2025-01-26 14:23:35,341 (Self-play) Model 1 wins: 1240. Draws: 238. Model 2 wins: 1022
2025-01-26 14:28:29,616 (Training) Epoch: 1. Total loss: 1.520. Value loss: 0.055. Policy accuracy: 0.460
2025-01-26 14:31:34,851 (Training) Epoch: 2. Total loss: 1.507. Value loss: 0.052. Policy accuracy: 0.464
2025-01-26 14:34:40,724 (Training) Epoch: 3. Total loss: 1.491. Value loss: 0.050. Policy accuracy: 0.471
2025-01-26 14:37:46,219 (Training) Epoch: 4. Total loss: 1.479. Value loss: 0.048. Policy accuracy: 0.475
2025-01-26 14:40:51,722 (Training) Epoch: 5. Total loss: 1.469. Value loss: 0.048. Policy accuracy: 0.479
2025-01-26 14:43:57,594 (Training) Epoch: 6. Total loss: 1.455. Value loss: 0.046. Policy accuracy: 0.486
2025-01-26 14:47:03,256 (Training) Epoch: 7. Total loss: 1.445. Value loss: 0.046. Policy accuracy: 0.487
2025-01-26 14:50:08,952 (Training) Epoch: 8. Total loss: 1.431. Value loss: 0.045. Policy accuracy: 0.495
2025-01-26 14:53:14,875 (Training) Epoch: 9. Total loss: 1.422. Value loss: 0.045. Policy accuracy: 0.498
2025-01-26 14:56:20,663 (Training) Epoch: 10. Total loss: 1.413. Value loss: 0.044. Policy accuracy: 0.499
2025-01-26 14:58:46,016 (Evaluation) Win rate: 0.375
2025-01-26 14:58:46,016 (Evaluation) Model 1 wins: 18. Draws: 7. Model 2 wins: 15
2025-01-26 14:58:46,189 (Evaluation) Rejecting new model...
2025-01-26 14:58:46,279 

2025-01-26 14:59:27,784 (Experiment) Win rate (random): 1.0
2025-01-26 15:07:55,230 (Experiment) Win rate (alpha-beta pruning with depth 6): 0.575
2025-01-26 15:07:55,232 Iteration 7
2025-01-26 16:59:05,195 (Self-play) Number of new training examples: 804554
2025-01-26 16:59:05,195 (Self-play) Number of total training examples: 804554
2025-01-26 16:59:05,195 (Self-play) Number of total unique state-reward pairs: 550538
2025-01-26 16:59:05,196 (Self-play) Model 1 wins: 1293. Draws: 252. Model 2 wins: 955
2025-01-26 17:05:13,539 (Training) Epoch: 1. Total loss: 1.532. Value loss: 0.064. Policy accuracy: 0.462
2025-01-26 17:08:51,341 (Training) Epoch: 2. Total loss: 1.514. Value loss: 0.060. Policy accuracy: 0.468
2025-01-26 17:12:29,358 (Training) Epoch: 3. Total loss: 1.500. Value loss: 0.057. Policy accuracy: 0.473
2025-01-26 17:16:07,062 (Training) Epoch: 4. Total loss: 1.489. Value loss: 0.055. Policy accuracy: 0.478
2025-01-26 17:19:45,706 (Training) Epoch: 5. Total loss: 1.476. Value loss: 0.053. Policy accuracy: 0.483
2025-01-26 17:23:23,889 (Training) Epoch: 6. Total loss: 1.464. Value loss: 0.053. Policy accuracy: 0.487
2025-01-26 17:27:02,105 (Training) Epoch: 7. Total loss: 1.454. Value loss: 0.051. Policy accuracy: 0.491
2025-01-26 17:30:40,858 (Training) Epoch: 8. Total loss: 1.445. Value loss: 0.051. Policy accuracy: 0.493
2025-01-26 17:34:56,046 (Training) Epoch: 9. Total loss: 1.433. Value loss: 0.049. Policy accuracy: 0.499
2025-01-26 17:38:59,553 (Training) Epoch: 10. Total loss: 1.423. Value loss: 0.049. Policy accuracy: 0.503
2025-01-26 17:41:20,264 (Evaluation) Win rate: 0.475
2025-01-26 17:41:20,264 (Evaluation) Model 1 wins: 16. Draws: 5. Model 2 wins: 19
2025-01-26 17:41:21,429 (Evaluation) Rejecting new model...
2025-01-26 17:41:21,942 

2025-01-26 17:42:02,305 (Experiment) Win rate (random): 1.0
2025-01-26 17:50:28,335 (Experiment) Win rate (alpha-beta pruning with depth 6): 0.625
2025-01-26 17:50:28,662 Iteration 8
2025-01-26 19:57:47,870 (Self-play) Number of new training examples: 925892
2025-01-26 19:57:47,871 (Self-play) Number of total training examples: 925892
2025-01-26 19:57:47,871 (Self-play) Number of total unique state-reward pairs: 630847
2025-01-26 19:57:47,871 (Self-play) Model 1 wins: 1255. Draws: 250. Model 2 wins: 995
2025-01-26 20:05:52,904 (Training) Epoch: 1. Total loss: 1.536. Value loss: 0.071. Policy accuracy: 0.464
2025-01-26 20:11:59,919 (Training) Epoch: 2. Total loss: 1.520. Value loss: 0.066. Policy accuracy: 0.470
2025-01-26 20:18:26,019 (Training) Epoch: 3. Total loss: 1.508. Value loss: 0.063. Policy accuracy: 0.474
2025-01-26 20:24:35,060 (Training) Epoch: 4. Total loss: 1.495. Value loss: 0.060. Policy accuracy: 0.480
2025-01-26 20:30:44,283 (Training) Epoch: 5. Total loss: 1.483. Value loss: 0.059. Policy accuracy: 0.484
2025-01-26 20:37:10,313 (Training) Epoch: 6. Total loss: 1.472. Value loss: 0.057. Policy accuracy: 0.488
2025-01-26 20:43:29,067 (Training) Epoch: 7. Total loss: 1.462. Value loss: 0.056. Policy accuracy: 0.493
2025-01-26 20:49:54,186 (Training) Epoch: 8. Total loss: 1.453. Value loss: 0.055. Policy accuracy: 0.497
2025-01-26 20:56:18,013 (Training) Epoch: 9. Total loss: 1.443. Value loss: 0.054. Policy accuracy: 0.501
2025-01-26 21:03:02,800 (Training) Epoch: 10. Total loss: 1.435. Value loss: 0.054. Policy accuracy: 0.503
2025-01-26 21:05:49,214 (Evaluation) Win rate: 0.375
2025-01-26 21:05:49,214 (Evaluation) Model 1 wins: 13. Draws: 12. Model 2 wins: 15
2025-01-26 21:05:49,797 (Evaluation) Rejecting new model...
2025-01-26 21:05:50,027 

2025-01-26 21:06:38,951 (Experiment) Win rate (random): 1.0
2025-01-26 21:15:08,095 (Experiment) Win rate (alpha-beta pruning with depth 6): 0.65
2025-01-26 21:15:08,131 Iteration 9
2025-01-27 00:29:53,717 (Self-play) Number of new training examples: 1046276
2025-01-27 00:29:53,717 (Self-play) Number of total training examples: 1046276
2025-01-27 00:29:53,717 (Self-play) Number of total unique state-reward pairs: 709635
2025-01-27 00:29:53,717 (Self-play) Model 1 wins: 1232. Draws: 226. Model 2 wins: 1042
2025-01-27 00:41:48,555 (Training) Epoch: 1. Total loss: 1.542. Value loss: 0.075. Policy accuracy: 0.465
2025-01-27 00:53:10,372 (Training) Epoch: 2. Total loss: 1.523. Value loss: 0.069. Policy accuracy: 0.472
2025-01-27 01:07:48,107 (Training) Epoch: 3. Total loss: 1.510. Value loss: 0.066. Policy accuracy: 0.478
2025-01-27 01:16:47,070 (Training) Epoch: 4. Total loss: 1.498. Value loss: 0.063. Policy accuracy: 0.482
2025-01-27 01:28:03,030 (Training) Epoch: 5. Total loss: 1.489. Value loss: 0.062. Policy accuracy: 0.485
2025-01-27 01:36:30,773 (Training) Epoch: 6. Total loss: 1.479. Value loss: 0.060. Policy accuracy: 0.490
2025-01-27 01:43:19,918 (Training) Epoch: 7. Total loss: 1.469. Value loss: 0.059. Policy accuracy: 0.493
2025-01-27 01:54:13,501 (Training) Epoch: 8. Total loss: 1.460. Value loss: 0.058. Policy accuracy: 0.498
2025-01-27 02:06:12,838 (Training) Epoch: 9. Total loss: 1.451. Value loss: 0.057. Policy accuracy: 0.501
2025-01-27 02:28:46,754 (Training) Epoch: 10. Total loss: 1.442. Value loss: 0.056. Policy accuracy: 0.503
2025-01-27 02:34:06,348 (Evaluation) Win rate: 0.4
2025-01-27 02:34:06,348 (Evaluation) Model 1 wins: 16. Draws: 8. Model 2 wins: 16
2025-01-27 02:34:07,029 (Evaluation) Rejecting new model...
2025-01-27 02:34:07,321 

2025-01-27 02:35:10,172 (Experiment) Win rate (random): 1.0
2025-01-27 02:44:38,859 (Experiment) Win rate (alpha-beta pruning with depth 6): 0.575
2025-01-27 02:44:38,880 Iteration 10
2025-01-27 04:59:57,885 (Self-play) Number of new training examples: 1166246
2025-01-27 04:59:57,886 (Self-play) Number of total training examples: 1166246
2025-01-27 04:59:57,886 (Self-play) Number of total unique state-reward pairs: 787620
2025-01-27 04:59:57,887 (Self-play) Model 1 wins: 1259. Draws: 251. Model 2 wins: 990
2025-01-27 05:08:28,344 (Training) Epoch: 1. Total loss: 1.544. Value loss: 0.079. Policy accuracy: 0.468
2025-01-27 05:13:43,172 (Training) Epoch: 2. Total loss: 1.529. Value loss: 0.074. Policy accuracy: 0.474
2025-01-27 05:19:00,894 (Training) Epoch: 3. Total loss: 1.515. Value loss: 0.070. Policy accuracy: 0.480
2025-01-27 05:24:18,999 (Training) Epoch: 4. Total loss: 1.503. Value loss: 0.068. Policy accuracy: 0.483
2025-01-27 05:29:37,253 (Training) Epoch: 5. Total loss: 1.493. Value loss: 0.065. Policy accuracy: 0.488
2025-01-27 05:34:56,036 (Training) Epoch: 6. Total loss: 1.483. Value loss: 0.064. Policy accuracy: 0.492
2025-01-27 05:40:14,261 (Training) Epoch: 7. Total loss: 1.477. Value loss: 0.063. Policy accuracy: 0.495
2025-01-27 05:45:32,756 (Training) Epoch: 8. Total loss: 1.466. Value loss: 0.061. Policy accuracy: 0.497
2025-01-27 05:50:51,502 (Training) Epoch: 9. Total loss: 1.458. Value loss: 0.061. Policy accuracy: 0.501
2025-01-27 05:56:10,241 (Training) Epoch: 10. Total loss: 1.450. Value loss: 0.060. Policy accuracy: 0.506
2025-01-27 05:58:32,838 (Evaluation) Win rate: 0.45
2025-01-27 05:58:32,838 (Evaluation) Model 1 wins: 13. Draws: 9. Model 2 wins: 18
2025-01-27 05:58:33,123 (Evaluation) Rejecting new model...
2025-01-27 05:58:33,207 

2025-01-27 05:59:18,518 (Experiment) Win rate (random): 0.975
2025-01-27 06:07:34,298 (Experiment) Win rate (alpha-beta pruning with depth 6): 0.725
2025-01-27 06:07:34,312 Iteration 11
2025-01-27 08:00:22,949 (Self-play) Number of new training examples: 1290032
2025-01-27 08:00:22,949 (Self-play) Number of total training examples: 1290032
2025-01-27 08:00:22,949 (Self-play) Number of total unique state-reward pairs: 868746
2025-01-27 08:00:22,949 (Self-play) Model 1 wins: 1293. Draws: 237. Model 2 wins: 970
2025-01-27 08:10:01,800 (Training) Epoch: 1. Total loss: 1.546. Value loss: 0.082. Policy accuracy: 0.469
2025-01-27 08:15:52,101 (Training) Epoch: 2. Total loss: 1.529. Value loss: 0.076. Policy accuracy: 0.475
2025-01-27 08:21:45,320 (Training) Epoch: 3. Total loss: 1.515. Value loss: 0.072. Policy accuracy: 0.482
2025-01-27 08:27:38,941 (Training) Epoch: 4. Total loss: 1.506. Value loss: 0.070. Policy accuracy: 0.484
2025-01-27 08:33:31,953 (Training) Epoch: 5. Total loss: 1.496. Value loss: 0.068. Policy accuracy: 0.489
2025-01-27 08:39:24,837 (Training) Epoch: 6. Total loss: 1.486. Value loss: 0.066. Policy accuracy: 0.492
2025-01-27 08:45:17,970 (Training) Epoch: 7. Total loss: 1.479. Value loss: 0.065. Policy accuracy: 0.496
2025-01-27 08:51:10,655 (Training) Epoch: 8. Total loss: 1.471. Value loss: 0.064. Policy accuracy: 0.499
2025-01-27 08:57:03,865 (Training) Epoch: 9. Total loss: 1.464. Value loss: 0.063. Policy accuracy: 0.502
2025-01-27 09:02:56,648 (Training) Epoch: 10. Total loss: 1.457. Value loss: 0.062. Policy accuracy: 0.504
2025-01-27 09:05:14,803 (Evaluation) Win rate: 0.375
2025-01-27 09:05:14,803 (Evaluation) Model 1 wins: 14. Draws: 11. Model 2 wins: 15
2025-01-27 09:05:15,016 (Evaluation) Rejecting new model...
2025-01-27 09:05:15,084 

2025-01-27 09:05:50,717 (Experiment) Win rate (random): 1.0
2025-01-27 09:13:00,463 (Experiment) Win rate (alpha-beta pruning with depth 6): 0.625
2025-01-27 09:13:00,465 Iteration 12
2025-01-27 11:05:06,221 (Self-play) Number of new training examples: 1413918
2025-01-27 11:05:06,221 (Self-play) Number of total training examples: 1413918
2025-01-27 11:05:06,221 (Self-play) Number of total unique state-reward pairs: 949431
2025-01-27 11:05:06,221 (Self-play) Model 1 wins: 1241. Draws: 272. Model 2 wins: 987
2025-01-27 11:15:17,334 (Training) Epoch: 1. Total loss: 1.545. Value loss: 0.085. Policy accuracy: 0.471
2025-01-27 11:21:43,461 (Training) Epoch: 2. Total loss: 1.529. Value loss: 0.079. Policy accuracy: 0.479
2025-01-27 11:28:10,171 (Training) Epoch: 3. Total loss: 1.518. Value loss: 0.076. Policy accuracy: 0.483
2025-01-27 11:34:36,934 (Training) Epoch: 4. Total loss: 1.507. Value loss: 0.073. Policy accuracy: 0.488
2025-01-27 11:41:03,062 (Training) Epoch: 5. Total loss: 1.498. Value loss: 0.071. Policy accuracy: 0.492
2025-01-27 11:47:29,960 (Training) Epoch: 6. Total loss: 1.489. Value loss: 0.070. Policy accuracy: 0.496
2025-01-27 11:53:56,579 (Training) Epoch: 7. Total loss: 1.481. Value loss: 0.068. Policy accuracy: 0.499
2025-01-27 12:00:23,406 (Training) Epoch: 8. Total loss: 1.473. Value loss: 0.067. Policy accuracy: 0.502
2025-01-27 12:06:49,749 (Training) Epoch: 9. Total loss: 1.467. Value loss: 0.066. Policy accuracy: 0.505
2025-01-27 12:13:16,291 (Training) Epoch: 10. Total loss: 1.460. Value loss: 0.065. Policy accuracy: 0.508
2025-01-27 12:15:42,604 (Evaluation) Win rate: 0.45
2025-01-27 12:15:42,604 (Evaluation) Model 1 wins: 15. Draws: 7. Model 2 wins: 18
2025-01-27 12:15:42,927 (Evaluation) Rejecting new model...
2025-01-27 12:15:43,019 

2025-01-27 12:16:29,259 (Experiment) Win rate (random): 1.0
2025-01-27 12:24:07,547 (Experiment) Win rate (alpha-beta pruning with depth 6): 0.6
2025-01-27 12:24:07,550 Iteration 13
2025-01-27 14:16:34,159 (Self-play) Number of new training examples: 1536258
2025-01-27 14:16:34,159 (Self-play) Number of total training examples: 1536258
2025-01-27 14:16:34,159 (Self-play) Number of total unique state-reward pairs: 1028287
2025-01-27 14:16:34,159 (Self-play) Model 1 wins: 1282. Draws: 232. Model 2 wins: 986
2025-01-27 14:28:51,233 (Training) Epoch: 1. Total loss: 1.547. Value loss: 0.086. Policy accuracy: 0.473
2025-01-27 14:35:48,633 (Training) Epoch: 2. Total loss: 1.530. Value loss: 0.081. Policy accuracy: 0.481
2025-01-27 14:42:47,736 (Training) Epoch: 3. Total loss: 1.517. Value loss: 0.078. Policy accuracy: 0.487
2025-01-27 14:49:47,156 (Training) Epoch: 4. Total loss: 1.508. Value loss: 0.075. Policy accuracy: 0.490
2025-01-27 14:56:46,503 (Training) Epoch: 5. Total loss: 1.500. Value loss: 0.073. Policy accuracy: 0.494
2025-01-27 15:03:46,070 (Training) Epoch: 6. Total loss: 1.491. Value loss: 0.071. Policy accuracy: 0.498
2025-01-27 15:10:45,527 (Training) Epoch: 7. Total loss: 1.483. Value loss: 0.070. Policy accuracy: 0.501
2025-01-27 15:17:44,874 (Training) Epoch: 8. Total loss: 1.477. Value loss: 0.069. Policy accuracy: 0.503
2025-01-27 15:24:45,150 (Training) Epoch: 9. Total loss: 1.470. Value loss: 0.068. Policy accuracy: 0.505
2025-01-27 15:31:44,237 (Training) Epoch: 10. Total loss: 1.465. Value loss: 0.067. Policy accuracy: 0.507
2025-01-27 15:34:15,385 (Evaluation) Win rate: 0.6
2025-01-27 15:34:15,385 (Evaluation) Model 1 wins: 7. Draws: 9. Model 2 wins: 24
2025-01-27 15:34:19,033 (Evaluation) Accepting new model...
2025-01-27 15:34:20,069 

2025-01-27 15:35:05,959 (Experiment) Win rate (random): 1.0
2025-01-27 15:42:43,332 (Experiment) Win rate (alpha-beta pruning with depth 6): 0.625
2025-01-27 15:42:43,355 Iteration 14
2025-01-27 17:43:26,454 (Self-play) Number of new training examples: 1667250
2025-01-27 17:43:26,506 (Self-play) Number of total training examples: 1667250
2025-01-27 17:43:26,507 (Self-play) Number of total unique state-reward pairs: 1115617
2025-01-27 17:43:26,507 (Self-play) Model 1 wins: 1184. Draws: 292. Model 2 wins: 1024
2025-01-27 17:56:58,188 (Training) Epoch: 1. Total loss: 1.467. Value loss: 0.071. Policy accuracy: 0.510
2025-01-27 18:04:29,037 (Training) Epoch: 2. Total loss: 1.462. Value loss: 0.070. Policy accuracy: 0.512
2025-01-27 18:12:00,100 (Training) Epoch: 3. Total loss: 1.457. Value loss: 0.069. Policy accuracy: 0.514
2025-01-27 18:19:31,166 (Training) Epoch: 4. Total loss: 1.451. Value loss: 0.068. Policy accuracy: 0.515
2025-01-27 18:27:02,666 (Training) Epoch: 5. Total loss: 1.447. Value loss: 0.068. Policy accuracy: 0.517
2025-01-27 18:34:33,599 (Training) Epoch: 6. Total loss: 1.444. Value loss: 0.067. Policy accuracy: 0.518
2025-01-27 18:42:04,598 (Training) Epoch: 7. Total loss: 1.439. Value loss: 0.067. Policy accuracy: 0.520
2025-01-27 18:49:34,854 (Training) Epoch: 8. Total loss: 1.437. Value loss: 0.067. Policy accuracy: 0.521
2025-01-27 18:57:06,035 (Training) Epoch: 9. Total loss: 1.433. Value loss: 0.066. Policy accuracy: 0.523
2025-01-27 19:04:36,376 (Training) Epoch: 10. Total loss: 1.432. Value loss: 0.066. Policy accuracy: 0.523
2025-01-27 19:07:12,030 (Evaluation) Win rate: 0.45
2025-01-27 19:07:12,030 (Evaluation) Model 1 wins: 16. Draws: 6. Model 2 wins: 18
2025-01-27 19:07:13,201 (Evaluation) Rejecting new model...
2025-01-27 19:07:13,568 

2025-01-27 19:07:58,445 (Experiment) Win rate (random): 1.0
2025-01-27 19:15:44,509 (Experiment) Win rate (alpha-beta pruning with depth 6): 0.75
2025-01-27 19:15:44,561 Iteration 15
2025-01-27 21:15:12,689 (Self-play) Number of new training examples: 1796518
2025-01-27 21:15:12,707 (Self-play) Number of total training examples: 1796518
2025-01-27 21:15:12,707 (Self-play) Number of total unique state-reward pairs: 1200955
2025-01-27 21:15:12,707 (Self-play) Model 1 wins: 1244. Draws: 308. Model 2 wins: 948
2025-01-27 21:28:43,506 (Training) Epoch: 1. Total loss: 1.473. Value loss: 0.075. Policy accuracy: 0.510
2025-01-27 21:36:48,176 (Training) Epoch: 2. Total loss: 1.467. Value loss: 0.073. Policy accuracy: 0.512
2025-01-27 21:44:55,649 (Training) Epoch: 3. Total loss: 1.462. Value loss: 0.072. Policy accuracy: 0.514
2025-01-27 21:53:03,372 (Training) Epoch: 4. Total loss: 1.457. Value loss: 0.071. Policy accuracy: 0.516
2025-01-27 22:01:11,204 (Training) Epoch: 5. Total loss: 1.454. Value loss: 0.071. Policy accuracy: 0.517
2025-01-27 22:09:18,987 (Training) Epoch: 6. Total loss: 1.451. Value loss: 0.070. Policy accuracy: 0.518
2025-01-27 22:17:26,739 (Training) Epoch: 7. Total loss: 1.446. Value loss: 0.070. Policy accuracy: 0.520
2025-01-27 22:25:35,949 (Training) Epoch: 8. Total loss: 1.443. Value loss: 0.070. Policy accuracy: 0.522
2025-01-27 22:33:44,657 (Training) Epoch: 9. Total loss: 1.441. Value loss: 0.069. Policy accuracy: 0.522
2025-01-27 22:41:54,161 (Training) Epoch: 10. Total loss: 1.438. Value loss: 0.069. Policy accuracy: 0.524
2025-01-27 22:44:39,563 (Evaluation) Win rate: 0.3
2025-01-27 22:44:39,563 (Evaluation) Model 1 wins: 15. Draws: 13. Model 2 wins: 12
2025-01-27 22:44:40,510 (Evaluation) Rejecting new model...
2025-01-27 22:44:41,010 

2025-01-27 22:45:24,212 (Experiment) Win rate (random): 1.0
2025-01-27 22:53:18,848 (Experiment) Win rate (alpha-beta pruning with depth 6): 0.775
2025-01-27 22:53:18,875 Iteration 16
2025-01-28 00:54:58,761 (Self-play) Number of new training examples: 1927248
2025-01-28 00:54:58,769 (Self-play) Number of total training examples: 1927248
2025-01-28 00:54:58,769 (Self-play) Number of total unique state-reward pairs: 1287368
2025-01-28 00:54:58,769 (Self-play) Model 1 wins: 1227. Draws: 294. Model 2 wins: 979
2025-01-28 01:10:22,242 (Training) Epoch: 1. Total loss: 1.477. Value loss: 0.078. Policy accuracy: 0.512
2025-01-28 01:19:00,039 (Training) Epoch: 2. Total loss: 1.471. Value loss: 0.076. Policy accuracy: 0.514
2025-01-28 01:27:42,113 (Training) Epoch: 3. Total loss: 1.467. Value loss: 0.075. Policy accuracy: 0.515
2025-01-28 01:36:20,370 (Training) Epoch: 4. Total loss: 1.462. Value loss: 0.074. Policy accuracy: 0.517
2025-01-28 01:44:57,966 (Training) Epoch: 5. Total loss: 1.458. Value loss: 0.073. Policy accuracy: 0.519
2025-01-28 01:53:36,253 (Training) Epoch: 6. Total loss: 1.454. Value loss: 0.073. Policy accuracy: 0.521
2025-01-28 02:02:15,216 (Training) Epoch: 7. Total loss: 1.451. Value loss: 0.072. Policy accuracy: 0.522
2025-01-28 02:10:53,264 (Training) Epoch: 8. Total loss: 1.450. Value loss: 0.072. Policy accuracy: 0.523
2025-01-28 02:19:31,407 (Training) Epoch: 9. Total loss: 1.445. Value loss: 0.072. Policy accuracy: 0.524
2025-01-28 02:28:09,575 (Training) Epoch: 10. Total loss: 1.442. Value loss: 0.071. Policy accuracy: 0.525
2025-01-28 02:31:05,039 (Evaluation) Win rate: 0.475
2025-01-28 02:31:05,039 (Evaluation) Model 1 wins: 15. Draws: 6. Model 2 wins: 19
2025-01-28 02:31:13,221 (Evaluation) Rejecting new model...
2025-01-28 02:31:14,332 

2025-01-28 02:31:57,821 (Experiment) Win rate (random): 1.0
2025-01-28 02:40:06,050 (Experiment) Win rate (alpha-beta pruning with depth 6): 0.625
2025-01-28 02:40:06,435 Iteration 17
2025-01-28 04:57:21,444 (Self-play) Number of new training examples: 2057794
2025-01-28 04:57:21,471 (Self-play) Number of total training examples: 2057794
2025-01-28 04:57:21,472 (Self-play) Number of total unique state-reward pairs: 1372893
2025-01-28 04:57:21,472 (Self-play) Model 1 wins: 1205. Draws: 295. Model 2 wins: 1000
