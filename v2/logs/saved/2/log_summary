2025-01-17 22:29:50,307 Iteration 1
2025-01-17 23:14:19,218 (Self-play) Number of new training examples: 5196
2025-01-17 23:14:19,218 (Self-play) Number of total training examples: 5196
2025-01-17 23:14:19,218 (Self-play) Model 1 wins: 50. Draws: 4. Model 2 wins: 46
2025-01-17 23:14:21,940 (Training) Epoch: 1. Total loss: 2.635. Value loss: 0.657. Policy accuracy: 0.228
2025-01-17 23:14:22,781 (Training) Epoch: 2. Total loss: 2.183. Value loss: 0.220. Policy accuracy: 0.264
2025-01-17 23:14:23,574 (Training) Epoch: 3. Total loss: 2.067. Value loss: 0.114. Policy accuracy: 0.281
2025-01-17 23:24:38,892 (Evaluation) Win rate: 0.55
2025-01-17 23:24:38,892 (Evaluation) Model 1 wins: 8. Draws: 1. Model 2 wins: 11
2025-01-17 23:24:38,900 (Evaluation) Accepting new model...
2025-01-17 23:24:38,913 

2025-01-17 23:24:38,914 Iteration 2
e2025-01-18 00:23:39,703 (Self-play) Number of new training examples: 4810
2025-01-18 00:23:39,703 (Self-play) Number of total training examples: 10006
2025-01-18 00:23:39,703 (Self-play) Model 1 wins: 51. Draws: 5. Model 2 wins: 44
2025-01-18 00:23:41,851 (Training) Epoch: 1. Total loss: 2.100. Value loss: 0.173. Policy accuracy: 0.297
2025-01-18 00:23:43,599 (Training) Epoch: 2. Total loss: 2.005. Value loss: 0.092. Policy accuracy: 0.304
2025-01-18 00:23:45,450 (Training) Epoch: 3. Total loss: 1.943. Value loss: 0.060. Policy accuracy: 0.348
2025-01-18 00:45:46,895 (Evaluation) Win rate: 0.5
2025-01-18 00:45:46,895 (Evaluation) Model 1 wins: 10. Draws: 0. Model 2 wins: 10
2025-01-18 00:45:46,904 (Evaluation) Rejecting new model...
2025-01-18 00:45:46,919 

2025-01-18 00:45:46,920 Iteration 3
2025-01-18 01:55:37,802 (Self-play) Number of new training examples: 5022
2025-01-18 01:55:37,802 (Self-play) Number of total training examples: 15028
2025-01-18 01:55:37,803 (Self-play) Model 1 wins: 43. Draws: 9. Model 2 wins: 48
2025-01-18 01:55:40,313 (Training) Epoch: 1. Total loss: 2.112. Value loss: 0.185. Policy accuracy: 0.285
2025-01-18 01:55:42,714 (Training) Epoch: 2. Total loss: 2.002. Value loss: 0.102. Policy accuracy: 0.323
2025-01-18 01:55:45,173 (Training) Epoch: 3. Total loss: 1.960. Value loss: 0.083. Policy accuracy: 0.339
2025-01-18 02:09:43,017 (Evaluation) Win rate: 0.75
2025-01-18 02:09:43,017 (Evaluation) Model 1 wins: 2. Draws: 3. Model 2 wins: 15
2025-01-18 02:09:43,026 (Evaluation) Accepting new model...
2025-01-18 02:09:43,040 

2025-01-18 02:09:43,044 Iteration 4
2025-01-18 03:07:58,507 (Self-play) Number of new training examples: 4724
2025-01-18 03:07:58,507 (Self-play) Number of total training examples: 19752
2025-01-18 03:07:58,507 (Self-play) Model 1 wins: 50. Draws: 11. Model 2 wins: 39
2025-01-18 03:08:01,938 (Training) Epoch: 1. Total loss: 1.930. Value loss: 0.102. Policy accuracy: 0.407
2025-01-18 03:08:05,277 (Training) Epoch: 2. Total loss: 1.898. Value loss: 0.088. Policy accuracy: 0.425
2025-01-18 03:08:08,509 (Training) Epoch: 3. Total loss: 1.868. Value loss: 0.072. Policy accuracy: 0.441
2025-01-18 03:18:03,944 (Evaluation) Win rate: 0.05
2025-01-18 03:18:03,944 (Evaluation) Model 1 wins: 12. Draws: 7. Model 2 wins: 1
2025-01-18 03:18:03,953 (Evaluation) Rejecting new model...
2025-01-18 03:18:03,966 

2025-01-18 03:18:03,969 Iteration 52025-01-18 04:39:20,598 (Self-play) Number of new training examples: 4804
2025-01-18 04:39:20,598 (Self-play) Number of total training examples: 24556
2025-01-18 04:39:20,598 (Self-play) Model 1 wins: 46. Draws: 15. Model 2 wins: 39
2025-01-18 04:39:24,854 (Training) Epoch: 1. Total loss: 1.940. Value loss: 0.126. Policy accuracy: 0.427
2025-01-18 04:39:28,897 (Training) Epoch: 2. Total loss: 1.891. Value loss: 0.097. Policy accuracy: 0.445
2025-01-18 04:39:32,747 (Training) Epoch: 3. Total loss: 1.877. Value loss: 0.094. Policy accuracy: 0.456
2025-01-18 04:48:46,551 (Evaluation) Win rate: 0.65
2025-01-18 04:48:46,551 (Evaluation) Model 1 wins: 7. Draws: 0. Model 2 wins: 13
2025-01-18 04:48:46,559 (Evaluation) Accepting new model...
2025-01-18 04:48:46,573 

2025-01-18 04:48:46,574 Iteration 6
2025-01-18 05:30:20,684 (Self-play) Number of new training examples: 5408
2025-01-18 05:30:20,684 (Self-play) Number of total training examples: 29964
2025-01-18 05:30:20,685 (Self-play) Model 1 wins: 44. Draws: 20. Model 2 wins: 36
2025-01-18 05:30:25,235 (Training) Epoch: 1. Total loss: 1.846. Value loss: 0.104. Policy accuracy: 0.500
2025-01-18 05:30:29,581 (Training) Epoch: 2. Total loss: 1.818. Value loss: 0.086. Policy accuracy: 0.511
2025-01-18 05:30:34,288 (Training) Epoch: 3. Total loss: 1.797. Value loss: 0.081. Policy accuracy: 0.526
2025-01-18 05:36:20,292 (Evaluation) Win rate: 0.5
2025-01-18 05:36:20,292 (Evaluation) Model 1 wins: 9. Draws: 1. Model 2 wins: 10
2025-01-18 05:36:20,299 (Evaluation) Rejecting new model...
2025-01-18 05:36:20,321 

2025-01-18 05:36:20,322 Iteration 7
2025-01-18 06:16:31,997 (Self-play) Number of new training examples: 5236
2025-01-18 06:16:31,997 (Self-play) Number of total training examples: 35200
2025-01-18 06:16:31,997 (Self-play) Model 1 wins: 48. Draws: 19. Model 2 wins: 33
2025-01-18 06:16:37,484 (Training) Epoch: 1. Total loss: 1.832. Value loss: 0.112. Policy accuracy: 0.523
2025-01-18 06:16:42,972 (Training) Epoch: 2. Total loss: 1.799. Value loss: 0.100. Policy accuracy: 0.540
2025-01-18 06:16:48,413 (Training) Epoch: 3. Total loss: 1.781. Value loss: 0.090. Policy accuracy: 0.551
2025-01-18 06:22:23,397 (Evaluation) Win rate: 0.7
2025-01-18 06:22:23,397 (Evaluation) Model 1 wins: 1. Draws: 5. Model 2 wins: 14
2025-01-18 06:22:23,407 (Evaluation) Accepting new model...
2025-01-18 06:22:23,422 

2025-01-18 06:22:23,423 Iteration 8
2025-01-18 07:05:33,974 (Self-play) Number of new training examples: 6076
2025-01-18 07:05:33,974 (Self-play) Number of total training examples: 41276
2025-01-18 07:05:33,974 (Self-play) Model 1 wins: 38. Draws: 29. Model 2 wins: 33
2025-01-18 07:05:40,042 (Training) Epoch: 1. Total loss: 1.771. Value loss: 0.102. Policy accuracy: 0.571
2025-01-18 07:05:46,274 (Training) Epoch: 2. Total loss: 1.764. Value loss: 0.103. Policy accuracy: 0.581
2025-01-18 07:05:52,832 (Training) Epoch: 3. Total loss: 1.746. Value loss: 0.088. Policy accuracy: 0.584
2025-01-18 07:11:30,629 (Evaluation) Win rate: 0.2
2025-01-18 07:11:30,629 (Evaluation) Model 1 wins: 15. Draws: 1. Model 2 wins: 4
2025-01-18 07:11:30,638 (Evaluation) Rejecting new model...
2025-01-18 07:11:30,649 

2025-01-18 07:11:30,650 Iteration 9
2025-01-18 07:54:24,531 (Self-play) Number of new training examples: 6056
2025-01-18 07:54:24,532 (Self-play) Number of total training examples: 47332
2025-01-18 07:54:24,532 (Self-play) Model 1 wins: 32. Draws: 29. Model 2 wins: 39
2025-01-18 07:54:32,187 (Training) Epoch: 1. Total loss: 1.767. Value loss: 0.117. Policy accuracy: 0.590
2025-01-18 07:54:39,573 (Training) Epoch: 2. Total loss: 1.754. Value loss: 0.105. Policy accuracy: 0.593
2025-01-18 07:54:46,580 (Training) Epoch: 3. Total loss: 1.743. Value loss: 0.102. Policy accuracy: 0.600
2025-01-18 08:00:30,295 (Evaluation) Win rate: 0.0
2025-01-18 08:00:30,295 (Evaluation) Model 1 wins: 17. Draws: 3. Model 2 wins: 0
2025-01-18 08:00:30,303 (Evaluation) Rejecting new model...
2025-01-18 08:00:30,316 

2025-01-18 08:00:30,316 Iteration 10
2025-01-18 08:43:11,170 (Self-play) Number of new training examples: 6092
2025-01-18 08:43:11,170 (Self-play) Number of total training examples: 53424
2025-01-18 08:43:11,171 (Self-play) Model 1 wins: 36. Draws: 26. Model 2 wins: 38
2025-01-18 08:43:19,259 (Training) Epoch: 1. Total loss: 1.756. Value loss: 0.125. Policy accuracy: 0.611
2025-01-18 08:43:27,637 (Training) Epoch: 2. Total loss: 1.742. Value loss: 0.112. Policy accuracy: 0.612
2025-01-18 08:43:35,699 (Training) Epoch: 3. Total loss: 1.733. Value loss: 0.108. Policy accuracy: 0.616
2025-01-18 08:48:35,805 (Evaluation) Win rate: 0.45
2025-01-18 08:48:35,805 (Evaluation) Model 1 wins: 8. Draws: 3. Model 2 wins: 9
2025-01-18 08:48:35,814 (Evaluation) Rejecting new model...
2025-01-18 08:48:35,827 

2025-01-18 08:48:35,827 Iteration 11
