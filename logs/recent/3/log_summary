2025-01-28 05:44:05,587 Iteration 1
2025-01-28 07:42:19,546 (Self-play) Number of new training examples: 2189070
2025-01-28 07:42:19,546 (Self-play) Number of total training examples: 2189070
2025-01-28 07:42:19,546 (Self-play) Number of total unique state-reward pairs: 1459249
2025-01-28 07:42:19,546 (Self-play) Model 1 wins: 1222. Draws: 330. Model 2 wins: 948
2025-01-28 07:57:34,762 (Training) Epoch: 1. Total loss: 1.484. Value loss: 0.084. Policy accuracy: 0.514
2025-01-28 08:07:10,959 (Training) Epoch: 2. Total loss: 1.478. Value loss: 0.081. Policy accuracy: 0.516
2025-01-28 08:16:46,456 (Training) Epoch: 3. Total loss: 1.473. Value loss: 0.080. Policy accuracy: 0.518
2025-01-28 08:26:21,841 (Training) Epoch: 4. Total loss: 1.468. Value loss: 0.079. Policy accuracy: 0.519
2025-01-28 08:35:57,442 (Training) Epoch: 5. Total loss: 1.466. Value loss: 0.078. Policy accuracy: 0.521
2025-01-28 08:45:32,738 (Training) Epoch: 6. Total loss: 1.462. Value loss: 0.077. Policy accuracy: 0.522
2025-01-28 08:55:08,256 (Training) Epoch: 7. Total loss: 1.462. Value loss: 0.077. Policy accuracy: 0.522
2025-01-28 09:04:44,332 (Training) Epoch: 8. Total loss: 1.456. Value loss: 0.076. Policy accuracy: 0.525
2025-01-28 09:14:20,608 (Training) Epoch: 9. Total loss: 1.454. Value loss: 0.076. Policy accuracy: 0.526
2025-01-28 09:23:56,707 (Training) Epoch: 10. Total loss: 1.451. Value loss: 0.076. Policy accuracy: 0.526
2025-01-28 09:26:31,046 (Evaluation) Win rate: 0.225
2025-01-28 09:26:31,046 (Evaluation) Model 1 wins: 18. Draws: 13. Model 2 wins: 9
2025-01-28 09:26:31,051 (Evaluation) Rejecting new model...
2025-01-28 09:27:17,286 (Experiment) Win rate (random): 1.0
2025-01-28 09:35:27,766 (Experiment) Win rate (alpha-beta pruning with depth 6): 0.675
2025-01-28 09:35:27,766 

2025-01-28 09:35:27,767 Iteration 2
2025-01-28 11:34:13,027 (Self-play) Number of new training examples: 2318676
2025-01-28 11:34:13,027 (Self-play) Number of total training examples: 2318676
2025-01-28 11:34:13,027 (Self-play) Number of total unique state-reward pairs: 1543489
2025-01-28 11:34:13,027 (Self-play) Model 1 wins: 1203. Draws: 298. Model 2 wins: 999
2025-01-28 11:49:59,254 (Training) Epoch: 1. Total loss: 1.486. Value loss: 0.086. Policy accuracy: 0.516
2025-01-28 12:00:11,658 (Training) Epoch: 2. Total loss: 1.480. Value loss: 0.083. Policy accuracy: 0.517
2025-01-28 12:10:24,011 (Training) Epoch: 3. Total loss: 1.476. Value loss: 0.082. Policy accuracy: 0.519
2025-01-28 12:20:36,464 (Training) Epoch: 4. Total loss: 1.472. Value loss: 0.081. Policy accuracy: 0.521
2025-01-28 12:30:49,132 (Training) Epoch: 5. Total loss: 1.468. Value loss: 0.079. Policy accuracy: 0.522
2025-01-28 12:41:01,013 (Training) Epoch: 6. Total loss: 1.466. Value loss: 0.079. Policy accuracy: 0.523
2025-01-28 12:51:13,187 (Training) Epoch: 7. Total loss: 1.462. Value loss: 0.079. Policy accuracy: 0.524
2025-01-28 13:01:25,010 (Training) Epoch: 8. Total loss: 1.460. Value loss: 0.078. Policy accuracy: 0.526
2025-01-28 13:11:38,085 (Training) Epoch: 9. Total loss: 1.458. Value loss: 0.078. Policy accuracy: 0.526
2025-01-28 13:21:51,403 (Training) Epoch: 10. Total loss: 1.455. Value loss: 0.077. Policy accuracy: 0.527
2025-01-28 13:24:11,996 (Evaluation) Win rate: 0.425
2025-01-28 13:24:11,997 (Evaluation) Model 1 wins: 14. Draws: 9. Model 2 wins: 17
2025-01-28 13:24:12,265 (Evaluation) Rejecting new model...
2025-01-28 13:24:59,275 (Experiment) Win rate (random): 0.975
2025-01-28 13:32:50,039 (Experiment) Win rate (alpha-beta pruning with depth 6): 0.65
2025-01-28 13:32:50,039 

2025-01-28 13:32:50,050 Iteration 3
2025-01-28 15:35:36,874 (Self-play) Number of new training examples: 2447378
2025-01-28 15:35:36,885 (Self-play) Number of total training examples: 2447378
2025-01-28 15:35:36,885 (Self-play) Number of total unique state-reward pairs: 1626567
2025-01-28 15:35:36,885 (Self-play) Model 1 wins: 1197. Draws: 287. Model 2 wins: 1016
