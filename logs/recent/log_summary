2025-01-29 03:09:16,609 Iteration 1
2025-01-29 03:40:58,690 (Self-play) Number of new training examples: 10556
2025-01-29 03:40:58,690 (Self-play) Number of total training examples: 10556
2025-01-29 03:40:58,690 (Self-play) Number of total unique state-reward pairs: 8649
2025-01-29 03:40:58,690 (Self-play) Model 1 wins: 126. Draws: 17. Model 2 wins: 107
2025-01-29 03:41:04,430 (Training) Epoch: 1. Total loss: 2.844. Value loss: 0.869. Policy accuracy: 0.156
2025-01-29 03:41:06,706 (Training) Epoch: 2. Total loss: 2.824. Value loss: 0.865. Policy accuracy: 0.161
2025-01-29 03:41:09,065 (Training) Epoch: 3. Total loss: 2.783. Value loss: 0.845. Policy accuracy: 0.178
2025-01-29 03:41:11,251 (Training) Epoch: 4. Total loss: 2.755. Value loss: 0.835. Policy accuracy: 0.185
2025-01-29 03:41:13,498 (Training) Epoch: 5. Total loss: 2.703. Value loss: 0.809. Policy accuracy: 0.194
2025-01-29 03:41:15,768 (Training) Epoch: 6. Total loss: 2.641. Value loss: 0.773. Policy accuracy: 0.208
2025-01-29 03:41:18,058 (Training) Epoch: 7. Total loss: 2.576. Value loss: 0.725. Policy accuracy: 0.211
2025-01-29 03:41:20,424 (Training) Epoch: 8. Total loss: 2.501. Value loss: 0.667. Policy accuracy: 0.233
2025-01-29 03:41:22,833 (Training) Epoch: 9. Total loss: 2.423. Value loss: 0.614. Policy accuracy: 0.249
2025-01-29 03:41:25,233 (Training) Epoch: 10. Total loss: 2.350. Value loss: 0.567. Policy accuracy: 0.261
2025-01-29 03:49:58,864 (Evaluation) Win rate: 0.58
2025-01-29 03:49:58,864 (Evaluation) Model 1 wins: 17. Draws: 4. Model 2 wins: 29
2025-01-29 03:49:58,873 (Evaluation) Accepting new model...
2025-01-29 03:51:29,422 (Experiment) Win rate (random): 1.0
2025-01-29 03:57:34,137 (Experiment) Win rate (alpha-beta pruning with depth 5): 0.82
2025-01-29 03:57:34,138 

2025-01-29 03:57:34,140 Iteration 2
2025-01-29 04:25:59,658 (Self-play) Number of new training examples: 9636
2025-01-29 04:25:59,658 (Self-play) Number of total training examples: 20192
2025-01-29 04:25:59,658 (Self-play) Number of total unique state-reward pairs: 15876
2025-01-29 04:25:59,658 (Self-play) Model 1 wins: 138. Draws: 10. Model 2 wins: 102
2025-01-29 04:26:06,999 (Training) Epoch: 1. Total loss: 2.500. Value loss: 0.712. Policy accuracy: 0.334
2025-01-29 04:26:11,641 (Training) Epoch: 2. Total loss: 2.424. Value loss: 0.651. Policy accuracy: 0.352
2025-01-29 04:26:16,013 (Training) Epoch: 3. Total loss: 2.367. Value loss: 0.614. Policy accuracy: 0.373
2025-01-29 04:26:20,246 (Training) Epoch: 4. Total loss: 2.316. Value loss: 0.578. Policy accuracy: 0.388
2025-01-29 04:26:24,586 (Training) Epoch: 5. Total loss: 2.271. Value loss: 0.554. Policy accuracy: 0.403
2025-01-29 04:26:28,886 (Training) Epoch: 6. Total loss: 2.270. Value loss: 0.547. Policy accuracy: 0.401
2025-01-29 04:26:33,248 (Training) Epoch: 7. Total loss: 2.222. Value loss: 0.519. Policy accuracy: 0.415
2025-01-29 04:26:37,686 (Training) Epoch: 8. Total loss: 2.200. Value loss: 0.505. Policy accuracy: 0.419
2025-01-29 04:26:42,274 (Training) Epoch: 9. Total loss: 2.165. Value loss: 0.485. Policy accuracy: 0.432
2025-01-29 04:26:46,961 (Training) Epoch: 10. Total loss: 2.146. Value loss: 0.472. Policy accuracy: 0.431
2025-01-29 04:35:26,784 (Evaluation) Win rate: 0.5
2025-01-29 04:35:26,784 (Evaluation) Model 1 wins: 20. Draws: 5. Model 2 wins: 25
2025-01-29 04:35:27,102 (Evaluation) Rejecting new model...
2025-01-29 04:37:02,795 (Experiment) Win rate (random): 1.0
2025-01-29 04:42:25,559 (Experiment) Win rate (alpha-beta pruning with depth 5): 0.92
2025-01-29 04:42:25,560 

2025-01-29 04:42:25,560 Iteration 3
2025-01-29 05:11:53,241 (Self-play) Number of new training examples: 9984
2025-01-29 05:11:53,241 (Self-play) Number of total training examples: 30176
2025-01-29 05:11:53,241 (Self-play) Number of total unique state-reward pairs: 23191
2025-01-29 05:11:53,241 (Self-play) Model 1 wins: 132. Draws: 10. Model 2 wins: 108
2025-01-29 05:12:04,510 (Training) Epoch: 1. Total loss: 2.557. Value loss: 0.770. Policy accuracy: 0.393
2025-01-29 05:12:10,891 (Training) Epoch: 2. Total loss: 2.485. Value loss: 0.720. Policy accuracy: 0.421
2025-01-29 05:12:17,464 (Training) Epoch: 3. Total loss: 2.408. Value loss: 0.664. Policy accuracy: 0.445
2025-01-29 05:12:23,953 (Training) Epoch: 4. Total loss: 2.358. Value loss: 0.626. Policy accuracy: 0.452
2025-01-29 05:12:30,650 (Training) Epoch: 5. Total loss: 2.332. Value loss: 0.601. Policy accuracy: 0.455
2025-01-29 05:12:37,139 (Training) Epoch: 6. Total loss: 2.301. Value loss: 0.584. Policy accuracy: 0.466
2025-01-29 05:12:43,440 (Training) Epoch: 7. Total loss: 2.260. Value loss: 0.557. Policy accuracy: 0.475
2025-01-29 05:12:49,973 (Training) Epoch: 8. Total loss: 2.231. Value loss: 0.536. Policy accuracy: 0.478
2025-01-29 05:12:56,488 (Training) Epoch: 9. Total loss: 2.221. Value loss: 0.529. Policy accuracy: 0.478
2025-01-29 05:13:03,087 (Training) Epoch: 10. Total loss: 2.183. Value loss: 0.506. Policy accuracy: 0.493
2025-01-29 05:21:37,168 (Evaluation) Win rate: 0.58
2025-01-29 05:21:37,169 (Evaluation) Model 1 wins: 17. Draws: 4. Model 2 wins: 29
2025-01-29 05:21:37,473 (Evaluation) Accepting new model...
2025-01-29 05:23:42,148 (Experiment) Win rate (random): 1.0
2025-01-29 05:29:01,418 (Experiment) Win rate (alpha-beta pruning with depth 5): 0.86
2025-01-29 05:29:01,418 

2025-01-29 05:29:01,429 Iteration 4
2025-01-29 06:00:23,486 (Self-play) Number of new training examples: 10952
2025-01-29 06:00:23,486 (Self-play) Number of total training examples: 41128
2025-01-29 06:00:23,486 (Self-play) Number of total unique state-reward pairs: 31301
2025-01-29 06:00:23,486 (Self-play) Model 1 wins: 132. Draws: 14. Model 2 wins: 104
2025-01-29 06:00:37,458 (Training) Epoch: 1. Total loss: 2.273. Value loss: 0.593. Policy accuracy: 0.478
2025-01-29 06:00:45,793 (Training) Epoch: 2. Total loss: 2.219. Value loss: 0.552. Policy accuracy: 0.483
2025-01-29 06:00:54,653 (Training) Epoch: 3. Total loss: 2.187. Value loss: 0.532. Policy accuracy: 0.488
2025-01-29 06:01:03,635 (Training) Epoch: 4. Total loss: 2.152. Value loss: 0.509. Policy accuracy: 0.497
2025-01-29 06:01:12,201 (Training) Epoch: 5. Total loss: 2.135. Value loss: 0.499. Policy accuracy: 0.499
2025-01-29 06:01:20,778 (Training) Epoch: 6. Total loss: 2.118. Value loss: 0.490. Policy accuracy: 0.507
2025-01-29 06:01:29,659 (Training) Epoch: 7. Total loss: 2.098. Value loss: 0.478. Policy accuracy: 0.508
2025-01-29 06:01:38,460 (Training) Epoch: 8. Total loss: 2.076. Value loss: 0.468. Policy accuracy: 0.519
2025-01-29 06:01:46,886 (Training) Epoch: 9. Total loss: 2.078. Value loss: 0.472. Policy accuracy: 0.518
2025-01-29 06:01:55,389 (Training) Epoch: 10. Total loss: 2.047. Value loss: 0.459. Policy accuracy: 0.528
2025-01-29 06:11:32,453 (Evaluation) Win rate: 0.6
2025-01-29 06:11:32,453 (Evaluation) Model 1 wins: 16. Draws: 4. Model 2 wins: 30
2025-01-29 06:11:32,761 (Evaluation) Accepting new model...
2025-01-29 06:13:18,094 (Experiment) Win rate (random): 1.0
2025-01-29 06:19:03,217 (Experiment) Win rate (alpha-beta pruning with depth 5): 0.9
2025-01-29 06:19:03,218 

2025-01-29 06:19:03,219 Iteration 5
2025-01-29 06:50:14,229 (Self-play) Number of new training examples: 11362
2025-01-29 06:50:14,229 (Self-play) Number of total training examples: 52490
2025-01-29 06:50:14,229 (Self-play) Number of total unique state-reward pairs: 39712
2025-01-29 06:50:14,229 (Self-play) Model 1 wins: 107. Draws: 11. Model 2 wins: 132
2025-01-29 06:50:33,533 (Training) Epoch: 1. Total loss: 2.134. Value loss: 0.544. Policy accuracy: 0.529
2025-01-29 06:50:44,871 (Training) Epoch: 2. Total loss: 2.094. Value loss: 0.508. Policy accuracy: 0.535
2025-01-29 06:50:55,757 (Training) Epoch: 3. Total loss: 2.075. Value loss: 0.495. Policy accuracy: 0.535
2025-01-29 06:51:06,714 (Training) Epoch: 4. Total loss: 2.054. Value loss: 0.484. Policy accuracy: 0.539
2025-01-29 06:51:18,030 (Training) Epoch: 5. Total loss: 2.025. Value loss: 0.467. Policy accuracy: 0.547
2025-01-29 06:51:28,718 (Training) Epoch: 6. Total loss: 2.023. Value loss: 0.464. Policy accuracy: 0.549
2025-01-29 06:51:39,940 (Training) Epoch: 7. Total loss: 1.997. Value loss: 0.453. Policy accuracy: 0.556
2025-01-29 06:51:51,216 (Training) Epoch: 8. Total loss: 1.978. Value loss: 0.445. Policy accuracy: 0.560
2025-01-29 06:52:02,123 (Training) Epoch: 9. Total loss: 1.980. Value loss: 0.445. Policy accuracy: 0.559
2025-01-29 06:52:13,221 (Training) Epoch: 10. Total loss: 1.954. Value loss: 0.435. Policy accuracy: 0.569
2025-01-29 07:00:35,457 (Evaluation) Win rate: 0.46
2025-01-29 07:00:35,458 (Evaluation) Model 1 wins: 20. Draws: 7. Model 2 wins: 23
2025-01-29 07:00:35,801 (Evaluation) Rejecting new model...
2025-01-29 07:02:31,587 (Experiment) Win rate (random): 1.0
2025-01-29 07:07:45,963 (Experiment) Win rate (alpha-beta pruning with depth 5): 0.94
2025-01-29 07:07:45,963 

2025-01-29 07:07:45,964 Iteration 6
2025-01-29 07:40:01,983 (Self-play) Number of new training examples: 11752
2025-01-29 07:40:01,984 (Self-play) Number of total training examples: 64242
2025-01-29 07:40:01,984 (Self-play) Number of total unique state-reward pairs: 48470
2025-01-29 07:40:01,984 (Self-play) Model 1 wins: 112. Draws: 18. Model 2 wins: 120
2025-01-29 07:40:25,905 (Training) Epoch: 1. Total loss: 2.194. Value loss: 0.588. Policy accuracy: 0.522
2025-01-29 07:40:39,204 (Training) Epoch: 2. Total loss: 2.132. Value loss: 0.545. Policy accuracy: 0.534
2025-01-29 07:40:53,043 (Training) Epoch: 3. Total loss: 2.101. Value loss: 0.525. Policy accuracy: 0.538
2025-01-29 07:41:06,369 (Training) Epoch: 4. Total loss: 2.075. Value loss: 0.506. Policy accuracy: 0.543
2025-01-29 07:41:20,122 (Training) Epoch: 5. Total loss: 2.060. Value loss: 0.493. Policy accuracy: 0.546
2025-01-29 07:41:33,605 (Training) Epoch: 6. Total loss: 2.032. Value loss: 0.478. Policy accuracy: 0.553
2025-01-29 07:41:46,730 (Training) Epoch: 7. Total loss: 2.025. Value loss: 0.477. Policy accuracy: 0.556
2025-01-29 07:42:00,543 (Training) Epoch: 8. Total loss: 1.994. Value loss: 0.463. Policy accuracy: 0.565
2025-01-29 07:42:14,077 (Training) Epoch: 9. Total loss: 1.992. Value loss: 0.460. Policy accuracy: 0.565
2025-01-29 07:42:27,733 (Training) Epoch: 10. Total loss: 1.973. Value loss: 0.450. Policy accuracy: 0.567
2025-01-29 07:51:52,346 (Evaluation) Win rate: 0.5
2025-01-29 07:51:52,346 (Evaluation) Model 1 wins: 22. Draws: 3. Model 2 wins: 25
2025-01-29 07:51:52,697 (Evaluation) Rejecting new model...
2025-01-29 07:53:32,057 (Experiment) Win rate (random): 1.0
2025-01-29 07:58:59,430 (Experiment) Win rate (alpha-beta pruning with depth 5): 0.86
2025-01-29 07:58:59,430 

2025-01-29 07:58:59,430 Iteration 7
2025-01-29 08:29:47,824 (Self-play) Number of new training examples: 11092
2025-01-29 08:29:47,825 (Self-play) Number of total training examples: 75334
2025-01-29 08:29:47,825 (Self-play) Number of total unique state-reward pairs: 56418
2025-01-29 08:29:47,825 (Self-play) Model 1 wins: 142. Draws: 14. Model 2 wins: 94
2025-01-29 08:30:14,752 (Training) Epoch: 1. Total loss: 2.215. Value loss: 0.612. Policy accuracy: 0.524
2025-01-29 08:30:30,531 (Training) Epoch: 2. Total loss: 2.151. Value loss: 0.566. Policy accuracy: 0.531
2025-01-29 08:30:46,621 (Training) Epoch: 3. Total loss: 2.116. Value loss: 0.537. Policy accuracy: 0.538
2025-01-29 08:31:02,204 (Training) Epoch: 4. Total loss: 2.090. Value loss: 0.520. Policy accuracy: 0.545
2025-01-29 08:31:17,916 (Training) Epoch: 5. Total loss: 2.076. Value loss: 0.512. Policy accuracy: 0.546
2025-01-29 08:31:33,143 (Training) Epoch: 6. Total loss: 2.047. Value loss: 0.494. Policy accuracy: 0.555
2025-01-29 08:31:49,467 (Training) Epoch: 7. Total loss: 2.032. Value loss: 0.488. Policy accuracy: 0.560
2025-01-29 08:32:04,963 (Training) Epoch: 8. Total loss: 2.021. Value loss: 0.481. Policy accuracy: 0.562
2025-01-29 08:32:20,966 (Training) Epoch: 9. Total loss: 2.004. Value loss: 0.472. Policy accuracy: 0.566
2025-01-29 08:32:36,488 (Training) Epoch: 10. Total loss: 1.980. Value loss: 0.460. Policy accuracy: 0.574
2025-01-29 08:41:30,787 (Evaluation) Win rate: 0.62
2025-01-29 08:41:30,787 (Evaluation) Model 1 wins: 16. Draws: 3. Model 2 wins: 31
2025-01-29 08:41:31,096 (Evaluation) Accepting new model...
2025-01-29 08:43:16,812 (Experiment) Win rate (random): 1.0
2025-01-29 08:48:17,727 (Experiment) Win rate (alpha-beta pruning with depth 5): 1.0
2025-01-29 08:48:17,728 

2025-01-29 08:48:17,728 Iteration 8
2025-01-29 09:21:42,599 (Self-play) Number of new training examples: 12374
2025-01-29 09:21:42,599 (Self-play) Number of total training examples: 87708
2025-01-29 09:21:42,599 (Self-play) Number of total unique state-reward pairs: 65592
2025-01-29 09:21:42,599 (Self-play) Model 1 wins: 125. Draws: 11. Model 2 wins: 114
2025-01-29 09:22:16,008 (Training) Epoch: 1. Total loss: 2.020. Value loss: 0.506. Policy accuracy: 0.580
2025-01-29 09:22:34,120 (Training) Epoch: 2. Total loss: 1.995. Value loss: 0.485. Policy accuracy: 0.579
2025-01-29 09:22:52,573 (Training) Epoch: 3. Total loss: 1.968. Value loss: 0.467. Policy accuracy: 0.583
2025-01-29 09:23:10,549 (Training) Epoch: 4. Total loss: 1.954. Value loss: 0.461. Policy accuracy: 0.586
2025-01-29 09:23:29,308 (Training) Epoch: 5. Total loss: 1.939. Value loss: 0.452. Policy accuracy: 0.589
2025-01-29 09:23:47,190 (Training) Epoch: 6. Total loss: 1.927. Value loss: 0.446. Policy accuracy: 0.594
2025-01-29 09:24:05,593 (Training) Epoch: 7. Total loss: 1.915. Value loss: 0.445. Policy accuracy: 0.600
2025-01-29 09:24:23,383 (Training) Epoch: 8. Total loss: 1.905. Value loss: 0.435. Policy accuracy: 0.600
2025-01-29 09:24:42,099 (Training) Epoch: 9. Total loss: 1.890. Value loss: 0.429. Policy accuracy: 0.602
2025-01-29 09:25:00,141 (Training) Epoch: 10. Total loss: 1.884. Value loss: 0.425. Policy accuracy: 0.604
2025-01-29 09:33:11,174 (Evaluation) Win rate: 0.6
2025-01-29 09:33:11,174 (Evaluation) Model 1 wins: 19. Draws: 1. Model 2 wins: 30
2025-01-29 09:33:11,471 (Evaluation) Accepting new model...
2025-01-29 09:34:50,541 (Experiment) Win rate (random): 1.0
2025-01-29 09:40:04,796 (Experiment) Win rate (alpha-beta pruning with depth 5): 0.98
2025-01-29 09:40:04,796 

2025-01-29 09:40:04,797 Iteration 9
2025-01-29 10:12:00,673 (Self-play) Number of new training examples: 12024
2025-01-29 10:12:00,674 (Self-play) Number of total training examples: 99732
2025-01-29 10:12:00,674 (Self-play) Number of total unique state-reward pairs: 74382
2025-01-29 10:12:00,674 (Self-play) Model 1 wins: 128. Draws: 12. Model 2 wins: 110
2025-01-29 10:12:37,787 (Training) Epoch: 1. Total loss: 1.928. Value loss: 0.472. Policy accuracy: 0.605
2025-01-29 10:12:58,729 (Training) Epoch: 2. Total loss: 1.912. Value loss: 0.457. Policy accuracy: 0.606
2025-01-29 10:13:19,533 (Training) Epoch: 3. Total loss: 1.889. Value loss: 0.446. Policy accuracy: 0.611
2025-01-29 10:13:40,483 (Training) Epoch: 4. Total loss: 1.879. Value loss: 0.439. Policy accuracy: 0.615
2025-01-29 10:14:01,274 (Training) Epoch: 5. Total loss: 1.869. Value loss: 0.433. Policy accuracy: 0.616
2025-01-29 10:14:21,978 (Training) Epoch: 6. Total loss: 1.853. Value loss: 0.429. Policy accuracy: 0.618
2025-01-29 10:14:42,943 (Training) Epoch: 7. Total loss: 1.842. Value loss: 0.421. Policy accuracy: 0.620
2025-01-29 10:15:03,184 (Training) Epoch: 8. Total loss: 1.838. Value loss: 0.420. Policy accuracy: 0.624
2025-01-29 10:15:24,350 (Training) Epoch: 9. Total loss: 1.824. Value loss: 0.414. Policy accuracy: 0.624
2025-01-29 10:15:44,819 (Training) Epoch: 10. Total loss: 1.820. Value loss: 0.412. Policy accuracy: 0.626
2025-01-29 10:24:30,883 (Evaluation) Win rate: 0.54
2025-01-29 10:24:30,883 (Evaluation) Model 1 wins: 11. Draws: 12. Model 2 wins: 27
2025-01-29 10:24:31,172 (Evaluation) Rejecting new model...
2025-01-29 10:26:24,429 (Experiment) Win rate (random): 1.0
2025-01-29 10:31:24,615 (Experiment) Win rate (alpha-beta pruning with depth 5): 1.0
2025-01-29 10:31:24,615 

2025-01-29 10:31:24,618 Iteration 10
2025-01-29 11:03:02,763 (Self-play) Number of new training examples: 11566
2025-01-29 11:03:02,763 (Self-play) Number of total training examples: 111298
2025-01-29 11:03:02,764 (Self-play) Number of total unique state-reward pairs: 82603
2025-01-29 11:03:02,764 (Self-play) Model 1 wins: 123. Draws: 16. Model 2 wins: 111
2025-01-29 11:03:42,066 (Training) Epoch: 1. Total loss: 1.979. Value loss: 0.510. Policy accuracy: 0.604
2025-01-29 11:04:05,950 (Training) Epoch: 2. Total loss: 1.954. Value loss: 0.487. Policy accuracy: 0.600
2025-01-29 11:04:29,031 (Training) Epoch: 3. Total loss: 1.928. Value loss: 0.471. Policy accuracy: 0.605
2025-01-29 11:04:51,653 (Training) Epoch: 4. Total loss: 1.910. Value loss: 0.461. Policy accuracy: 0.612
2025-01-29 11:05:15,007 (Training) Epoch: 5. Total loss: 1.894. Value loss: 0.453. Policy accuracy: 0.614
2025-01-29 11:05:38,157 (Training) Epoch: 6. Total loss: 1.882. Value loss: 0.445. Policy accuracy: 0.616
2025-01-29 11:06:00,868 (Training) Epoch: 7. Total loss: 1.874. Value loss: 0.440. Policy accuracy: 0.620
2025-01-29 11:06:24,522 (Training) Epoch: 8. Total loss: 1.866. Value loss: 0.439. Policy accuracy: 0.623
2025-01-29 11:06:47,837 (Training) Epoch: 9. Total loss: 1.850. Value loss: 0.431. Policy accuracy: 0.624
2025-01-29 11:07:10,850 (Training) Epoch: 10. Total loss: 1.835. Value loss: 0.423. Policy accuracy: 0.627
2025-01-29 11:16:22,088 (Evaluation) Win rate: 0.36
2025-01-29 11:16:22,088 (Evaluation) Model 1 wins: 17. Draws: 15. Model 2 wins: 18
2025-01-29 11:16:22,408 (Evaluation) Rejecting new model...
2025-01-29 11:18:14,982 (Experiment) Win rate (random): 1.0
2025-01-29 11:23:49,861 (Experiment) Win rate (alpha-beta pruning with depth 5): 0.94
2025-01-29 11:23:49,862 

2025-01-29 11:23:49,862 Iteration 11
2025-01-29 11:55:34,249 (Self-play) Number of new training examples: 11788
2025-01-29 11:55:34,249 (Self-play) Number of total training examples: 123086
2025-01-29 11:55:34,249 (Self-play) Number of total unique state-reward pairs: 91025
2025-01-29 11:55:34,249 (Self-play) Model 1 wins: 122. Draws: 27. Model 2 wins: 101
2025-01-29 11:56:19,055 (Training) Epoch: 1. Total loss: 2.003. Value loss: 0.532. Policy accuracy: 0.601
2025-01-29 11:56:45,174 (Training) Epoch: 2. Total loss: 1.965. Value loss: 0.502. Policy accuracy: 0.607
2025-01-29 11:57:11,268 (Training) Epoch: 3. Total loss: 1.943. Value loss: 0.486. Policy accuracy: 0.607
2025-01-29 11:57:36,855 (Training) Epoch: 4. Total loss: 1.926. Value loss: 0.477. Policy accuracy: 0.611
2025-01-29 11:58:02,712 (Training) Epoch: 5. Total loss: 1.908. Value loss: 0.462. Policy accuracy: 0.616
2025-01-29 11:58:29,163 (Training) Epoch: 6. Total loss: 1.892. Value loss: 0.456. Policy accuracy: 0.616
2025-01-29 11:58:54,829 (Training) Epoch: 7. Total loss: 1.883. Value loss: 0.452. Policy accuracy: 0.622
2025-01-29 11:59:20,657 (Training) Epoch: 8. Total loss: 1.869. Value loss: 0.447. Policy accuracy: 0.625
2025-01-29 11:59:46,658 (Training) Epoch: 9. Total loss: 1.863. Value loss: 0.442. Policy accuracy: 0.624
2025-01-29 12:00:12,778 (Training) Epoch: 10. Total loss: 1.857. Value loss: 0.439. Policy accuracy: 0.626
2025-01-29 12:09:28,377 (Evaluation) Win rate: 0.52
2025-01-29 12:09:28,377 (Evaluation) Model 1 wins: 18. Draws: 6. Model 2 wins: 26
2025-01-29 12:09:28,705 (Evaluation) Rejecting new model...
2025-01-29 12:11:22,348 (Experiment) Win rate (random): 1.0
2025-01-29 12:16:40,578 (Experiment) Win rate (alpha-beta pruning with depth 5): 0.96
2025-01-29 12:16:40,578 

2025-01-29 12:16:40,579 Iteration 12
2025-01-29 12:50:00,906 (Self-play) Number of new training examples: 12252
2025-01-29 12:50:00,906 (Self-play) Number of total training examples: 135338
2025-01-29 12:50:00,906 (Self-play) Number of total unique state-reward pairs: 99882
2025-01-29 12:50:00,906 (Self-play) Model 1 wins: 126. Draws: 17. Model 2 wins: 107
2025-01-29 12:50:52,768 (Training) Epoch: 1. Total loss: 2.024. Value loss: 0.551. Policy accuracy: 0.598
2025-01-29 12:51:21,085 (Training) Epoch: 2. Total loss: 1.994. Value loss: 0.525. Policy accuracy: 0.603
2025-01-29 12:51:49,513 (Training) Epoch: 3. Total loss: 1.959. Value loss: 0.500. Policy accuracy: 0.608
2025-01-29 12:52:18,302 (Training) Epoch: 4. Total loss: 1.935. Value loss: 0.486. Policy accuracy: 0.614
2025-01-29 12:52:46,883 (Training) Epoch: 5. Total loss: 1.918. Value loss: 0.478. Policy accuracy: 0.615
2025-01-29 12:53:15,671 (Training) Epoch: 6. Total loss: 1.903. Value loss: 0.467. Policy accuracy: 0.618
2025-01-29 12:53:44,220 (Training) Epoch: 7. Total loss: 1.892. Value loss: 0.462. Policy accuracy: 0.622
2025-01-29 12:54:12,993 (Training) Epoch: 8. Total loss: 1.883. Value loss: 0.456. Policy accuracy: 0.625
2025-01-29 12:54:41,207 (Training) Epoch: 9. Total loss: 1.864. Value loss: 0.448. Policy accuracy: 0.627
2025-01-29 12:55:09,726 (Training) Epoch: 10. Total loss: 1.856. Value loss: 0.442. Policy accuracy: 0.630
2025-01-29 13:03:55,894 (Evaluation) Win rate: 0.26
2025-01-29 13:03:55,894 (Evaluation) Model 1 wins: 33. Draws: 4. Model 2 wins: 13
2025-01-29 13:03:56,230 (Evaluation) Rejecting new model...
2025-01-29 13:05:40,483 (Experiment) Win rate (random): 1.0
2025-01-29 13:10:32,495 (Experiment) Win rate (alpha-beta pruning with depth 5): 1.0
2025-01-29 13:10:32,495 

2025-01-29 13:10:32,496 Iteration 13
2025-01-29 13:43:58,365 (Self-play) Number of new training examples: 12032
2025-01-29 13:43:58,366 (Self-play) Number of total training examples: 147370
2025-01-29 13:43:58,366 (Self-play) Number of total unique state-reward pairs: 108509
2025-01-29 13:43:58,366 (Self-play) Model 1 wins: 126. Draws: 17. Model 2 wins: 107
2025-01-29 13:44:54,378 (Training) Epoch: 1. Total loss: 2.040. Value loss: 0.567. Policy accuracy: 0.599
2025-01-29 13:45:25,353 (Training) Epoch: 2. Total loss: 2.000. Value loss: 0.532. Policy accuracy: 0.602
2025-01-29 13:45:56,965 (Training) Epoch: 3. Total loss: 1.973. Value loss: 0.515. Policy accuracy: 0.609
2025-01-29 13:46:27,937 (Training) Epoch: 4. Total loss: 1.949. Value loss: 0.498. Policy accuracy: 0.614
2025-01-29 13:46:59,238 (Training) Epoch: 5. Total loss: 1.933. Value loss: 0.490. Policy accuracy: 0.616
2025-01-29 13:47:29,950 (Training) Epoch: 6. Total loss: 1.914. Value loss: 0.477. Policy accuracy: 0.619
2025-01-29 13:48:01,173 (Training) Epoch: 7. Total loss: 1.905. Value loss: 0.473. Policy accuracy: 0.623
2025-01-29 13:48:32,207 (Training) Epoch: 8. Total loss: 1.887. Value loss: 0.463. Policy accuracy: 0.627
2025-01-29 13:49:03,451 (Training) Epoch: 9. Total loss: 1.878. Value loss: 0.455. Policy accuracy: 0.626
2025-01-29 13:49:34,613 (Training) Epoch: 10. Total loss: 1.864. Value loss: 0.450. Policy accuracy: 0.629
2025-01-29 13:58:10,880 (Evaluation) Win rate: 0.76
2025-01-29 13:58:10,880 (Evaluation) Model 1 wins: 8. Draws: 4. Model 2 wins: 38
2025-01-29 13:58:11,179 (Evaluation) Accepting new model...
2025-01-29 14:00:06,397 (Experiment) Win rate (random): 1.0
2025-01-29 14:05:11,979 (Experiment) Win rate (alpha-beta pruning with depth 5): 0.94
2025-01-29 14:05:11,979 

2025-01-29 14:05:11,980 Iteration 14
2025-01-29 14:37:43,277 (Self-play) Number of new training examples: 12012
2025-01-29 14:37:43,278 (Self-play) Number of total training examples: 159382
2025-01-29 14:37:43,278 (Self-play) Number of total unique state-reward pairs: 117122
2025-01-29 14:37:43,278 (Self-play) Model 1 wins: 122. Draws: 18. Model 2 wins: 110
2025-01-29 14:38:42,460 (Training) Epoch: 1. Total loss: 1.880. Value loss: 0.467. Policy accuracy: 0.632
2025-01-29 14:39:17,048 (Training) Epoch: 2. Total loss: 1.866. Value loss: 0.455. Policy accuracy: 0.636
2025-01-29 14:39:51,145 (Training) Epoch: 3. Total loss: 1.854. Value loss: 0.450. Policy accuracy: 0.638
2025-01-29 14:40:25,936 (Training) Epoch: 4. Total loss: 1.844. Value loss: 0.445. Policy accuracy: 0.641
2025-01-29 14:41:00,346 (Training) Epoch: 5. Total loss: 1.830. Value loss: 0.438. Policy accuracy: 0.643
2025-01-29 14:41:35,191 (Training) Epoch: 6. Total loss: 1.822. Value loss: 0.436. Policy accuracy: 0.645
2025-01-29 14:42:09,866 (Training) Epoch: 7. Total loss: 1.811. Value loss: 0.429. Policy accuracy: 0.647
2025-01-29 14:42:44,323 (Training) Epoch: 8. Total loss: 1.808. Value loss: 0.428. Policy accuracy: 0.648
2025-01-29 14:43:18,609 (Training) Epoch: 9. Total loss: 1.797. Value loss: 0.422. Policy accuracy: 0.651
2025-01-29 14:43:52,998 (Training) Epoch: 10. Total loss: 1.791. Value loss: 0.418. Policy accuracy: 0.652
2025-01-29 14:52:53,822 (Evaluation) Win rate: 0.52
2025-01-29 14:52:53,822 (Evaluation) Model 1 wins: 18. Draws: 6. Model 2 wins: 26
2025-01-29 14:52:54,149 (Evaluation) Rejecting new model...
2025-01-29 14:54:45,322 (Experiment) Win rate (random): 1.0
2025-01-29 14:59:46,162 (Experiment) Win rate (alpha-beta pruning with depth 5): 1.0
2025-01-29 14:59:46,162 

2025-01-29 14:59:46,163 Iteration 15
2025-01-29 15:33:53,057 (Self-play) Number of new training examples: 12606
2025-01-29 15:33:53,057 (Self-play) Number of total training examples: 171988
2025-01-29 15:33:53,058 (Self-play) Number of total unique state-reward pairs: 126240
2025-01-29 15:33:53,058 (Self-play) Model 1 wins: 105. Draws: 19. Model 2 wins: 126
2025-01-29 15:34:59,904 (Training) Epoch: 1. Total loss: 1.900. Value loss: 0.488. Policy accuracy: 0.634
2025-01-29 15:35:36,831 (Training) Epoch: 2. Total loss: 1.887. Value loss: 0.474. Policy accuracy: 0.634
2025-01-29 15:36:14,076 (Training) Epoch: 3. Total loss: 1.861. Value loss: 0.460. Policy accuracy: 0.639
2025-01-29 15:36:50,696 (Training) Epoch: 4. Total loss: 1.855. Value loss: 0.455. Policy accuracy: 0.640
2025-01-29 15:37:28,123 (Training) Epoch: 5. Total loss: 1.842. Value loss: 0.450. Policy accuracy: 0.642
2025-01-29 15:38:04,823 (Training) Epoch: 6. Total loss: 1.834. Value loss: 0.444. Policy accuracy: 0.645
2025-01-29 15:38:42,208 (Training) Epoch: 7. Total loss: 1.825. Value loss: 0.439. Policy accuracy: 0.646
2025-01-29 15:39:19,146 (Training) Epoch: 8. Total loss: 1.815. Value loss: 0.434. Policy accuracy: 0.649
2025-01-29 15:39:56,742 (Training) Epoch: 9. Total loss: 1.805. Value loss: 0.432. Policy accuracy: 0.652
2025-01-29 15:40:34,017 (Training) Epoch: 10. Total loss: 1.800. Value loss: 0.428. Policy accuracy: 0.651
2025-01-29 15:49:46,156 (Evaluation) Win rate: 0.44
2025-01-29 15:49:46,157 (Evaluation) Model 1 wins: 21. Draws: 7. Model 2 wins: 22
2025-01-29 15:49:46,475 (Evaluation) Rejecting new model...
2025-01-29 15:51:34,495 (Experiment) Win rate (random): 1.0
2025-01-29 15:56:47,431 (Experiment) Win rate (alpha-beta pruning with depth 5): 1.0
2025-01-29 15:56:47,432 

2025-01-29 15:56:47,433 Iteration 16
2025-01-29 16:29:34,967 (Self-play) Number of new training examples: 12274
2025-01-29 16:29:34,968 (Self-play) Number of total training examples: 184262
2025-01-29 16:29:34,968 (Self-play) Number of total unique state-reward pairs: 134922
2025-01-29 16:29:34,968 (Self-play) Model 1 wins: 119. Draws: 21. Model 2 wins: 110
2025-01-29 16:30:45,316 (Training) Epoch: 1. Total loss: 1.927. Value loss: 0.508. Policy accuracy: 0.632
2025-01-29 16:31:24,603 (Training) Epoch: 2. Total loss: 1.902. Value loss: 0.488. Policy accuracy: 0.637
2025-01-29 16:32:03,377 (Training) Epoch: 3. Total loss: 1.877. Value loss: 0.475. Policy accuracy: 0.641
2025-01-29 16:32:42,568 (Training) Epoch: 4. Total loss: 1.868. Value loss: 0.467. Policy accuracy: 0.641
2025-01-29 16:33:21,484 (Training) Epoch: 5. Total loss: 1.853. Value loss: 0.460. Policy accuracy: 0.646
2025-01-29 16:34:01,333 (Training) Epoch: 6. Total loss: 1.843. Value loss: 0.452. Policy accuracy: 0.648
2025-01-29 16:34:41,236 (Training) Epoch: 7. Total loss: 1.834. Value loss: 0.446. Policy accuracy: 0.648
2025-01-29 16:35:20,690 (Training) Epoch: 8. Total loss: 1.822. Value loss: 0.441. Policy accuracy: 0.652
2025-01-29 16:36:00,229 (Training) Epoch: 9. Total loss: 1.818. Value loss: 0.440. Policy accuracy: 0.654
2025-01-29 16:36:39,021 (Training) Epoch: 10. Total loss: 1.804. Value loss: 0.434. Policy accuracy: 0.657
2025-01-29 16:45:11,128 (Evaluation) Win rate: 0.34
2025-01-29 16:45:11,128 (Evaluation) Model 1 wins: 24. Draws: 9. Model 2 wins: 17
2025-01-29 16:45:11,476 (Evaluation) Rejecting new model...
2025-01-29 16:46:54,067 (Experiment) Win rate (random): 1.0
2025-01-29 16:51:39,756 (Experiment) Win rate (alpha-beta pruning with depth 5): 1.0
2025-01-29 16:51:39,756 

2025-01-29 16:51:39,757 Iteration 17
2025-01-29 17:26:37,336 (Self-play) Number of new training examples: 12220
2025-01-29 17:26:37,336 (Self-play) Number of total training examples: 196482
2025-01-29 17:26:37,336 (Self-play) Number of total unique state-reward pairs: 143523
2025-01-29 17:26:37,336 (Self-play) Model 1 wins: 114. Draws: 17. Model 2 wins: 119
2025-01-29 17:27:54,372 (Training) Epoch: 1. Total loss: 1.943. Value loss: 0.519. Policy accuracy: 0.633
2025-01-29 17:28:38,268 (Training) Epoch: 2. Total loss: 1.911. Value loss: 0.497. Policy accuracy: 0.639
2025-01-29 17:29:20,724 (Training) Epoch: 3. Total loss: 1.892. Value loss: 0.482. Policy accuracy: 0.641
2025-01-29 17:30:03,376 (Training) Epoch: 4. Total loss: 1.877. Value loss: 0.471. Policy accuracy: 0.644
2025-01-29 17:30:45,872 (Training) Epoch: 5. Total loss: 1.867. Value loss: 0.467. Policy accuracy: 0.646
2025-01-29 17:31:29,088 (Training) Epoch: 6. Total loss: 1.854. Value loss: 0.458. Policy accuracy: 0.648
2025-01-29 17:32:11,839 (Training) Epoch: 7. Total loss: 1.841. Value loss: 0.451. Policy accuracy: 0.650
2025-01-29 17:32:53,555 (Training) Epoch: 8. Total loss: 1.832. Value loss: 0.448. Policy accuracy: 0.653
2025-01-29 17:33:35,898 (Training) Epoch: 9. Total loss: 1.820. Value loss: 0.444. Policy accuracy: 0.656
2025-01-29 17:34:18,143 (Training) Epoch: 10. Total loss: 1.812. Value loss: 0.439. Policy accuracy: 0.659
2025-01-29 17:43:11,614 (Evaluation) Win rate: 0.8
2025-01-29 17:43:11,614 (Evaluation) Model 1 wins: 4. Draws: 6. Model 2 wins: 40
2025-01-29 17:43:11,965 (Evaluation) Accepting new model...
2025-01-29 17:44:58,022 (Experiment) Win rate (random): 1.0
2025-01-29 17:50:05,484 (Experiment) Win rate (alpha-beta pruning with depth 5): 0.96
2025-01-29 17:50:05,484 

2025-01-29 17:50:05,485 Iteration 18
2025-01-29 18:24:19,600 (Self-play) Number of new training examples: 12644
2025-01-29 18:24:19,600 (Self-play) Number of total training examples: 209126
2025-01-29 18:24:19,600 (Self-play) Number of total unique state-reward pairs: 152511
2025-01-29 18:24:19,600 (Self-play) Model 1 wins: 112. Draws: 23. Model 2 wins: 115
2025-01-29 18:25:40,831 (Training) Epoch: 1. Total loss: 1.827. Value loss: 0.453. Policy accuracy: 0.659
2025-01-29 18:26:25,904 (Training) Epoch: 2. Total loss: 1.810. Value loss: 0.443. Policy accuracy: 0.661
2025-01-29 18:27:10,317 (Training) Epoch: 3. Total loss: 1.802. Value loss: 0.440. Policy accuracy: 0.661
2025-01-29 18:27:55,584 (Training) Epoch: 4. Total loss: 1.793. Value loss: 0.435. Policy accuracy: 0.665
2025-01-29 18:28:41,010 (Training) Epoch: 5. Total loss: 1.786. Value loss: 0.430. Policy accuracy: 0.666
2025-01-29 18:29:25,344 (Training) Epoch: 6. Total loss: 1.775. Value loss: 0.425. Policy accuracy: 0.667
2025-01-29 18:30:11,010 (Training) Epoch: 7. Total loss: 1.768. Value loss: 0.423. Policy accuracy: 0.669
2025-01-29 18:30:55,956 (Training) Epoch: 8. Total loss: 1.768. Value loss: 0.421. Policy accuracy: 0.671
2025-01-29 18:31:40,288 (Training) Epoch: 9. Total loss: 1.763. Value loss: 0.419. Policy accuracy: 0.674
2025-01-29 18:32:25,177 (Training) Epoch: 10. Total loss: 1.750. Value loss: 0.415. Policy accuracy: 0.674
2025-01-29 18:42:13,959 (Evaluation) Win rate: 0.46
2025-01-29 18:42:13,959 (Evaluation) Model 1 wins: 17. Draws: 10. Model 2 wins: 23
2025-01-29 18:42:14,326 (Evaluation) Rejecting new model...
2025-01-29 18:44:02,059 (Experiment) Win rate (random): 1.0
2025-01-29 18:48:56,450 (Experiment) Win rate (alpha-beta pruning with depth 5): 0.98
2025-01-29 18:48:56,450 

2025-01-29 18:48:56,451 Iteration 19
2025-01-29 19:23:14,786 (Self-play) Number of new training examples: 12374
2025-01-29 19:23:14,786 (Self-play) Number of total training examples: 221500
2025-01-29 19:23:14,786 (Self-play) Number of total unique state-reward pairs: 161137
2025-01-29 19:23:14,786 (Self-play) Model 1 wins: 139. Draws: 19. Model 2 wins: 92
2025-01-29 19:24:47,540 (Training) Epoch: 1. Total loss: 1.843. Value loss: 0.471. Policy accuracy: 0.660
2025-01-29 19:25:39,072 (Training) Epoch: 2. Total loss: 1.824. Value loss: 0.456. Policy accuracy: 0.661
2025-01-29 19:26:30,334 (Training) Epoch: 3. Total loss: 1.815. Value loss: 0.450. Policy accuracy: 0.664
2025-01-29 19:27:21,888 (Training) Epoch: 4. Total loss: 1.804. Value loss: 0.443. Policy accuracy: 0.665
2025-01-29 19:28:13,538 (Training) Epoch: 5. Total loss: 1.791. Value loss: 0.434. Policy accuracy: 0.665
2025-01-29 19:29:04,470 (Training) Epoch: 6. Total loss: 1.785. Value loss: 0.432. Policy accuracy: 0.668
2025-01-29 19:29:57,188 (Training) Epoch: 7. Total loss: 1.783. Value loss: 0.432. Policy accuracy: 0.672
2025-01-29 19:30:47,433 (Training) Epoch: 8. Total loss: 1.769. Value loss: 0.428. Policy accuracy: 0.675
2025-01-29 19:31:38,153 (Training) Epoch: 9. Total loss: 1.768. Value loss: 0.426. Policy accuracy: 0.674
2025-01-29 19:32:29,221 (Training) Epoch: 10. Total loss: 1.760. Value loss: 0.421. Policy accuracy: 0.675
2025-01-29 19:42:04,146 (Evaluation) Win rate: 0.48
2025-01-29 19:42:04,146 (Evaluation) Model 1 wins: 18. Draws: 8. Model 2 wins: 24
2025-01-29 19:42:04,541 (Evaluation) Rejecting new model...
2025-01-29 19:43:41,692 (Experiment) Win rate (random): 1.0
2025-01-29 19:48:40,763 (Experiment) Win rate (alpha-beta pruning with depth 5): 0.98
2025-01-29 19:48:40,764 

2025-01-29 19:48:40,764 Iteration 20
2025-01-29 20:23:04,940 (Self-play) Number of new training examples: 12624
2025-01-29 20:23:04,940 (Self-play) Number of total training examples: 234124
2025-01-29 20:23:04,940 (Self-play) Number of total unique state-reward pairs: 170039
2025-01-29 20:23:04,940 (Self-play) Model 1 wins: 136. Draws: 22. Model 2 wins: 92
2025-01-29 20:24:37,590 (Training) Epoch: 1. Total loss: 1.861. Value loss: 0.483. Policy accuracy: 0.659
2025-01-29 20:25:30,589 (Training) Epoch: 2. Total loss: 1.836. Value loss: 0.464. Policy accuracy: 0.662
2025-01-29 20:26:22,254 (Training) Epoch: 3. Total loss: 1.822. Value loss: 0.456. Policy accuracy: 0.664
2025-01-29 20:27:13,911 (Training) Epoch: 4. Total loss: 1.814. Value loss: 0.451. Policy accuracy: 0.664
2025-01-29 20:28:04,975 (Training) Epoch: 5. Total loss: 1.805. Value loss: 0.446. Policy accuracy: 0.669
2025-01-29 20:28:57,396 (Training) Epoch: 6. Total loss: 1.790. Value loss: 0.441. Policy accuracy: 0.670
2025-01-29 20:29:48,113 (Training) Epoch: 7. Total loss: 1.782. Value loss: 0.435. Policy accuracy: 0.670
2025-01-29 20:30:40,234 (Training) Epoch: 8. Total loss: 1.777. Value loss: 0.433. Policy accuracy: 0.675
2025-01-29 20:31:31,201 (Training) Epoch: 9. Total loss: 1.772. Value loss: 0.430. Policy accuracy: 0.676
2025-01-29 20:32:23,265 (Training) Epoch: 10. Total loss: 1.765. Value loss: 0.427. Policy accuracy: 0.678
2025-01-29 20:41:43,020 (Evaluation) Win rate: 0.28
2025-01-29 20:41:43,020 (Evaluation) Model 1 wins: 32. Draws: 4. Model 2 wins: 14
2025-01-29 20:41:43,345 (Evaluation) Rejecting new model...
2025-01-29 20:43:24,309 (Experiment) Win rate (random): 1.0
2025-01-29 20:48:07,400 (Experiment) Win rate (alpha-beta pruning with depth 5): 1.0
2025-01-29 20:48:07,400 

2025-01-29 20:48:07,401 Iteration 21
2025-01-29 21:23:13,905 (Self-play) Number of new training examples: 12806
2025-01-29 21:23:13,905 (Self-play) Number of total training examples: 246930
2025-01-29 21:23:13,905 (Self-play) Number of total unique state-reward pairs: 179141
2025-01-29 21:23:13,905 (Self-play) Model 1 wins: 121. Draws: 21. Model 2 wins: 108
2025-01-29 21:24:49,603 (Training) Epoch: 1. Total loss: 1.871. Value loss: 0.495. Policy accuracy: 0.660
2025-01-29 21:25:46,818 (Training) Epoch: 2. Total loss: 1.848. Value loss: 0.476. Policy accuracy: 0.662
2025-01-29 21:26:43,559 (Training) Epoch: 3. Total loss: 1.832. Value loss: 0.467. Policy accuracy: 0.664
2025-01-29 21:27:40,037 (Training) Epoch: 4. Total loss: 1.818. Value loss: 0.458. Policy accuracy: 0.666
2025-01-29 21:28:37,017 (Training) Epoch: 5. Total loss: 1.810. Value loss: 0.453. Policy accuracy: 0.669
2025-01-29 21:29:33,359 (Training) Epoch: 6. Total loss: 1.801. Value loss: 0.447. Policy accuracy: 0.669
2025-01-29 21:30:31,363 (Training) Epoch: 7. Total loss: 1.788. Value loss: 0.442. Policy accuracy: 0.673
2025-01-29 21:31:27,747 (Training) Epoch: 8. Total loss: 1.787. Value loss: 0.441. Policy accuracy: 0.675
2025-01-29 21:32:24,426 (Training) Epoch: 9. Total loss: 1.773. Value loss: 0.435. Policy accuracy: 0.677
2025-01-29 21:33:20,844 (Training) Epoch: 10. Total loss: 1.771. Value loss: 0.433. Policy accuracy: 0.677
2025-01-29 21:42:33,918 (Evaluation) Win rate: 0.42
2025-01-29 21:42:33,918 (Evaluation) Model 1 wins: 24. Draws: 5. Model 2 wins: 21
2025-01-29 21:42:34,261 (Evaluation) Rejecting new model...
2025-01-29 21:44:25,919 (Experiment) Win rate (random): 1.0
2025-01-29 21:49:13,782 (Experiment) Win rate (alpha-beta pruning with depth 5): 0.98
2025-01-29 21:49:13,782 

2025-01-29 21:49:13,783 Iteration 22
2025-01-29 22:24:45,576 (Self-play) Number of new training examples: 13198
2025-01-29 22:24:45,577 (Self-play) Number of total training examples: 260128
2025-01-29 22:24:45,577 (Self-play) Number of total unique state-reward pairs: 188469
2025-01-29 22:24:45,577 (Self-play) Model 1 wins: 115. Draws: 20. Model 2 wins: 115
2025-01-29 22:26:32,438 (Training) Epoch: 1. Total loss: 1.877. Value loss: 0.503. Policy accuracy: 0.661
2025-01-29 22:27:32,928 (Training) Epoch: 2. Total loss: 1.853. Value loss: 0.485. Policy accuracy: 0.664
2025-01-29 22:28:32,074 (Training) Epoch: 3. Total loss: 1.838. Value loss: 0.474. Policy accuracy: 0.666
2025-01-29 22:29:31,549 (Training) Epoch: 4. Total loss: 1.821. Value loss: 0.464. Policy accuracy: 0.668
2025-01-29 22:30:31,391 (Training) Epoch: 5. Total loss: 1.812. Value loss: 0.460. Policy accuracy: 0.671
2025-01-29 22:31:30,452 (Training) Epoch: 6. Total loss: 1.804. Value loss: 0.455. Policy accuracy: 0.672
2025-01-29 22:32:28,728 (Training) Epoch: 7. Total loss: 1.794. Value loss: 0.449. Policy accuracy: 0.674
2025-01-29 22:33:28,751 (Training) Epoch: 8. Total loss: 1.784. Value loss: 0.444. Policy accuracy: 0.677
2025-01-29 22:34:28,701 (Training) Epoch: 9. Total loss: 1.780. Value loss: 0.441. Policy accuracy: 0.678
2025-01-29 22:35:27,776 (Training) Epoch: 10. Total loss: 1.769. Value loss: 0.436. Policy accuracy: 0.681
2025-01-29 22:44:39,905 (Evaluation) Win rate: 0.38
2025-01-29 22:44:39,905 (Evaluation) Model 1 wins: 18. Draws: 13. Model 2 wins: 19
2025-01-29 22:44:40,260 (Evaluation) Rejecting new model...
2025-01-29 22:46:26,857 (Experiment) Win rate (random): 1.0
2025-01-29 22:51:18,196 (Experiment) Win rate (alpha-beta pruning with depth 5): 1.0
2025-01-29 22:51:18,196 

2025-01-29 22:51:18,197 Iteration 23
2025-01-29 23:25:52,663 (Self-play) Number of new training examples: 12664
2025-01-29 23:25:52,664 (Self-play) Number of total training examples: 272792
2025-01-29 23:25:52,664 (Self-play) Number of total unique state-reward pairs: 197241
2025-01-29 23:25:52,664 (Self-play) Model 1 wins: 126. Draws: 25. Model 2 wins: 99
2025-01-29 23:27:41,010 (Training) Epoch: 1. Total loss: 1.884. Value loss: 0.512. Policy accuracy: 0.661
2025-01-29 23:28:41,749 (Training) Epoch: 2. Total loss: 1.858. Value loss: 0.493. Policy accuracy: 0.666
2025-01-29 23:29:42,143 (Training) Epoch: 3. Total loss: 1.842. Value loss: 0.481. Policy accuracy: 0.668
2025-01-29 23:30:43,362 (Training) Epoch: 4. Total loss: 1.828. Value loss: 0.473. Policy accuracy: 0.671
2025-01-29 23:31:44,065 (Training) Epoch: 5. Total loss: 1.818. Value loss: 0.465. Policy accuracy: 0.672
2025-01-29 23:32:44,202 (Training) Epoch: 6. Total loss: 1.805. Value loss: 0.459. Policy accuracy: 0.674
2025-01-29 23:33:42,897 (Training) Epoch: 7. Total loss: 1.795. Value loss: 0.453. Policy accuracy: 0.676
2025-01-29 23:34:43,149 (Training) Epoch: 8. Total loss: 1.788. Value loss: 0.447. Policy accuracy: 0.677
2025-01-29 23:35:45,005 (Training) Epoch: 9. Total loss: 1.779. Value loss: 0.445. Policy accuracy: 0.679
2025-01-29 23:36:45,872 (Training) Epoch: 10. Total loss: 1.772. Value loss: 0.441. Policy accuracy: 0.681
2025-01-29 23:46:51,259 (Evaluation) Win rate: 0.48
2025-01-29 23:46:51,259 (Evaluation) Model 1 wins: 16. Draws: 10. Model 2 wins: 24
2025-01-29 23:46:51,633 (Evaluation) Rejecting new model...
2025-01-29 23:48:22,934 (Experiment) Win rate (random): 1.0
2025-01-29 23:53:28,312 (Experiment) Win rate (alpha-beta pruning with depth 5): 0.98
2025-01-29 23:53:28,313 

2025-01-29 23:53:28,314 Iteration 24
2025-01-30 00:29:21,865 (Self-play) Number of new training examples: 12930
2025-01-30 00:29:21,865 (Self-play) Number of total training examples: 285722
2025-01-30 00:29:21,865 (Self-play) Number of total unique state-reward pairs: 206206
2025-01-30 00:29:21,865 (Self-play) Model 1 wins: 109. Draws: 20. Model 2 wins: 121
2025-01-30 00:31:13,418 (Training) Epoch: 1. Total loss: 1.893. Value loss: 0.519. Policy accuracy: 0.662
2025-01-30 00:32:15,759 (Training) Epoch: 2. Total loss: 1.864. Value loss: 0.498. Policy accuracy: 0.666
2025-01-30 00:33:18,486 (Training) Epoch: 3. Total loss: 1.845. Value loss: 0.485. Policy accuracy: 0.668
2025-01-30 00:34:20,942 (Training) Epoch: 4. Total loss: 1.823. Value loss: 0.473. Policy accuracy: 0.673
2025-01-30 00:35:23,729 (Training) Epoch: 5. Total loss: 1.815. Value loss: 0.467. Policy accuracy: 0.674
2025-01-30 00:36:26,888 (Training) Epoch: 6. Total loss: 1.807. Value loss: 0.461. Policy accuracy: 0.675
2025-01-30 00:37:32,352 (Training) Epoch: 7. Total loss: 1.798. Value loss: 0.457. Policy accuracy: 0.680
2025-01-30 00:38:40,991 (Training) Epoch: 8. Total loss: 1.788. Value loss: 0.451. Policy accuracy: 0.680
2025-01-30 00:39:44,574 (Training) Epoch: 9. Total loss: 1.782. Value loss: 0.447. Policy accuracy: 0.683
2025-01-30 00:40:47,492 (Training) Epoch: 10. Total loss: 1.776. Value loss: 0.443. Policy accuracy: 0.682
2025-01-30 00:50:19,531 (Evaluation) Win rate: 0.34
2025-01-30 00:50:19,531 (Evaluation) Model 1 wins: 24. Draws: 9. Model 2 wins: 17
2025-01-30 00:50:19,903 (Evaluation) Rejecting new model...
2025-01-30 00:52:06,537 (Experiment) Win rate (random): 1.0
2025-01-30 00:56:51,856 (Experiment) Win rate (alpha-beta pruning with depth 5): 1.0
2025-01-30 00:56:51,856 

2025-01-30 00:56:51,857 Iteration 25
2025-01-30 01:31:28,387 (Self-play) Number of new training examples: 12380
2025-01-30 01:31:28,387 (Self-play) Number of total training examples: 298102
2025-01-30 01:31:28,387 (Self-play) Number of total unique state-reward pairs: 214715
2025-01-30 01:31:28,387 (Self-play) Model 1 wins: 122. Draws: 19. Model 2 wins: 109
2025-01-30 01:33:25,019 (Training) Epoch: 1. Total loss: 1.895. Value loss: 0.524. Policy accuracy: 0.662
2025-01-30 01:34:29,879 (Training) Epoch: 2. Total loss: 1.868. Value loss: 0.504. Policy accuracy: 0.667
2025-01-30 01:35:34,341 (Training) Epoch: 3. Total loss: 1.851. Value loss: 0.488. Policy accuracy: 0.669
2025-01-30 01:36:40,098 (Training) Epoch: 4. Total loss: 1.832. Value loss: 0.477. Policy accuracy: 0.671
2025-01-30 01:37:44,352 (Training) Epoch: 5. Total loss: 1.822. Value loss: 0.472. Policy accuracy: 0.675
2025-01-30 01:38:49,299 (Training) Epoch: 6. Total loss: 1.809. Value loss: 0.464. Policy accuracy: 0.675
2025-01-30 01:39:53,821 (Training) Epoch: 7. Total loss: 1.798. Value loss: 0.458. Policy accuracy: 0.679
2025-01-30 01:40:59,051 (Training) Epoch: 8. Total loss: 1.791. Value loss: 0.455. Policy accuracy: 0.681
2025-01-30 01:42:04,851 (Training) Epoch: 9. Total loss: 1.784. Value loss: 0.449. Policy accuracy: 0.683
2025-01-30 01:43:09,785 (Training) Epoch: 10. Total loss: 1.779. Value loss: 0.446. Policy accuracy: 0.683
2025-01-30 01:52:29,708 (Evaluation) Win rate: 0.68
2025-01-30 01:52:29,708 (Evaluation) Model 1 wins: 15. Draws: 1. Model 2 wins: 34
2025-01-30 01:52:30,083 (Evaluation) Accepting new model...
2025-01-30 01:54:24,350 (Experiment) Win rate (random): 1.0
2025-01-30 01:59:22,213 (Experiment) Win rate (alpha-beta pruning with depth 5): 1.0
2025-01-30 01:59:22,214 

2025-01-30 01:59:22,215 Iteration 26
2025-01-30 02:35:42,183 (Self-play) Number of new training examples: 13668
2025-01-30 02:35:42,183 (Self-play) Number of total training examples: 311770
2025-01-30 02:35:42,183 (Self-play) Number of total unique state-reward pairs: 224377
2025-01-30 02:35:42,183 (Self-play) Model 1 wins: 120. Draws: 26. Model 2 wins: 104
2025-01-30 02:37:39,283 (Training) Epoch: 1. Total loss: 1.775. Value loss: 0.450. Policy accuracy: 0.687
2025-01-30 02:38:45,155 (Training) Epoch: 2. Total loss: 1.768. Value loss: 0.446. Policy accuracy: 0.690
2025-01-30 02:39:50,609 (Training) Epoch: 3. Total loss: 1.758. Value loss: 0.441. Policy accuracy: 0.691
2025-01-30 02:40:56,018 (Training) Epoch: 4. Total loss: 1.752. Value loss: 0.436. Policy accuracy: 0.691
2025-01-30 02:42:01,333 (Training) Epoch: 5. Total loss: 1.742. Value loss: 0.434. Policy accuracy: 0.694
2025-01-30 02:43:06,205 (Training) Epoch: 6. Total loss: 1.740. Value loss: 0.432. Policy accuracy: 0.695
2025-01-30 02:44:11,574 (Training) Epoch: 7. Total loss: 1.730. Value loss: 0.428. Policy accuracy: 0.696
2025-01-30 02:45:17,043 (Training) Epoch: 8. Total loss: 1.729. Value loss: 0.426. Policy accuracy: 0.697
2025-01-30 02:46:22,351 (Training) Epoch: 9. Total loss: 1.724. Value loss: 0.426. Policy accuracy: 0.699
2025-01-30 02:47:27,053 (Training) Epoch: 10. Total loss: 1.718. Value loss: 0.421. Policy accuracy: 0.698
2025-01-30 02:57:08,515 (Evaluation) Win rate: 0.26
2025-01-30 02:57:08,515 (Evaluation) Model 1 wins: 21. Draws: 16. Model 2 wins: 13
2025-01-30 02:57:08,831 (Evaluation) Rejecting new model...
2025-01-30 02:58:57,367 (Experiment) Win rate (random): 1.0
2025-01-30 03:03:55,295 (Experiment) Win rate (alpha-beta pruning with depth 5): 0.98
2025-01-30 03:03:55,295 

2025-01-30 03:03:55,296 Iteration 27
2025-01-30 03:38:27,511 (Self-play) Number of new training examples: 13000
2025-01-30 03:38:27,511 (Self-play) Number of total training examples: 324770
2025-01-30 03:38:27,511 (Self-play) Number of total unique state-reward pairs: 233413
2025-01-30 03:38:27,511 (Self-play) Model 1 wins: 128. Draws: 21. Model 2 wins: 101
2025-01-30 03:40:28,547 (Training) Epoch: 1. Total loss: 1.785. Value loss: 0.461. Policy accuracy: 0.687
2025-01-30 03:41:36,050 (Training) Epoch: 2. Total loss: 1.774. Value loss: 0.453. Policy accuracy: 0.688
2025-01-30 03:42:43,694 (Training) Epoch: 3. Total loss: 1.759. Value loss: 0.446. Policy accuracy: 0.691
2025-01-30 03:43:51,300 (Training) Epoch: 4. Total loss: 1.756. Value loss: 0.443. Policy accuracy: 0.691
2025-01-30 03:44:59,518 (Training) Epoch: 5. Total loss: 1.753. Value loss: 0.442. Policy accuracy: 0.694
2025-01-30 03:46:07,283 (Training) Epoch: 6. Total loss: 1.745. Value loss: 0.438. Policy accuracy: 0.694
2025-01-30 03:47:15,096 (Training) Epoch: 7. Total loss: 1.735. Value loss: 0.432. Policy accuracy: 0.697
2025-01-30 03:48:22,875 (Training) Epoch: 8. Total loss: 1.733. Value loss: 0.431. Policy accuracy: 0.698
2025-01-30 03:49:30,739 (Training) Epoch: 9. Total loss: 1.726. Value loss: 0.428. Policy accuracy: 0.698
2025-01-30 03:50:38,770 (Training) Epoch: 10. Total loss: 1.723. Value loss: 0.425. Policy accuracy: 0.700
2025-01-30 03:59:52,056 (Evaluation) Win rate: 0.44
2025-01-30 03:59:52,056 (Evaluation) Model 1 wins: 15. Draws: 13. Model 2 wins: 22
2025-01-30 03:59:52,391 (Evaluation) Rejecting new model...
2025-01-30 04:01:39,381 (Experiment) Win rate (random): 1.0
2025-01-30 04:06:33,168 (Experiment) Win rate (alpha-beta pruning with depth 5): 1.0
2025-01-30 04:06:33,169 

2025-01-30 04:06:33,170 Iteration 28
2025-01-30 04:43:22,904 (Self-play) Number of new training examples: 13876
2025-01-30 04:43:22,904 (Self-play) Number of total training examples: 338646
2025-01-30 04:43:22,904 (Self-play) Number of total unique state-reward pairs: 243211
2025-01-30 04:43:22,904 (Self-play) Model 1 wins: 128. Draws: 17. Model 2 wins: 105
2025-01-30 04:45:28,346 (Training) Epoch: 1. Total loss: 1.793. Value loss: 0.472. Policy accuracy: 0.689
2025-01-30 04:46:38,526 (Training) Epoch: 2. Total loss: 1.774. Value loss: 0.459. Policy accuracy: 0.691
2025-01-30 04:47:49,070 (Training) Epoch: 3. Total loss: 1.769. Value loss: 0.454. Policy accuracy: 0.692
2025-01-30 04:48:59,570 (Training) Epoch: 4. Total loss: 1.757. Value loss: 0.447. Policy accuracy: 0.693
2025-01-30 04:50:10,363 (Training) Epoch: 5. Total loss: 1.751. Value loss: 0.443. Policy accuracy: 0.694
2025-01-30 04:51:20,788 (Training) Epoch: 6. Total loss: 1.744. Value loss: 0.440. Policy accuracy: 0.696
2025-01-30 04:52:31,313 (Training) Epoch: 7. Total loss: 1.736. Value loss: 0.436. Policy accuracy: 0.698
2025-01-30 04:53:41,730 (Training) Epoch: 8. Total loss: 1.735. Value loss: 0.436. Policy accuracy: 0.699
2025-01-30 04:54:52,525 (Training) Epoch: 9. Total loss: 1.723. Value loss: 0.428. Policy accuracy: 0.701
2025-01-30 04:56:03,209 (Training) Epoch: 10. Total loss: 1.720. Value loss: 0.428. Policy accuracy: 0.702
2025-01-30 05:05:33,307 (Evaluation) Win rate: 0.28
2025-01-30 05:05:33,307 (Evaluation) Model 1 wins: 25. Draws: 11. Model 2 wins: 14
2025-01-30 05:05:33,641 (Evaluation) Rejecting new model...
2025-01-30 05:07:24,675 (Experiment) Win rate (random): 1.0
2025-01-30 05:12:37,685 (Experiment) Win rate (alpha-beta pruning with depth 5): 0.98
2025-01-30 05:12:37,685 

2025-01-30 05:12:37,686 Iteration 29
2025-01-30 05:47:43,626 (Self-play) Number of new training examples: 13258
2025-01-30 05:47:43,626 (Self-play) Number of total training examples: 351904
2025-01-30 05:47:43,626 (Self-play) Number of total unique state-reward pairs: 252485
2025-01-30 05:47:43,626 (Self-play) Model 1 wins: 117. Draws: 14. Model 2 wins: 119
2025-01-30 05:49:55,025 (Training) Epoch: 1. Total loss: 1.796. Value loss: 0.476. Policy accuracy: 0.688
2025-01-30 05:51:08,490 (Training) Epoch: 2. Total loss: 1.781. Value loss: 0.464. Policy accuracy: 0.689
2025-01-30 05:52:21,972 (Training) Epoch: 3. Total loss: 1.769. Value loss: 0.457. Policy accuracy: 0.691
2025-01-30 05:53:35,422 (Training) Epoch: 4. Total loss: 1.758. Value loss: 0.452. Policy accuracy: 0.693
2025-01-30 05:54:48,788 (Training) Epoch: 5. Total loss: 1.749. Value loss: 0.446. Policy accuracy: 0.695
2025-01-30 05:56:01,842 (Training) Epoch: 6. Total loss: 1.746. Value loss: 0.444. Policy accuracy: 0.697
2025-01-30 05:57:14,423 (Training) Epoch: 7. Total loss: 1.738. Value loss: 0.440. Policy accuracy: 0.698
2025-01-30 05:58:27,784 (Training) Epoch: 8. Total loss: 1.731. Value loss: 0.437. Policy accuracy: 0.699
2025-01-30 05:59:41,045 (Training) Epoch: 9. Total loss: 1.729. Value loss: 0.434. Policy accuracy: 0.700
2025-01-30 06:00:54,961 (Training) Epoch: 10. Total loss: 1.720. Value loss: 0.430. Policy accuracy: 0.702
2025-01-30 06:10:06,817 (Evaluation) Win rate: 0.52
2025-01-30 06:10:06,817 (Evaluation) Model 1 wins: 16. Draws: 8. Model 2 wins: 26
2025-01-30 06:10:07,145 (Evaluation) Rejecting new model...
2025-01-30 06:11:57,291 (Experiment) Win rate (random): 1.0
2025-01-30 06:16:47,412 (Experiment) Win rate (alpha-beta pruning with depth 5): 1.0
2025-01-30 06:16:47,412 

2025-01-30 06:16:47,413 Iteration 30
2025-01-30 06:52:30,746 (Self-play) Number of new training examples: 13450
2025-01-30 06:52:30,746 (Self-play) Number of total training examples: 365354
2025-01-30 06:52:30,747 (Self-play) Number of total unique state-reward pairs: 261840
2025-01-30 06:52:30,747 (Self-play) Model 1 wins: 129. Draws: 20. Model 2 wins: 101
2025-01-30 06:54:46,649 (Training) Epoch: 1. Total loss: 1.799. Value loss: 0.482. Policy accuracy: 0.689
2025-01-30 06:56:03,352 (Training) Epoch: 2. Total loss: 1.786. Value loss: 0.468. Policy accuracy: 0.690
2025-01-30 06:57:19,928 (Training) Epoch: 3. Total loss: 1.773. Value loss: 0.462. Policy accuracy: 0.693
2025-01-30 06:58:36,257 (Training) Epoch: 4. Total loss: 1.761. Value loss: 0.455. Policy accuracy: 0.695
2025-01-30 06:59:52,925 (Training) Epoch: 5. Total loss: 1.754. Value loss: 0.450. Policy accuracy: 0.695
2025-01-30 07:01:09,568 (Training) Epoch: 6. Total loss: 1.748. Value loss: 0.447. Policy accuracy: 0.698
2025-01-30 07:02:25,917 (Training) Epoch: 7. Total loss: 1.740. Value loss: 0.442. Policy accuracy: 0.698
2025-01-30 07:03:42,594 (Training) Epoch: 8. Total loss: 1.733. Value loss: 0.438. Policy accuracy: 0.700
2025-01-30 07:04:59,814 (Training) Epoch: 9. Total loss: 1.728. Value loss: 0.436. Policy accuracy: 0.701
2025-01-30 07:06:15,700 (Training) Epoch: 10. Total loss: 1.720. Value loss: 0.431. Policy accuracy: 0.703
2025-01-30 07:15:20,409 (Evaluation) Win rate: 0.42
2025-01-30 07:15:20,409 (Evaluation) Model 1 wins: 19. Draws: 10. Model 2 wins: 21
2025-01-30 07:15:20,755 (Evaluation) Rejecting new model...
2025-01-30 07:17:12,065 (Experiment) Win rate (random): 1.0
2025-01-30 07:22:16,808 (Experiment) Win rate (alpha-beta pruning with depth 5): 1.0
2025-01-30 07:22:16,808 

2025-01-30 07:22:16,809 Iteration 31
2025-01-30 07:57:53,981 (Self-play) Number of new training examples: 13246
2025-01-30 07:57:53,981 (Self-play) Number of total training examples: 378600
2025-01-30 07:57:53,981 (Self-play) Number of total unique state-reward pairs: 270923
2025-01-30 07:57:53,981 (Self-play) Model 1 wins: 103. Draws: 19. Model 2 wins: 128
2025-01-30 08:00:14,143 (Training) Epoch: 1. Total loss: 1.809. Value loss: 0.491. Policy accuracy: 0.689
2025-01-30 08:01:33,474 (Training) Epoch: 2. Total loss: 1.788. Value loss: 0.475. Policy accuracy: 0.690
2025-01-30 08:02:52,393 (Training) Epoch: 3. Total loss: 1.776. Value loss: 0.466. Policy accuracy: 0.692
2025-01-30 08:04:11,410 (Training) Epoch: 4. Total loss: 1.768. Value loss: 0.460. Policy accuracy: 0.695
2025-01-30 08:05:31,297 (Training) Epoch: 5. Total loss: 1.756. Value loss: 0.455. Policy accuracy: 0.697
2025-01-30 08:06:51,331 (Training) Epoch: 6. Total loss: 1.747. Value loss: 0.449. Policy accuracy: 0.698
2025-01-30 08:08:10,779 (Training) Epoch: 7. Total loss: 1.744. Value loss: 0.447. Policy accuracy: 0.699
2025-01-30 08:09:29,942 (Training) Epoch: 8. Total loss: 1.736. Value loss: 0.441. Policy accuracy: 0.699
2025-01-30 08:10:49,451 (Training) Epoch: 9. Total loss: 1.728. Value loss: 0.437. Policy accuracy: 0.701
2025-01-30 08:12:08,373 (Training) Epoch: 10. Total loss: 1.724. Value loss: 0.436. Policy accuracy: 0.703
2025-01-30 08:21:05,410 (Evaluation) Win rate: 0.18
2025-01-30 08:21:05,411 (Evaluation) Model 1 wins: 21. Draws: 20. Model 2 wins: 9
2025-01-30 08:21:05,757 (Evaluation) Rejecting new model...
2025-01-30 08:22:56,504 (Experiment) Win rate (random): 1.0
2025-01-30 08:28:13,005 (Experiment) Win rate (alpha-beta pruning with depth 5): 1.0
2025-01-30 08:28:13,005 

2025-01-30 08:28:13,006 Iteration 32
2025-01-30 09:03:59,962 (Self-play) Number of new training examples: 13344
2025-01-30 09:03:59,963 (Self-play) Number of total training examples: 391944
2025-01-30 09:03:59,963 (Self-play) Number of total unique state-reward pairs: 280159
2025-01-30 09:03:59,963 (Self-play) Model 1 wins: 124. Draws: 23. Model 2 wins: 103
2025-01-30 09:06:26,162 (Training) Epoch: 1. Total loss: 1.815. Value loss: 0.497. Policy accuracy: 0.688
2025-01-30 09:07:48,497 (Training) Epoch: 2. Total loss: 1.792. Value loss: 0.480. Policy accuracy: 0.691
2025-01-30 09:09:09,963 (Training) Epoch: 3. Total loss: 1.779. Value loss: 0.470. Policy accuracy: 0.692
2025-01-30 09:10:32,429 (Training) Epoch: 4. Total loss: 1.767. Value loss: 0.462. Policy accuracy: 0.695
2025-01-30 09:11:54,239 (Training) Epoch: 5. Total loss: 1.757. Value loss: 0.456. Policy accuracy: 0.697
2025-01-30 09:13:16,157 (Training) Epoch: 6. Total loss: 1.751. Value loss: 0.453. Policy accuracy: 0.698
2025-01-30 09:14:38,405 (Training) Epoch: 7. Total loss: 1.741. Value loss: 0.449. Policy accuracy: 0.700
2025-01-30 09:15:59,964 (Training) Epoch: 8. Total loss: 1.735. Value loss: 0.443. Policy accuracy: 0.701
2025-01-30 09:17:21,586 (Training) Epoch: 9. Total loss: 1.727. Value loss: 0.440. Policy accuracy: 0.703
2025-01-30 09:18:43,393 (Training) Epoch: 10. Total loss: 1.723. Value loss: 0.437. Policy accuracy: 0.703
2025-01-30 09:27:43,796 (Evaluation) Win rate: 0.28
2025-01-30 09:27:43,796 (Evaluation) Model 1 wins: 12. Draws: 24. Model 2 wins: 14
2025-01-30 09:27:44,136 (Evaluation) Rejecting new model...
2025-01-30 09:29:39,633 (Experiment) Win rate (random): 1.0
2025-01-30 09:34:28,287 (Experiment) Win rate (alpha-beta pruning with depth 5): 0.98
2025-01-30 09:34:28,287 

2025-01-30 09:34:28,288 Iteration 33
2025-01-30 10:11:11,996 (Self-play) Number of new training examples: 13686
2025-01-30 10:11:11,996 (Self-play) Number of total training examples: 405630
2025-01-30 10:11:11,997 (Self-play) Number of total unique state-reward pairs: 289603
2025-01-30 10:11:11,997 (Self-play) Model 1 wins: 123. Draws: 23. Model 2 wins: 104
2025-01-30 10:13:43,980 (Training) Epoch: 1. Total loss: 1.815. Value loss: 0.502. Policy accuracy: 0.689
2025-01-30 10:15:08,504 (Training) Epoch: 2. Total loss: 1.795. Value loss: 0.484. Policy accuracy: 0.691
2025-01-30 10:16:33,083 (Training) Epoch: 3. Total loss: 1.785. Value loss: 0.477. Policy accuracy: 0.693
2025-01-30 10:17:57,856 (Training) Epoch: 4. Total loss: 1.769. Value loss: 0.467. Policy accuracy: 0.697
2025-01-30 10:19:22,403 (Training) Epoch: 5. Total loss: 1.759. Value loss: 0.461. Policy accuracy: 0.697
2025-01-30 10:20:47,814 (Training) Epoch: 6. Total loss: 1.753. Value loss: 0.457. Policy accuracy: 0.699
2025-01-30 10:22:12,567 (Training) Epoch: 7. Total loss: 1.741. Value loss: 0.451. Policy accuracy: 0.702
2025-01-30 10:23:37,147 (Training) Epoch: 8. Total loss: 1.736. Value loss: 0.446. Policy accuracy: 0.702
2025-01-30 10:25:01,068 (Training) Epoch: 9. Total loss: 1.727. Value loss: 0.444. Policy accuracy: 0.702
2025-01-30 10:26:26,322 (Training) Epoch: 10. Total loss: 1.725. Value loss: 0.440. Policy accuracy: 0.704
2025-01-30 10:35:44,256 (Evaluation) Win rate: 0.18
2025-01-30 10:35:44,257 (Evaluation) Model 1 wins: 14. Draws: 27. Model 2 wins: 9
2025-01-30 10:35:44,582 (Evaluation) Rejecting new model...
2025-01-30 10:37:24,902 (Experiment) Win rate (random): 1.0
2025-01-30 10:42:25,655 (Experiment) Win rate (alpha-beta pruning with depth 5): 1.0
2025-01-30 10:42:25,655 

2025-01-30 10:42:25,657 Iteration 34
2025-01-30 11:17:33,249 (Self-play) Number of new training examples: 13060
2025-01-30 11:17:33,249 (Self-play) Number of total training examples: 418690
2025-01-30 11:17:33,249 (Self-play) Number of total unique state-reward pairs: 298556
2025-01-30 11:17:33,249 (Self-play) Model 1 wins: 120. Draws: 23. Model 2 wins: 107
2025-01-30 11:20:08,124 (Training) Epoch: 1. Total loss: 1.819. Value loss: 0.505. Policy accuracy: 0.689
2025-01-30 11:21:35,993 (Training) Epoch: 2. Total loss: 1.795. Value loss: 0.487. Policy accuracy: 0.692
2025-01-30 11:23:03,264 (Training) Epoch: 3. Total loss: 1.781. Value loss: 0.476. Policy accuracy: 0.693
2025-01-30 11:24:31,306 (Training) Epoch: 4. Total loss: 1.767. Value loss: 0.469. Policy accuracy: 0.696
2025-01-30 11:25:59,105 (Training) Epoch: 5. Total loss: 1.760. Value loss: 0.463. Policy accuracy: 0.699
2025-01-30 11:27:27,072 (Training) Epoch: 6. Total loss: 1.748. Value loss: 0.457. Policy accuracy: 0.701
2025-01-30 11:28:55,316 (Training) Epoch: 7. Total loss: 1.742. Value loss: 0.452. Policy accuracy: 0.701
2025-01-30 11:30:23,168 (Training) Epoch: 8. Total loss: 1.735. Value loss: 0.448. Policy accuracy: 0.701
2025-01-30 11:31:51,452 (Training) Epoch: 9. Total loss: 1.730. Value loss: 0.446. Policy accuracy: 0.705
2025-01-30 11:33:18,638 (Training) Epoch: 10. Total loss: 1.722. Value loss: 0.441. Policy accuracy: 0.705
2025-01-30 11:42:46,250 (Evaluation) Win rate: 0.4
2025-01-30 11:42:46,250 (Evaluation) Model 1 wins: 14. Draws: 16. Model 2 wins: 20
2025-01-30 11:42:46,581 (Evaluation) Rejecting new model...
2025-01-30 11:44:39,628 (Experiment) Win rate (random): 1.0
2025-01-30 11:49:54,096 (Experiment) Win rate (alpha-beta pruning with depth 5): 1.0
2025-01-30 11:49:54,096 

2025-01-30 11:49:54,097 Iteration 35
2025-01-30 12:27:00,580 (Self-play) Number of new training examples: 13710
2025-01-30 12:27:00,581 (Self-play) Number of total training examples: 432400
2025-01-30 12:27:00,581 (Self-play) Number of total unique state-reward pairs: 307940
2025-01-30 12:27:00,581 (Self-play) Model 1 wins: 121. Draws: 20. Model 2 wins: 109
2025-01-30 12:29:45,962 (Training) Epoch: 1. Total loss: 1.825. Value loss: 0.511. Policy accuracy: 0.689
2025-01-30 12:31:17,067 (Training) Epoch: 2. Total loss: 1.797. Value loss: 0.491. Policy accuracy: 0.694
2025-01-30 12:32:48,123 (Training) Epoch: 3. Total loss: 1.785. Value loss: 0.483. Policy accuracy: 0.696
2025-01-30 12:34:20,311 (Training) Epoch: 4. Total loss: 1.769. Value loss: 0.471. Policy accuracy: 0.696
2025-01-30 12:35:51,462 (Training) Epoch: 5. Total loss: 1.762. Value loss: 0.467. Policy accuracy: 0.700
2025-01-30 12:37:23,235 (Training) Epoch: 6. Total loss: 1.751. Value loss: 0.460. Policy accuracy: 0.701
2025-01-30 12:38:56,064 (Training) Epoch: 7. Total loss: 1.740. Value loss: 0.456. Policy accuracy: 0.703
2025-01-30 12:40:27,257 (Training) Epoch: 8. Total loss: 1.731. Value loss: 0.450. Policy accuracy: 0.704
2025-01-30 12:41:59,219 (Training) Epoch: 9. Total loss: 1.728. Value loss: 0.447. Policy accuracy: 0.705
2025-01-30 12:43:30,828 (Training) Epoch: 10. Total loss: 1.723. Value loss: 0.445. Policy accuracy: 0.707
2025-01-30 12:52:49,027 (Evaluation) Win rate: 0.3
2025-01-30 12:52:49,027 (Evaluation) Model 1 wins: 10. Draws: 25. Model 2 wins: 15
2025-01-30 12:52:49,405 (Evaluation) Rejecting new model...
2025-01-30 12:54:38,240 (Experiment) Win rate (random): 1.0
2025-01-30 12:59:36,903 (Experiment) Win rate (alpha-beta pruning with depth 5): 1.0
2025-01-30 12:59:36,903 

2025-01-30 12:59:36,904 Iteration 36
2025-01-30 13:36:48,280 (Self-play) Number of new training examples: 13838
2025-01-30 13:36:48,281 (Self-play) Number of total training examples: 446238
2025-01-30 13:36:48,281 (Self-play) Number of total unique state-reward pairs: 317528
2025-01-30 13:36:48,281 (Self-play) Model 1 wins: 123. Draws: 18. Model 2 wins: 109
2025-01-30 13:39:44,065 (Training) Epoch: 1. Total loss: 1.830. Value loss: 0.518. Policy accuracy: 0.690
2025-01-30 13:41:20,655 (Training) Epoch: 2. Total loss: 1.799. Value loss: 0.495. Policy accuracy: 0.695
2025-01-30 13:42:58,250 (Training) Epoch: 3. Total loss: 1.780. Value loss: 0.482. Policy accuracy: 0.697
2025-01-30 13:44:35,069 (Training) Epoch: 4. Total loss: 1.770. Value loss: 0.475. Policy accuracy: 0.699
2025-01-30 13:46:11,757 (Training) Epoch: 5. Total loss: 1.760. Value loss: 0.468. Policy accuracy: 0.701
2025-01-30 13:47:48,999 (Training) Epoch: 6. Total loss: 1.751. Value loss: 0.463. Policy accuracy: 0.702
2025-01-30 13:49:24,889 (Training) Epoch: 7. Total loss: 1.743. Value loss: 0.459. Policy accuracy: 0.705
2025-01-30 13:51:00,779 (Training) Epoch: 8. Total loss: 1.732. Value loss: 0.452. Policy accuracy: 0.705
2025-01-30 13:52:35,516 (Training) Epoch: 9. Total loss: 1.731. Value loss: 0.452. Policy accuracy: 0.706
2025-01-30 13:54:10,113 (Training) Epoch: 10. Total loss: 1.723. Value loss: 0.447. Policy accuracy: 0.709
2025-01-30 14:03:23,271 (Evaluation) Win rate: 0.58
2025-01-30 14:03:23,271 (Evaluation) Model 1 wins: 15. Draws: 6. Model 2 wins: 29
2025-01-30 14:03:23,677 (Evaluation) Accepting new model...
2025-01-30 14:05:11,584 (Experiment) Win rate (random): 1.0
2025-01-30 14:10:34,412 (Experiment) Win rate (alpha-beta pruning with depth 5): 0.98
2025-01-30 14:10:34,412 

2025-01-30 14:10:34,414 Iteration 37
2025-01-30 14:47:24,840 (Self-play) Number of new training examples: 13608
2025-01-30 14:47:24,840 (Self-play) Number of total training examples: 459846
2025-01-30 14:47:24,840 (Self-play) Number of total unique state-reward pairs: 326898
2025-01-30 14:47:24,840 (Self-play) Model 1 wins: 118. Draws: 18. Model 2 wins: 114
2025-01-30 14:50:35,885 (Training) Epoch: 1. Total loss: 1.723. Value loss: 0.449. Policy accuracy: 0.709
2025-01-30 14:52:19,432 (Training) Epoch: 2. Total loss: 1.715. Value loss: 0.445. Policy accuracy: 0.709
2025-01-30 14:54:02,886 (Training) Epoch: 3. Total loss: 1.709. Value loss: 0.441. Policy accuracy: 0.712
2025-01-30 14:55:46,715 (Training) Epoch: 4. Total loss: 1.704. Value loss: 0.440. Policy accuracy: 0.715
2025-01-30 14:57:29,641 (Training) Epoch: 5. Total loss: 1.697. Value loss: 0.434. Policy accuracy: 0.714
2025-01-30 14:59:14,149 (Training) Epoch: 6. Total loss: 1.694. Value loss: 0.433. Policy accuracy: 0.716
2025-01-30 15:00:57,543 (Training) Epoch: 7. Total loss: 1.689. Value loss: 0.431. Policy accuracy: 0.716
2025-01-30 15:02:40,823 (Training) Epoch: 8. Total loss: 1.686. Value loss: 0.429. Policy accuracy: 0.718
2025-01-30 15:04:23,862 (Training) Epoch: 9. Total loss: 1.681. Value loss: 0.428. Policy accuracy: 0.718
2025-01-30 15:06:07,431 (Training) Epoch: 10. Total loss: 1.679. Value loss: 0.425. Policy accuracy: 0.717
2025-01-30 15:16:19,172 (Evaluation) Win rate: 0.52
2025-01-30 15:16:19,172 (Evaluation) Model 1 wins: 12. Draws: 12. Model 2 wins: 26
2025-01-30 15:16:19,623 (Evaluation) Rejecting new model...
2025-01-30 15:18:23,948 (Experiment) Win rate (random): 1.0
2025-01-30 15:23:30,892 (Experiment) Win rate (alpha-beta pruning with depth 5): 0.96
2025-01-30 15:23:30,892 

2025-01-30 15:23:30,896 Iteration 38
2025-01-30 16:03:28,357 (Self-play) Number of new training examples: 14204
2025-01-30 16:03:28,357 (Self-play) Number of total training examples: 474050
2025-01-30 16:03:28,357 (Self-play) Number of total unique state-reward pairs: 336884
2025-01-30 16:03:28,357 (Self-play) Model 1 wins: 112. Draws: 26. Model 2 wins: 112
2025-01-30 16:06:35,095 (Training) Epoch: 1. Total loss: 1.726. Value loss: 0.454. Policy accuracy: 0.710
2025-01-30 16:08:21,837 (Training) Epoch: 2. Total loss: 1.718. Value loss: 0.450. Policy accuracy: 0.712
2025-01-30 16:10:08,958 (Training) Epoch: 3. Total loss: 1.707. Value loss: 0.444. Policy accuracy: 0.713
2025-01-30 16:11:55,923 (Training) Epoch: 4. Total loss: 1.702. Value loss: 0.440. Policy accuracy: 0.715
2025-01-30 16:13:43,926 (Training) Epoch: 5. Total loss: 1.699. Value loss: 0.438. Policy accuracy: 0.715
2025-01-30 16:15:30,823 (Training) Epoch: 6. Total loss: 1.693. Value loss: 0.435. Policy accuracy: 0.716
2025-01-30 16:17:18,671 (Training) Epoch: 7. Total loss: 1.690. Value loss: 0.434. Policy accuracy: 0.717
2025-01-30 16:19:06,143 (Training) Epoch: 8. Total loss: 1.687. Value loss: 0.433. Policy accuracy: 0.717
2025-01-30 16:20:52,646 (Training) Epoch: 9. Total loss: 1.679. Value loss: 0.429. Policy accuracy: 0.720
2025-01-30 16:22:39,186 (Training) Epoch: 10. Total loss: 1.676. Value loss: 0.427. Policy accuracy: 0.720
2025-01-30 16:32:21,731 (Evaluation) Win rate: 0.5
2025-01-30 16:32:21,731 (Evaluation) Model 1 wins: 16. Draws: 9. Model 2 wins: 25
2025-01-30 16:32:22,203 (Evaluation) Rejecting new model...
2025-01-30 16:34:19,465 (Experiment) Win rate (random): 1.0
2025-01-30 16:39:32,361 (Experiment) Win rate (alpha-beta pruning with depth 5): 1.0
2025-01-30 16:39:32,361 

2025-01-30 16:39:32,374 Iteration 39
2025-01-30 17:19:34,977 (Self-play) Number of new training examples: 14204
2025-01-30 17:19:34,978 (Self-play) Number of total training examples: 488254
2025-01-30 17:19:34,978 (Self-play) Number of total unique state-reward pairs: 346915
2025-01-30 17:19:34,978 (Self-play) Model 1 wins: 122. Draws: 14. Model 2 wins: 114
2025-01-30 17:22:53,203 (Training) Epoch: 1. Total loss: 1.727. Value loss: 0.459. Policy accuracy: 0.711
2025-01-30 17:24:40,788 (Training) Epoch: 2. Total loss: 1.716. Value loss: 0.451. Policy accuracy: 0.713
2025-01-30 17:26:29,720 (Training) Epoch: 3. Total loss: 1.711. Value loss: 0.449. Policy accuracy: 0.714
2025-01-30 17:28:17,733 (Training) Epoch: 4. Total loss: 1.706. Value loss: 0.444. Policy accuracy: 0.716
2025-01-30 17:30:06,955 (Training) Epoch: 5. Total loss: 1.697. Value loss: 0.440. Policy accuracy: 0.715
2025-01-30 17:31:56,103 (Training) Epoch: 6. Total loss: 1.691. Value loss: 0.437. Policy accuracy: 0.718
2025-01-30 17:33:44,294 (Training) Epoch: 7. Total loss: 1.687. Value loss: 0.435. Policy accuracy: 0.719
2025-01-30 17:35:33,048 (Training) Epoch: 8. Total loss: 1.685. Value loss: 0.434. Policy accuracy: 0.720
2025-01-30 17:37:21,380 (Training) Epoch: 9. Total loss: 1.677. Value loss: 0.430. Policy accuracy: 0.722
2025-01-30 17:39:10,588 (Training) Epoch: 10. Total loss: 1.680. Value loss: 0.430. Policy accuracy: 0.720
2025-01-30 17:48:42,964 (Evaluation) Win rate: 0.3
2025-01-30 17:48:42,964 (Evaluation) Model 1 wins: 14. Draws: 21. Model 2 wins: 15
2025-01-30 17:48:43,401 (Evaluation) Rejecting new model...
2025-01-30 17:50:39,332 (Experiment) Win rate (random): 1.0
2025-01-30 17:56:03,784 (Experiment) Win rate (alpha-beta pruning with depth 5): 1.0
2025-01-30 17:56:03,784 

2025-01-30 17:56:03,799 Iteration 40
2025-01-30 18:35:52,165 (Self-play) Number of new training examples: 14046
2025-01-30 18:35:52,166 (Self-play) Number of total training examples: 502300
2025-01-30 18:35:52,166 (Self-play) Number of total unique state-reward pairs: 356677
2025-01-30 18:35:52,166 (Self-play) Model 1 wins: 123. Draws: 20. Model 2 wins: 107
2025-01-30 18:39:12,237 (Training) Epoch: 1. Total loss: 1.733. Value loss: 0.465. Policy accuracy: 0.712
2025-01-30 18:41:03,150 (Training) Epoch: 2. Total loss: 1.718. Value loss: 0.455. Policy accuracy: 0.713
2025-01-30 18:42:53,654 (Training) Epoch: 3. Total loss: 1.712. Value loss: 0.451. Policy accuracy: 0.715
2025-01-30 18:44:42,588 (Training) Epoch: 4. Total loss: 1.706. Value loss: 0.447. Policy accuracy: 0.716
2025-01-30 18:46:32,777 (Training) Epoch: 5. Total loss: 1.696. Value loss: 0.441. Policy accuracy: 0.716
2025-01-30 18:48:23,328 (Training) Epoch: 6. Total loss: 1.692. Value loss: 0.440. Policy accuracy: 0.718
2025-01-30 18:50:15,031 (Training) Epoch: 7. Total loss: 1.688. Value loss: 0.438. Policy accuracy: 0.718
2025-01-30 18:52:07,961 (Training) Epoch: 8. Total loss: 1.685. Value loss: 0.436. Policy accuracy: 0.720
2025-01-30 18:54:01,667 (Training) Epoch: 9. Total loss: 1.681. Value loss: 0.433. Policy accuracy: 0.721
2025-01-30 18:55:52,312 (Training) Epoch: 10. Total loss: 1.679. Value loss: 0.431. Policy accuracy: 0.721
2025-01-30 19:05:01,272 (Evaluation) Win rate: 0.24
2025-01-30 19:05:01,272 (Evaluation) Model 1 wins: 27. Draws: 11. Model 2 wins: 12
2025-01-30 19:05:01,685 (Evaluation) Rejecting new model...
2025-01-30 19:06:58,864 (Experiment) Win rate (random): 1.0
2025-01-30 19:12:05,791 (Experiment) Win rate (alpha-beta pruning with depth 5): 0.96
2025-01-30 19:12:05,791 

2025-01-30 19:12:05,804 Iteration 41
2025-01-30 19:50:28,200 (Self-play) Number of new training examples: 13366
2025-01-30 19:50:28,200 (Self-play) Number of total training examples: 515666
2025-01-30 19:50:28,200 (Self-play) Number of total unique state-reward pairs: 365770
2025-01-30 19:50:28,201 (Self-play) Model 1 wins: 107. Draws: 14. Model 2 wins: 129
2025-01-30 19:53:57,822 (Training) Epoch: 1. Total loss: 1.737. Value loss: 0.470. Policy accuracy: 0.713
2025-01-30 19:55:52,648 (Training) Epoch: 2. Total loss: 1.723. Value loss: 0.459. Policy accuracy: 0.714
2025-01-30 19:57:47,870 (Training) Epoch: 3. Total loss: 1.713. Value loss: 0.454. Policy accuracy: 0.716
2025-01-30 19:59:42,039 (Training) Epoch: 4. Total loss: 1.708. Value loss: 0.451. Policy accuracy: 0.717
2025-01-30 20:01:36,221 (Training) Epoch: 5. Total loss: 1.700. Value loss: 0.445. Policy accuracy: 0.717
2025-01-30 20:03:32,646 (Training) Epoch: 6. Total loss: 1.693. Value loss: 0.442. Policy accuracy: 0.720
2025-01-30 20:05:27,075 (Training) Epoch: 7. Total loss: 1.691. Value loss: 0.441. Policy accuracy: 0.720
2025-01-30 20:07:19,715 (Training) Epoch: 8. Total loss: 1.688. Value loss: 0.439. Policy accuracy: 0.721
2025-01-30 20:09:12,317 (Training) Epoch: 9. Total loss: 1.680. Value loss: 0.435. Policy accuracy: 0.722
2025-01-30 20:11:03,858 (Training) Epoch: 10. Total loss: 1.676. Value loss: 0.433. Policy accuracy: 0.723
2025-01-30 20:20:29,433 (Evaluation) Win rate: 0.72
2025-01-30 20:20:29,433 (Evaluation) Model 1 wins: 7. Draws: 7. Model 2 wins: 36
2025-01-30 20:20:29,836 (Evaluation) Accepting new model...
2025-01-30 20:22:24,330 (Experiment) Win rate (random): 1.0
2025-01-30 20:27:12,020 (Experiment) Win rate (alpha-beta pruning with depth 5): 1.0
2025-01-30 20:27:12,020 

2025-01-30 20:27:12,021 Iteration 42
2025-01-30 21:04:21,347 (Self-play) Number of new training examples: 13466
2025-01-30 21:04:21,347 (Self-play) Number of total training examples: 529132
2025-01-30 21:04:21,347 (Self-play) Number of total unique state-reward pairs: 375069
2025-01-30 21:04:21,347 (Self-play) Model 1 wins: 131. Draws: 20. Model 2 wins: 99
2025-01-30 21:07:39,602 (Training) Epoch: 1. Total loss: 1.685. Value loss: 0.440. Policy accuracy: 0.723
2025-01-30 21:09:36,095 (Training) Epoch: 2. Total loss: 1.677. Value loss: 0.437. Policy accuracy: 0.725
2025-01-30 21:11:32,304 (Training) Epoch: 3. Total loss: 1.672. Value loss: 0.431. Policy accuracy: 0.723
2025-01-30 21:13:28,007 (Training) Epoch: 4. Total loss: 1.669. Value loss: 0.430. Policy accuracy: 0.726
2025-01-30 21:15:23,951 (Training) Epoch: 5. Total loss: 1.667. Value loss: 0.430. Policy accuracy: 0.726
2025-01-30 21:17:20,329 (Training) Epoch: 6. Total loss: 1.661. Value loss: 0.427. Policy accuracy: 0.726
2025-01-30 21:19:17,125 (Training) Epoch: 7. Total loss: 1.659. Value loss: 0.425. Policy accuracy: 0.728
2025-01-30 21:21:12,567 (Training) Epoch: 8. Total loss: 1.652. Value loss: 0.423. Policy accuracy: 0.729
2025-01-30 21:23:08,870 (Training) Epoch: 9. Total loss: 1.651. Value loss: 0.421. Policy accuracy: 0.728
2025-01-30 21:25:05,270 (Training) Epoch: 10. Total loss: 1.647. Value loss: 0.421. Policy accuracy: 0.730
2025-01-30 21:34:15,071 (Evaluation) Win rate: 0.26
2025-01-30 21:34:15,071 (Evaluation) Model 1 wins: 30. Draws: 7. Model 2 wins: 13
2025-01-30 21:34:15,472 (Evaluation) Rejecting new model...
2025-01-30 21:36:09,152 (Experiment) Win rate (random): 1.0
2025-01-30 21:41:50,807 (Experiment) Win rate (alpha-beta pruning with depth 5): 0.96
2025-01-30 21:41:50,807 

2025-01-30 21:41:50,808 Iteration 43
2025-01-30 22:19:31,210 (Self-play) Number of new training examples: 13506
2025-01-30 22:19:31,210 (Self-play) Number of total training examples: 542638
2025-01-30 22:19:31,210 (Self-play) Number of total unique state-reward pairs: 384313
2025-01-30 22:19:31,210 (Self-play) Model 1 wins: 111. Draws: 21. Model 2 wins: 118
2025-01-30 22:23:16,041 (Training) Epoch: 1. Total loss: 1.684. Value loss: 0.443. Policy accuracy: 0.723
2025-01-30 22:25:21,115 (Training) Epoch: 2. Total loss: 1.680. Value loss: 0.440. Policy accuracy: 0.724
2025-01-30 22:27:27,191 (Training) Epoch: 3. Total loss: 1.673. Value loss: 0.436. Policy accuracy: 0.726
2025-01-30 22:29:32,220 (Training) Epoch: 4. Total loss: 1.670. Value loss: 0.433. Policy accuracy: 0.726
2025-01-30 22:31:37,821 (Training) Epoch: 5. Total loss: 1.663. Value loss: 0.430. Policy accuracy: 0.728
2025-01-30 22:33:42,313 (Training) Epoch: 6. Total loss: 1.665. Value loss: 0.429. Policy accuracy: 0.727
2025-01-30 22:35:47,096 (Training) Epoch: 7. Total loss: 1.656. Value loss: 0.426. Policy accuracy: 0.729
2025-01-30 22:37:52,921 (Training) Epoch: 8. Total loss: 1.653. Value loss: 0.426. Policy accuracy: 0.730
2025-01-30 22:39:59,744 (Training) Epoch: 9. Total loss: 1.656. Value loss: 0.425. Policy accuracy: 0.729
2025-01-30 22:42:02,993 (Training) Epoch: 10. Total loss: 1.647. Value loss: 0.421. Policy accuracy: 0.730
2025-01-30 22:51:42,607 (Evaluation) Win rate: 0.52
2025-01-30 22:51:42,607 (Evaluation) Model 1 wins: 15. Draws: 9. Model 2 wins: 26
2025-01-30 22:51:43,144 (Evaluation) Rejecting new model...
2025-01-30 22:53:40,798 (Experiment) Win rate (random): 1.0
2025-01-30 22:59:00,988 (Experiment) Win rate (alpha-beta pruning with depth 5): 0.98
2025-01-30 22:59:00,988 

2025-01-30 22:59:00,996 Iteration 44
2025-01-30 23:37:40,347 (Self-play) Number of new training examples: 13496
2025-01-30 23:37:40,348 (Self-play) Number of total training examples: 556134
2025-01-30 23:37:40,348 (Self-play) Number of total unique state-reward pairs: 393563
2025-01-30 23:37:40,348 (Self-play) Model 1 wins: 118. Draws: 9. Model 2 wins: 123
2025-01-30 23:41:21,589 (Training) Epoch: 1. Total loss: 1.691. Value loss: 0.449. Policy accuracy: 0.723
2025-01-30 23:43:25,401 (Training) Epoch: 2. Total loss: 1.680. Value loss: 0.442. Policy accuracy: 0.725
2025-01-30 23:45:31,704 (Training) Epoch: 3. Total loss: 1.677. Value loss: 0.439. Policy accuracy: 0.725
2025-01-30 23:47:37,261 (Training) Epoch: 4. Total loss: 1.673. Value loss: 0.438. Policy accuracy: 0.727
2025-01-30 23:49:43,889 (Training) Epoch: 5. Total loss: 1.667. Value loss: 0.434. Policy accuracy: 0.728
2025-01-30 23:51:49,740 (Training) Epoch: 6. Total loss: 1.661. Value loss: 0.430. Policy accuracy: 0.729
2025-01-30 23:53:56,358 (Training) Epoch: 7. Total loss: 1.659. Value loss: 0.429. Policy accuracy: 0.729
2025-01-30 23:56:03,128 (Training) Epoch: 8. Total loss: 1.656. Value loss: 0.427. Policy accuracy: 0.729
2025-01-30 23:58:08,715 (Training) Epoch: 9. Total loss: 1.652. Value loss: 0.426. Policy accuracy: 0.731
2025-01-31 00:00:13,519 (Training) Epoch: 10. Total loss: 1.650. Value loss: 0.425. Policy accuracy: 0.732
2025-01-31 00:09:47,106 (Evaluation) Win rate: 0.6
2025-01-31 00:09:47,106 (Evaluation) Model 1 wins: 15. Draws: 5. Model 2 wins: 30
2025-01-31 00:09:47,564 (Evaluation) Accepting new model...
2025-01-31 00:11:50,159 (Experiment) Win rate (random): 1.0
2025-01-31 00:16:54,437 (Experiment) Win rate (alpha-beta pruning with depth 5): 1.0
2025-01-31 00:16:54,437 

2025-01-31 00:16:54,452 Iteration 45
2025-01-31 00:56:52,993 (Self-play) Number of new training examples: 13894
2025-01-31 00:56:52,995 (Self-play) Number of total training examples: 570028
2025-01-31 00:56:52,995 (Self-play) Number of total unique state-reward pairs: 403158
2025-01-31 00:56:52,995 (Self-play) Model 1 wins: 119. Draws: 26. Model 2 wins: 105
2025-01-31 01:00:30,368 (Training) Epoch: 1. Total loss: 1.652. Value loss: 0.428. Policy accuracy: 0.732
2025-01-31 01:02:33,049 (Training) Epoch: 2. Total loss: 1.646. Value loss: 0.426. Policy accuracy: 0.733
2025-01-31 01:04:36,779 (Training) Epoch: 3. Total loss: 1.644. Value loss: 0.423. Policy accuracy: 0.732
2025-01-31 01:06:41,263 (Training) Epoch: 4. Total loss: 1.639. Value loss: 0.421. Policy accuracy: 0.734
2025-01-31 01:08:45,742 (Training) Epoch: 5. Total loss: 1.637. Value loss: 0.420. Policy accuracy: 0.734
2025-01-31 01:10:50,116 (Training) Epoch: 6. Total loss: 1.636. Value loss: 0.419. Policy accuracy: 0.734
2025-01-31 01:12:54,074 (Training) Epoch: 7. Total loss: 1.632. Value loss: 0.417. Policy accuracy: 0.735
2025-01-31 01:14:58,736 (Training) Epoch: 8. Total loss: 1.633. Value loss: 0.418. Policy accuracy: 0.735
2025-01-31 01:17:02,811 (Training) Epoch: 9. Total loss: 1.630. Value loss: 0.416. Policy accuracy: 0.735
2025-01-31 01:19:06,547 (Training) Epoch: 10. Total loss: 1.627. Value loss: 0.415. Policy accuracy: 0.736
2025-01-31 01:28:44,723 (Evaluation) Win rate: 0.36
2025-01-31 01:28:44,723 (Evaluation) Model 1 wins: 27. Draws: 5. Model 2 wins: 18
2025-01-31 01:28:45,124 (Evaluation) Rejecting new model...
2025-01-31 01:30:34,476 (Experiment) Win rate (random): 1.0
2025-01-31 01:35:51,972 (Experiment) Win rate (alpha-beta pruning with depth 5): 1.0
2025-01-31 01:35:51,973 

2025-01-31 01:35:51,973 Iteration 46
2025-01-31 02:12:28,793 (Self-play) Number of new training examples: 13058
2025-01-31 02:12:28,793 (Self-play) Number of total training examples: 583086
2025-01-31 02:12:28,794 (Self-play) Number of total unique state-reward pairs: 412074
2025-01-31 02:12:28,794 (Self-play) Model 1 wins: 131. Draws: 18. Model 2 wins: 101
2025-01-31 02:16:14,633 (Training) Epoch: 1. Total loss: 1.657. Value loss: 0.433. Policy accuracy: 0.732
2025-01-31 02:18:20,165 (Training) Epoch: 2. Total loss: 1.649. Value loss: 0.429. Policy accuracy: 0.733
2025-01-31 02:20:25,461 (Training) Epoch: 3. Total loss: 1.650. Value loss: 0.428. Policy accuracy: 0.732
2025-01-31 02:22:31,372 (Training) Epoch: 4. Total loss: 1.647. Value loss: 0.426. Policy accuracy: 0.733
2025-01-31 02:24:37,396 (Training) Epoch: 5. Total loss: 1.640. Value loss: 0.424. Policy accuracy: 0.735
2025-01-31 02:26:42,560 (Training) Epoch: 6. Total loss: 1.641. Value loss: 0.423. Policy accuracy: 0.734
2025-01-31 02:28:48,160 (Training) Epoch: 7. Total loss: 1.634. Value loss: 0.420. Policy accuracy: 0.735
2025-01-31 02:30:53,834 (Training) Epoch: 8. Total loss: 1.631. Value loss: 0.417. Policy accuracy: 0.735
2025-01-31 02:32:59,129 (Training) Epoch: 9. Total loss: 1.633. Value loss: 0.418. Policy accuracy: 0.736
2025-01-31 02:35:04,129 (Training) Epoch: 10. Total loss: 1.630. Value loss: 0.418. Policy accuracy: 0.738
2025-01-31 02:44:30,137 (Evaluation) Win rate: 0.36
2025-01-31 02:44:30,137 (Evaluation) Model 1 wins: 18. Draws: 14. Model 2 wins: 18
2025-01-31 02:44:30,519 (Evaluation) Rejecting new model...
2025-01-31 02:46:30,318 (Experiment) Win rate (random): 1.0
2025-01-31 02:51:20,529 (Experiment) Win rate (alpha-beta pruning with depth 5): 1.0
2025-01-31 02:51:20,530 

2025-01-31 02:51:20,530 Iteration 47
2025-01-31 03:28:43,658 (Self-play) Number of new training examples: 13582
2025-01-31 03:28:43,659 (Self-play) Number of total training examples: 596668
2025-01-31 03:28:43,659 (Self-play) Number of total unique state-reward pairs: 421363
2025-01-31 03:28:43,659 (Self-play) Model 1 wins: 121. Draws: 21. Model 2 wins: 108
2025-01-31 03:32:32,567 (Training) Epoch: 1. Total loss: 1.662. Value loss: 0.437. Policy accuracy: 0.731
2025-01-31 03:34:40,193 (Training) Epoch: 2. Total loss: 1.655. Value loss: 0.432. Policy accuracy: 0.732
2025-01-31 03:36:48,224 (Training) Epoch: 3. Total loss: 1.649. Value loss: 0.429. Policy accuracy: 0.733
2025-01-31 03:38:56,716 (Training) Epoch: 4. Total loss: 1.650. Value loss: 0.429. Policy accuracy: 0.732
2025-01-31 03:41:05,058 (Training) Epoch: 5. Total loss: 1.645. Value loss: 0.427. Policy accuracy: 0.734
2025-01-31 03:43:13,003 (Training) Epoch: 6. Total loss: 1.644. Value loss: 0.425. Policy accuracy: 0.733
2025-01-31 03:45:20,749 (Training) Epoch: 7. Total loss: 1.637. Value loss: 0.422. Policy accuracy: 0.734
2025-01-31 03:47:29,023 (Training) Epoch: 8. Total loss: 1.634. Value loss: 0.420. Policy accuracy: 0.736
2025-01-31 03:49:37,407 (Training) Epoch: 9. Total loss: 1.633. Value loss: 0.420. Policy accuracy: 0.737
2025-01-31 03:51:45,494 (Training) Epoch: 10. Total loss: 1.628. Value loss: 0.417. Policy accuracy: 0.737
2025-01-31 04:00:56,488 (Evaluation) Win rate: 0.44
2025-01-31 04:00:56,488 (Evaluation) Model 1 wins: 11. Draws: 17. Model 2 wins: 22
2025-01-31 04:00:56,850 (Evaluation) Rejecting new model...
2025-01-31 04:02:47,757 (Experiment) Win rate (random): 1.0
2025-01-31 04:07:49,147 (Experiment) Win rate (alpha-beta pruning with depth 5): 0.98
2025-01-31 04:07:49,147 

2025-01-31 04:07:49,149 Iteration 48
2025-01-31 04:46:41,775 (Self-play) Number of new training examples: 14310
2025-01-31 04:46:41,775 (Self-play) Number of total training examples: 610978
2025-01-31 04:46:41,775 (Self-play) Number of total unique state-reward pairs: 431376
2025-01-31 04:46:41,775 (Self-play) Model 1 wins: 105. Draws: 24. Model 2 wins: 121
2025-01-31 04:50:38,023 (Training) Epoch: 1. Total loss: 1.663. Value loss: 0.439. Policy accuracy: 0.732
2025-01-31 04:52:48,565 (Training) Epoch: 2. Total loss: 1.657. Value loss: 0.435. Policy accuracy: 0.732
2025-01-31 04:54:59,467 (Training) Epoch: 3. Total loss: 1.652. Value loss: 0.432. Policy accuracy: 0.733
2025-01-31 04:57:11,228 (Training) Epoch: 4. Total loss: 1.648. Value loss: 0.429. Policy accuracy: 0.734
2025-01-31 04:59:22,805 (Training) Epoch: 5. Total loss: 1.644. Value loss: 0.427. Policy accuracy: 0.734
2025-01-31 05:01:33,699 (Training) Epoch: 6. Total loss: 1.640. Value loss: 0.425. Policy accuracy: 0.735
2025-01-31 05:03:43,853 (Training) Epoch: 7. Total loss: 1.639. Value loss: 0.425. Policy accuracy: 0.736
2025-01-31 05:05:55,041 (Training) Epoch: 8. Total loss: 1.636. Value loss: 0.423. Policy accuracy: 0.735
2025-01-31 05:08:05,896 (Training) Epoch: 9. Total loss: 1.635. Value loss: 0.422. Policy accuracy: 0.736
2025-01-31 05:10:16,938 (Training) Epoch: 10. Total loss: 1.634. Value loss: 0.420. Policy accuracy: 0.736
2025-01-31 05:19:47,589 (Evaluation) Win rate: 0.42
2025-01-31 05:19:47,590 (Evaluation) Model 1 wins: 26. Draws: 3. Model 2 wins: 21
2025-01-31 05:19:47,958 (Evaluation) Rejecting new model...
2025-01-31 05:21:43,824 (Experiment) Win rate (random): 1.0
2025-01-31 05:26:31,214 (Experiment) Win rate (alpha-beta pruning with depth 5): 1.0
2025-01-31 05:26:31,214 

2025-01-31 05:26:31,215 Iteration 49
2025-01-31 06:04:21,424 (Self-play) Number of new training examples: 13718
2025-01-31 06:04:21,424 (Self-play) Number of total training examples: 624696
2025-01-31 06:04:21,424 (Self-play) Number of total unique state-reward pairs: 440793
2025-01-31 06:04:21,424 (Self-play) Model 1 wins: 115. Draws: 20. Model 2 wins: 115
2025-01-31 06:08:46,671 (Training) Epoch: 1. Total loss: 1.666. Value loss: 0.444. Policy accuracy: 0.731
2025-01-31 06:10:59,668 (Training) Epoch: 2. Total loss: 1.660. Value loss: 0.438. Policy accuracy: 0.731
2025-01-31 06:13:13,580 (Training) Epoch: 3. Total loss: 1.654. Value loss: 0.435. Policy accuracy: 0.733
2025-01-31 06:15:27,910 (Training) Epoch: 4. Total loss: 1.653. Value loss: 0.433. Policy accuracy: 0.733
2025-01-31 06:17:42,084 (Training) Epoch: 5. Total loss: 1.647. Value loss: 0.429. Policy accuracy: 0.734
2025-01-31 06:19:56,136 (Training) Epoch: 6. Total loss: 1.643. Value loss: 0.428. Policy accuracy: 0.735
2025-01-31 06:22:10,791 (Training) Epoch: 7. Total loss: 1.640. Value loss: 0.426. Policy accuracy: 0.735
2025-01-31 06:24:24,794 (Training) Epoch: 8. Total loss: 1.636. Value loss: 0.424. Policy accuracy: 0.736
2025-01-31 06:26:39,285 (Training) Epoch: 9. Total loss: 1.635. Value loss: 0.423. Policy accuracy: 0.735
2025-01-31 06:28:53,848 (Training) Epoch: 10. Total loss: 1.635. Value loss: 0.423. Policy accuracy: 0.736
2025-01-31 06:38:15,337 (Evaluation) Win rate: 0.3
2025-01-31 06:38:15,337 (Evaluation) Model 1 wins: 24. Draws: 11. Model 2 wins: 15
2025-01-31 06:38:15,717 (Evaluation) Rejecting new model...
2025-01-31 06:40:06,428 (Experiment) Win rate (random): 1.0
2025-01-31 06:45:12,323 (Experiment) Win rate (alpha-beta pruning with depth 5): 0.98
2025-01-31 06:45:12,323 

2025-01-31 06:45:12,326 Iteration 50
2025-01-31 07:23:57,271 (Self-play) Number of new training examples: 14236
2025-01-31 07:23:57,271 (Self-play) Number of total training examples: 638932
2025-01-31 07:23:57,271 (Self-play) Number of total unique state-reward pairs: 450684
2025-01-31 07:23:57,271 (Self-play) Model 1 wins: 112. Draws: 20. Model 2 wins: 118
2025-01-31 07:27:54,471 (Training) Epoch: 1. Total loss: 1.672. Value loss: 0.447. Policy accuracy: 0.731
2025-01-31 07:30:09,501 (Training) Epoch: 2. Total loss: 1.660. Value loss: 0.440. Policy accuracy: 0.732
2025-01-31 07:32:25,699 (Training) Epoch: 3. Total loss: 1.655. Value loss: 0.436. Policy accuracy: 0.733
2025-01-31 07:34:41,874 (Training) Epoch: 4. Total loss: 1.651. Value loss: 0.434. Policy accuracy: 0.734
2025-01-31 07:36:58,461 (Training) Epoch: 5. Total loss: 1.648. Value loss: 0.432. Policy accuracy: 0.735
2025-01-31 07:39:14,486 (Training) Epoch: 6. Total loss: 1.648. Value loss: 0.431. Policy accuracy: 0.735
2025-01-31 07:41:30,891 (Training) Epoch: 7. Total loss: 1.641. Value loss: 0.427. Policy accuracy: 0.736
2025-01-31 07:43:46,899 (Training) Epoch: 8. Total loss: 1.638. Value loss: 0.426. Policy accuracy: 0.736
2025-01-31 07:46:04,087 (Training) Epoch: 9. Total loss: 1.636. Value loss: 0.425. Policy accuracy: 0.736
2025-01-31 07:48:20,449 (Training) Epoch: 10. Total loss: 1.634. Value loss: 0.423. Policy accuracy: 0.737
2025-01-31 07:57:49,864 (Evaluation) Win rate: 0.46
2025-01-31 07:57:49,864 (Evaluation) Model 1 wins: 19. Draws: 8. Model 2 wins: 23
2025-01-31 07:57:50,235 (Evaluation) Rejecting new model...
2025-01-31 07:59:32,451 (Experiment) Win rate (random): 1.0
2025-01-31 08:04:34,690 (Experiment) Win rate (alpha-beta pruning with depth 5): 1.0
2025-01-31 08:04:34,691 

2025-01-31 08:04:34,693 Iteration 51
2025-01-31 08:41:35,905 (Self-play) Number of new training examples: 13462
2025-01-31 08:41:35,916 (Self-play) Number of total training examples: 652394
2025-01-31 08:41:35,916 (Self-play) Number of total unique state-reward pairs: 459883
2025-01-31 08:41:35,916 (Self-play) Model 1 wins: 125. Draws: 16. Model 2 wins: 109
2025-01-31 08:45:49,637 (Training) Epoch: 1. Total loss: 1.673. Value loss: 0.451. Policy accuracy: 0.731
2025-01-31 08:48:08,899 (Training) Epoch: 2. Total loss: 1.664. Value loss: 0.444. Policy accuracy: 0.732
2025-01-31 08:50:29,376 (Training) Epoch: 3. Total loss: 1.656. Value loss: 0.439. Policy accuracy: 0.734
2025-01-31 08:52:47,725 (Training) Epoch: 4. Total loss: 1.651. Value loss: 0.435. Policy accuracy: 0.733
2025-01-31 08:55:07,564 (Training) Epoch: 5. Total loss: 1.645. Value loss: 0.432. Policy accuracy: 0.734
2025-01-31 08:57:26,577 (Training) Epoch: 6. Total loss: 1.644. Value loss: 0.432. Policy accuracy: 0.736
2025-01-31 08:59:46,531 (Training) Epoch: 7. Total loss: 1.641. Value loss: 0.429. Policy accuracy: 0.735
2025-01-31 09:02:05,978 (Training) Epoch: 8. Total loss: 1.638. Value loss: 0.427. Policy accuracy: 0.736
2025-01-31 09:04:26,036 (Training) Epoch: 9. Total loss: 1.634. Value loss: 0.426. Policy accuracy: 0.738
2025-01-31 09:06:45,928 (Training) Epoch: 10. Total loss: 1.631. Value loss: 0.424. Policy accuracy: 0.737
2025-01-31 09:16:20,524 (Evaluation) Win rate: 0.18
2025-01-31 09:16:20,524 (Evaluation) Model 1 wins: 30. Draws: 11. Model 2 wins: 9
2025-01-31 09:16:20,878 (Evaluation) Rejecting new model...
2025-01-31 09:18:16,637 (Experiment) Win rate (random): 1.0
2025-01-31 09:23:16,743 (Experiment) Win rate (alpha-beta pruning with depth 5): 1.0
2025-01-31 09:23:16,743 

2025-01-31 09:23:16,744 Iteration 52
2025-01-31 10:01:11,057 (Self-play) Number of new training examples: 13870
2025-01-31 10:01:11,057 (Self-play) Number of total training examples: 666264
2025-01-31 10:01:11,057 (Self-play) Number of total unique state-reward pairs: 469463
2025-01-31 10:01:11,057 (Self-play) Model 1 wins: 125. Draws: 18. Model 2 wins: 107
2025-01-31 10:05:29,882 (Training) Epoch: 1. Total loss: 1.678. Value loss: 0.455. Policy accuracy: 0.731
2025-01-31 10:07:51,721 (Training) Epoch: 2. Total loss: 1.665. Value loss: 0.446. Policy accuracy: 0.733
2025-01-31 10:10:14,727 (Training) Epoch: 3. Total loss: 1.657. Value loss: 0.442. Policy accuracy: 0.734
2025-01-31 10:12:37,832 (Training) Epoch: 4. Total loss: 1.654. Value loss: 0.438. Policy accuracy: 0.734
2025-01-31 10:14:59,525 (Training) Epoch: 5. Total loss: 1.650. Value loss: 0.435. Policy accuracy: 0.734
2025-01-31 10:17:20,847 (Training) Epoch: 6. Total loss: 1.646. Value loss: 0.433. Policy accuracy: 0.735
2025-01-31 10:19:43,003 (Training) Epoch: 7. Total loss: 1.642. Value loss: 0.431. Policy accuracy: 0.735
2025-01-31 10:22:04,644 (Training) Epoch: 8. Total loss: 1.641. Value loss: 0.430. Policy accuracy: 0.736
2025-01-31 10:24:26,922 (Training) Epoch: 9. Total loss: 1.637. Value loss: 0.428. Policy accuracy: 0.737
2025-01-31 10:26:48,716 (Training) Epoch: 10. Total loss: 1.634. Value loss: 0.426. Policy accuracy: 0.738
2025-01-31 10:36:08,281 (Evaluation) Win rate: 0.5
2025-01-31 10:36:08,281 (Evaluation) Model 1 wins: 11. Draws: 14. Model 2 wins: 25
2025-01-31 10:36:08,621 (Evaluation) Rejecting new model...
2025-01-31 10:37:58,052 (Experiment) Win rate (random): 1.0
2025-01-31 10:43:26,104 (Experiment) Win rate (alpha-beta pruning with depth 5): 1.0
2025-01-31 10:43:26,104 

2025-01-31 10:43:26,105 Iteration 53
2025-01-31 11:23:00,098 (Self-play) Number of new training examples: 14818
2025-01-31 11:23:00,098 (Self-play) Number of total training examples: 681082
2025-01-31 11:23:00,098 (Self-play) Number of total unique state-reward pairs: 479768
2025-01-31 11:23:00,098 (Self-play) Model 1 wins: 115. Draws: 22. Model 2 wins: 113
2025-01-31 11:27:19,896 (Training) Epoch: 1. Total loss: 1.676. Value loss: 0.458. Policy accuracy: 0.731
2025-01-31 11:29:43,737 (Training) Epoch: 2. Total loss: 1.669. Value loss: 0.450. Policy accuracy: 0.733
2025-01-31 11:32:08,795 (Training) Epoch: 3. Total loss: 1.660. Value loss: 0.445. Policy accuracy: 0.734
2025-01-31 11:34:33,169 (Training) Epoch: 4. Total loss: 1.654. Value loss: 0.439. Policy accuracy: 0.735
2025-01-31 11:36:56,402 (Training) Epoch: 5. Total loss: 1.649. Value loss: 0.437. Policy accuracy: 0.736
2025-01-31 11:39:20,211 (Training) Epoch: 6. Total loss: 1.644. Value loss: 0.433. Policy accuracy: 0.736
2025-01-31 11:41:45,284 (Training) Epoch: 7. Total loss: 1.644. Value loss: 0.435. Policy accuracy: 0.737
2025-01-31 11:44:14,041 (Training) Epoch: 8. Total loss: 1.637. Value loss: 0.431. Policy accuracy: 0.737
2025-01-31 11:46:43,511 (Training) Epoch: 9. Total loss: 1.636. Value loss: 0.429. Policy accuracy: 0.738
2025-01-31 11:49:11,464 (Training) Epoch: 10. Total loss: 1.632. Value loss: 0.427. Policy accuracy: 0.739
2025-01-31 11:58:11,420 (Evaluation) Win rate: 0.52
2025-01-31 11:58:11,420 (Evaluation) Model 1 wins: 24. Draws: 0. Model 2 wins: 26
2025-01-31 11:58:11,833 (Evaluation) Rejecting new model...
2025-01-31 11:59:55,031 (Experiment) Win rate (random): 1.0
2025-01-31 12:05:13,209 (Experiment) Win rate (alpha-beta pruning with depth 5): 1.0
2025-01-31 12:05:13,210 

2025-01-31 12:05:13,211 Iteration 54
2025-01-31 12:43:06,838 (Self-play) Number of new training examples: 14142
2025-01-31 12:43:06,838 (Self-play) Number of total training examples: 695224
2025-01-31 12:43:06,838 (Self-play) Number of total unique state-reward pairs: 489604
2025-01-31 12:43:06,839 (Self-play) Model 1 wins: 113. Draws: 22. Model 2 wins: 115
2025-01-31 12:47:29,057 (Training) Epoch: 1. Total loss: 1.684. Value loss: 0.464. Policy accuracy: 0.731
2025-01-31 12:49:57,883 (Training) Epoch: 2. Total loss: 1.670. Value loss: 0.453. Policy accuracy: 0.733
2025-01-31 12:52:27,856 (Training) Epoch: 3. Total loss: 1.660. Value loss: 0.445. Policy accuracy: 0.734
2025-01-31 12:54:57,240 (Training) Epoch: 4. Total loss: 1.653. Value loss: 0.442. Policy accuracy: 0.735
2025-01-31 12:57:26,761 (Training) Epoch: 5. Total loss: 1.651. Value loss: 0.440. Policy accuracy: 0.735
2025-01-31 12:59:55,961 (Training) Epoch: 6. Total loss: 1.647. Value loss: 0.437. Policy accuracy: 0.736
2025-01-31 13:02:25,054 (Training) Epoch: 7. Total loss: 1.643. Value loss: 0.435. Policy accuracy: 0.737
2025-01-31 13:04:55,137 (Training) Epoch: 8. Total loss: 1.641. Value loss: 0.434. Policy accuracy: 0.738
2025-01-31 13:07:24,476 (Training) Epoch: 9. Total loss: 1.634. Value loss: 0.431. Policy accuracy: 0.739
2025-01-31 13:09:54,098 (Training) Epoch: 10. Total loss: 1.632. Value loss: 0.428. Policy accuracy: 0.737
2025-01-31 13:19:14,546 (Evaluation) Win rate: 0.54
2025-01-31 13:19:14,546 (Evaluation) Model 1 wins: 13. Draws: 10. Model 2 wins: 27
2025-01-31 13:19:14,914 (Evaluation) Rejecting new model...
2025-01-31 13:21:04,331 (Experiment) Win rate (random): 1.0
2025-01-31 13:25:47,207 (Experiment) Win rate (alpha-beta pruning with depth 5): 1.0
2025-01-31 13:25:47,207 

2025-01-31 13:25:47,208 Iteration 55
2025-01-31 14:03:48,338 (Self-play) Number of new training examples: 13920
2025-01-31 14:03:48,338 (Self-play) Number of total training examples: 709144
2025-01-31 14:03:48,339 (Self-play) Number of total unique state-reward pairs: 499116
2025-01-31 14:03:48,339 (Self-play) Model 1 wins: 118. Draws: 22. Model 2 wins: 110
2025-01-31 14:08:26,063 (Training) Epoch: 1. Total loss: 1.684. Value loss: 0.466. Policy accuracy: 0.732
2025-01-31 14:11:03,438 (Training) Epoch: 2. Total loss: 1.671. Value loss: 0.455. Policy accuracy: 0.733
2025-01-31 14:13:41,379 (Training) Epoch: 3. Total loss: 1.664. Value loss: 0.449. Policy accuracy: 0.734
2025-01-31 14:16:18,938 (Training) Epoch: 4. Total loss: 1.658. Value loss: 0.445. Policy accuracy: 0.736
2025-01-31 14:18:56,488 (Training) Epoch: 5. Total loss: 1.651. Value loss: 0.440. Policy accuracy: 0.736
2025-01-31 14:21:33,057 (Training) Epoch: 6. Total loss: 1.651. Value loss: 0.441. Policy accuracy: 0.736
2025-01-31 14:24:09,189 (Training) Epoch: 7. Total loss: 1.645. Value loss: 0.437. Policy accuracy: 0.738
2025-01-31 14:26:46,735 (Training) Epoch: 8. Total loss: 1.640. Value loss: 0.435. Policy accuracy: 0.737
2025-01-31 14:29:22,939 (Training) Epoch: 9. Total loss: 1.638. Value loss: 0.433. Policy accuracy: 0.738
2025-01-31 14:31:59,409 (Training) Epoch: 10. Total loss: 1.636. Value loss: 0.432. Policy accuracy: 0.738
2025-01-31 14:40:59,267 (Evaluation) Win rate: 0.58
2025-01-31 14:40:59,268 (Evaluation) Model 1 wins: 10. Draws: 11. Model 2 wins: 29
2025-01-31 14:40:59,743 (Evaluation) Accepting new model...
2025-01-31 14:42:56,578 (Experiment) Win rate (random): 1.0
2025-01-31 14:47:53,634 (Experiment) Win rate (alpha-beta pruning with depth 5): 0.98
2025-01-31 14:47:53,634 

2025-01-31 14:47:53,648 Iteration 56
2025-01-31 15:26:21,702 (Self-play) Number of new training examples: 14040
2025-01-31 15:26:21,702 (Self-play) Number of total training examples: 723184
2025-01-31 15:26:21,702 (Self-play) Number of total unique state-reward pairs: 508704
2025-01-31 15:26:21,702 (Self-play) Model 1 wins: 122. Draws: 27. Model 2 wins: 101
2025-01-31 15:31:05,309 (Training) Epoch: 1. Total loss: 1.636. Value loss: 0.434. Policy accuracy: 0.740
2025-01-31 15:33:42,851 (Training) Epoch: 2. Total loss: 1.630. Value loss: 0.430. Policy accuracy: 0.741
2025-01-31 15:36:20,465 (Training) Epoch: 3. Total loss: 1.628. Value loss: 0.428. Policy accuracy: 0.741
2025-01-31 15:38:57,103 (Training) Epoch: 4. Total loss: 1.624. Value loss: 0.427. Policy accuracy: 0.742
2025-01-31 15:41:36,265 (Training) Epoch: 5. Total loss: 1.624. Value loss: 0.427. Policy accuracy: 0.742
2025-01-31 15:44:15,114 (Training) Epoch: 6. Total loss: 1.620. Value loss: 0.425. Policy accuracy: 0.742
2025-01-31 15:46:51,302 (Training) Epoch: 7. Total loss: 1.620. Value loss: 0.424. Policy accuracy: 0.743
2025-01-31 15:49:27,977 (Training) Epoch: 8. Total loss: 1.618. Value loss: 0.423. Policy accuracy: 0.743
2025-01-31 15:52:05,378 (Training) Epoch: 9. Total loss: 1.615. Value loss: 0.422. Policy accuracy: 0.744
2025-01-31 15:54:42,606 (Training) Epoch: 10. Total loss: 1.613. Value loss: 0.421. Policy accuracy: 0.744
2025-01-31 16:04:16,870 (Evaluation) Win rate: 0.48
2025-01-31 16:04:16,870 (Evaluation) Model 1 wins: 13. Draws: 13. Model 2 wins: 24
2025-01-31 16:04:17,340 (Evaluation) Rejecting new model...
2025-01-31 16:06:23,299 (Experiment) Win rate (random): 1.0
2025-01-31 16:11:23,960 (Experiment) Win rate (alpha-beta pruning with depth 5): 1.0
2025-01-31 16:11:23,961 

2025-01-31 16:11:23,962 Iteration 57
2025-01-31 16:49:46,788 (Self-play) Number of new training examples: 13754
2025-01-31 16:49:46,789 (Self-play) Number of total training examples: 736938
2025-01-31 16:49:46,789 (Self-play) Number of total unique state-reward pairs: 518025
2025-01-31 16:49:46,789 (Self-play) Model 1 wins: 117. Draws: 21. Model 2 wins: 112
2025-01-31 16:54:51,106 (Training) Epoch: 1. Total loss: 1.637. Value loss: 0.436. Policy accuracy: 0.739
2025-01-31 16:57:39,102 (Training) Epoch: 2. Total loss: 1.634. Value loss: 0.434. Policy accuracy: 0.741
2025-01-31 17:00:26,441 (Training) Epoch: 3. Total loss: 1.629. Value loss: 0.431. Policy accuracy: 0.741
2025-01-31 17:03:16,173 (Training) Epoch: 4. Total loss: 1.627. Value loss: 0.430. Policy accuracy: 0.742
2025-01-31 17:06:06,492 (Training) Epoch: 5. Total loss: 1.626. Value loss: 0.429. Policy accuracy: 0.741
2025-01-31 17:08:51,139 (Training) Epoch: 6. Total loss: 1.625. Value loss: 0.428. Policy accuracy: 0.743
2025-01-31 17:11:33,090 (Training) Epoch: 7. Total loss: 1.622. Value loss: 0.426. Policy accuracy: 0.742
2025-01-31 17:14:15,756 (Training) Epoch: 8. Total loss: 1.619. Value loss: 0.425. Policy accuracy: 0.743
2025-01-31 17:17:01,114 (Training) Epoch: 9. Total loss: 1.616. Value loss: 0.423. Policy accuracy: 0.743
2025-01-31 17:19:49,573 (Training) Epoch: 10. Total loss: 1.615. Value loss: 0.422. Policy accuracy: 0.744
2025-01-31 17:29:13,836 (Evaluation) Win rate: 0.28
2025-01-31 17:29:13,836 (Evaluation) Model 1 wins: 23. Draws: 13. Model 2 wins: 14
2025-01-31 17:29:14,483 (Evaluation) Rejecting new model...
2025-01-31 17:31:13,534 (Experiment) Win rate (random): 1.0
2025-01-31 17:36:20,711 (Experiment) Win rate (alpha-beta pruning with depth 5): 1.0
2025-01-31 17:36:20,711 

2025-01-31 17:36:20,713 Iteration 58
2025-01-31 18:14:27,409 (Self-play) Number of new training examples: 13998
2025-01-31 18:14:27,409 (Self-play) Number of total training examples: 750936
2025-01-31 18:14:27,409 (Self-play) Number of total unique state-reward pairs: 527659
2025-01-31 18:14:27,409 (Self-play) Model 1 wins: 119. Draws: 17. Model 2 wins: 114
2025-01-31 18:19:25,436 (Training) Epoch: 1. Total loss: 1.640. Value loss: 0.440. Policy accuracy: 0.739
2025-01-31 18:22:11,053 (Training) Epoch: 2. Total loss: 1.636. Value loss: 0.437. Policy accuracy: 0.740
2025-01-31 18:24:59,754 (Training) Epoch: 3. Total loss: 1.635. Value loss: 0.435. Policy accuracy: 0.741
2025-01-31 18:27:47,702 (Training) Epoch: 4. Total loss: 1.631. Value loss: 0.433. Policy accuracy: 0.741
2025-01-31 18:30:34,698 (Training) Epoch: 5. Total loss: 1.626. Value loss: 0.430. Policy accuracy: 0.742
2025-01-31 18:33:21,296 (Training) Epoch: 6. Total loss: 1.622. Value loss: 0.428. Policy accuracy: 0.742
2025-01-31 18:36:08,383 (Training) Epoch: 7. Total loss: 1.622. Value loss: 0.427. Policy accuracy: 0.743
2025-01-31 18:38:53,708 (Training) Epoch: 8. Total loss: 1.620. Value loss: 0.426. Policy accuracy: 0.743
2025-01-31 18:41:38,069 (Training) Epoch: 9. Total loss: 1.617. Value loss: 0.424. Policy accuracy: 0.744
2025-01-31 18:44:19,917 (Training) Epoch: 10. Total loss: 1.614. Value loss: 0.422. Policy accuracy: 0.743
2025-01-31 18:54:02,771 (Evaluation) Win rate: 0.28
2025-01-31 18:54:02,771 (Evaluation) Model 1 wins: 12. Draws: 24. Model 2 wins: 14
2025-01-31 18:54:03,257 (Evaluation) Rejecting new model...
2025-01-31 18:55:55,152 (Experiment) Win rate (random): 1.0
2025-01-31 19:01:20,697 (Experiment) Win rate (alpha-beta pruning with depth 5): 1.0
2025-01-31 19:01:20,697 

2025-01-31 19:01:20,698 Iteration 59
2025-01-31 19:51:29,804 (Self-play) Number of new training examples: 14478
2025-01-31 19:51:29,805 (Self-play) Number of total training examples: 765414
2025-01-31 19:51:29,805 (Self-play) Number of total unique state-reward pairs: 537782
2025-01-31 19:51:29,805 (Self-play) Model 1 wins: 117. Draws: 26. Model 2 wins: 107
2025-01-31 19:57:29,527 (Training) Epoch: 1. Total loss: 1.641. Value loss: 0.441. Policy accuracy: 0.740
2025-01-31 20:02:48,499 (Training) Epoch: 2. Total loss: 1.638. Value loss: 0.438. Policy accuracy: 0.741
2025-01-31 20:07:46,945 (Training) Epoch: 3. Total loss: 1.633. Value loss: 0.435. Policy accuracy: 0.742
2025-01-31 20:12:40,962 (Training) Epoch: 4. Total loss: 1.629. Value loss: 0.432. Policy accuracy: 0.740
2025-01-31 20:16:19,979 (Training) Epoch: 5. Total loss: 1.627. Value loss: 0.432. Policy accuracy: 0.741
2025-01-31 20:20:38,687 (Training) Epoch: 6. Total loss: 1.623. Value loss: 0.429. Policy accuracy: 0.743
2025-01-31 20:24:30,200 (Training) Epoch: 7. Total loss: 1.624. Value loss: 0.430. Policy accuracy: 0.744
2025-01-31 20:28:08,981 (Training) Epoch: 8. Total loss: 1.617. Value loss: 0.426. Policy accuracy: 0.744
2025-01-31 20:31:48,483 (Training) Epoch: 9. Total loss: 1.618. Value loss: 0.426. Policy accuracy: 0.744
2025-01-31 20:36:12,869 (Training) Epoch: 10. Total loss: 1.612. Value loss: 0.423. Policy accuracy: 0.745
2025-01-31 20:49:30,335 (Evaluation) Win rate: 0.1
2025-01-31 20:49:30,336 (Evaluation) Model 1 wins: 24. Draws: 21. Model 2 wins: 5
2025-01-31 20:49:31,799 (Evaluation) Rejecting new model...
2025-01-31 20:52:06,497 (Experiment) Win rate (random): 1.0
2025-01-31 20:59:37,323 (Experiment) Win rate (alpha-beta pruning with depth 5): 1.0
2025-01-31 20:59:37,323 

2025-01-31 20:59:37,347 Iteration 60
2025-01-31 22:05:19,034 (Self-play) Number of new training examples: 14096
2025-01-31 22:05:19,043 (Self-play) Number of total training examples: 779510
2025-01-31 22:05:19,043 (Self-play) Number of total unique state-reward pairs: 547366
2025-01-31 22:05:19,043 (Self-play) Model 1 wins: 120. Draws: 17. Model 2 wins: 113
2025-01-31 22:12:36,626 (Training) Epoch: 1. Total loss: 1.649. Value loss: 0.448. Policy accuracy: 0.740
2025-01-31 22:16:59,725 (Training) Epoch: 2. Total loss: 1.641. Value loss: 0.442. Policy accuracy: 0.741
2025-01-31 22:21:29,721 (Training) Epoch: 3. Total loss: 1.635. Value loss: 0.438. Policy accuracy: 0.741
2025-01-31 22:25:46,503 (Training) Epoch: 4. Total loss: 1.631. Value loss: 0.436. Policy accuracy: 0.742
2025-01-31 22:30:15,270 (Training) Epoch: 5. Total loss: 1.629. Value loss: 0.435. Policy accuracy: 0.743
2025-01-31 22:33:31,643 (Training) Epoch: 6. Total loss: 1.624. Value loss: 0.431. Policy accuracy: 0.743
2025-01-31 22:36:31,362 (Training) Epoch: 7. Total loss: 1.622. Value loss: 0.431. Policy accuracy: 0.744
2025-01-31 22:39:32,633 (Training) Epoch: 8. Total loss: 1.620. Value loss: 0.429. Policy accuracy: 0.743
2025-01-31 22:42:31,310 (Training) Epoch: 9. Total loss: 1.617. Value loss: 0.427. Policy accuracy: 0.744
2025-01-31 22:45:29,331 (Training) Epoch: 10. Total loss: 1.618. Value loss: 0.428. Policy accuracy: 0.745
2025-01-31 22:55:02,698 (Evaluation) Win rate: 0.22
2025-01-31 22:55:02,698 (Evaluation) Model 1 wins: 15. Draws: 24. Model 2 wins: 11
2025-01-31 22:55:04,018 (Evaluation) Rejecting new model...
2025-01-31 22:56:50,614 (Experiment) Win rate (random): 1.0
2025-01-31 23:02:00,337 (Experiment) Win rate (alpha-beta pruning with depth 5): 1.0
2025-01-31 23:02:00,338 

2025-01-31 23:02:00,373 Iteration 61
2025-01-31 23:43:34,377 (Self-play) Number of new training examples: 14072
2025-01-31 23:43:34,377 (Self-play) Number of total training examples: 793582
2025-01-31 23:43:34,378 (Self-play) Number of total unique state-reward pairs: 556880
2025-01-31 23:43:34,378 (Self-play) Model 1 wins: 120. Draws: 25. Model 2 wins: 105
2025-01-31 23:49:03,341 (Training) Epoch: 1. Total loss: 1.649. Value loss: 0.450. Policy accuracy: 0.740
2025-01-31 23:52:07,190 (Training) Epoch: 2. Total loss: 1.640. Value loss: 0.443. Policy accuracy: 0.741
2025-01-31 23:55:12,194 (Training) Epoch: 3. Total loss: 1.635. Value loss: 0.439. Policy accuracy: 0.742
2025-01-31 23:58:15,295 (Training) Epoch: 4. Total loss: 1.631. Value loss: 0.437. Policy accuracy: 0.742
2025-02-01 00:01:20,283 (Training) Epoch: 5. Total loss: 1.631. Value loss: 0.436. Policy accuracy: 0.742
2025-02-01 00:04:20,070 (Training) Epoch: 6. Total loss: 1.628. Value loss: 0.434. Policy accuracy: 0.744
2025-02-01 00:07:22,971 (Training) Epoch: 7. Total loss: 1.624. Value loss: 0.431. Policy accuracy: 0.744
2025-02-01 00:10:27,840 (Training) Epoch: 8. Total loss: 1.622. Value loss: 0.430. Policy accuracy: 0.744
2025-02-01 00:13:32,622 (Training) Epoch: 9. Total loss: 1.619. Value loss: 0.429. Policy accuracy: 0.744
2025-02-01 00:16:37,622 (Training) Epoch: 10. Total loss: 1.618. Value loss: 0.428. Policy accuracy: 0.745
2025-02-01 00:25:58,240 (Evaluation) Win rate: 0.24
2025-02-01 00:25:58,241 (Evaluation) Model 1 wins: 29. Draws: 9. Model 2 wins: 12
2025-02-01 00:25:58,817 (Evaluation) Rejecting new model...
2025-02-01 00:27:55,603 (Experiment) Win rate (random): 1.0
2025-02-01 00:33:19,280 (Experiment) Win rate (alpha-beta pruning with depth 5): 1.0
2025-02-01 00:33:19,281 

2025-02-01 00:33:19,290 Iteration 62
2025-02-01 01:20:05,508 (Self-play) Number of new training examples: 14566
2025-02-01 01:20:05,522 (Self-play) Number of total training examples: 808148
2025-02-01 01:20:05,522 (Self-play) Number of total unique state-reward pairs: 566768
2025-02-01 01:20:05,522 (Self-play) Model 1 wins: 125. Draws: 32. Model 2 wins: 93
2025-02-01 01:25:27,345 (Training) Epoch: 1. Total loss: 1.650. Value loss: 0.451. Policy accuracy: 0.741
2025-02-01 01:28:27,054 (Training) Epoch: 2. Total loss: 1.640. Value loss: 0.445. Policy accuracy: 0.742
2025-02-01 01:31:27,624 (Training) Epoch: 3. Total loss: 1.634. Value loss: 0.440. Policy accuracy: 0.742
2025-02-01 01:34:28,042 (Training) Epoch: 4. Total loss: 1.631. Value loss: 0.438. Policy accuracy: 0.743
2025-02-01 01:37:27,300 (Training) Epoch: 5. Total loss: 1.631. Value loss: 0.436. Policy accuracy: 0.742
2025-02-01 01:40:21,767 (Training) Epoch: 6. Total loss: 1.627. Value loss: 0.434. Policy accuracy: 0.742
2025-02-01 01:43:19,440 (Training) Epoch: 7. Total loss: 1.621. Value loss: 0.432. Policy accuracy: 0.744
2025-02-01 01:46:18,099 (Training) Epoch: 8. Total loss: 1.620. Value loss: 0.431. Policy accuracy: 0.744
2025-02-01 01:49:15,871 (Training) Epoch: 9. Total loss: 1.617. Value loss: 0.430. Policy accuracy: 0.745
2025-02-01 01:52:11,485 (Training) Epoch: 10. Total loss: 1.614. Value loss: 0.428. Policy accuracy: 0.746
2025-02-01 02:01:25,636 (Evaluation) Win rate: 0.54
2025-02-01 02:01:25,636 (Evaluation) Model 1 wins: 14. Draws: 9. Model 2 wins: 27
2025-02-01 02:01:26,045 (Evaluation) Rejecting new model...
2025-02-01 02:03:33,702 (Experiment) Win rate (random): 1.0
2025-02-01 02:08:21,636 (Experiment) Win rate (alpha-beta pruning with depth 5): 1.0
2025-02-01 02:08:21,636 

2025-02-01 02:08:21,637 Iteration 63
2025-02-01 02:45:16,450 (Self-play) Number of new training examples: 13330
2025-02-01 02:45:16,450 (Self-play) Number of total training examples: 821478
2025-02-01 02:45:16,450 (Self-play) Number of total unique state-reward pairs: 575570
2025-02-01 02:45:16,450 (Self-play) Model 1 wins: 133. Draws: 14. Model 2 wins: 103
2025-02-01 02:50:32,313 (Training) Epoch: 1. Total loss: 1.650. Value loss: 0.452. Policy accuracy: 0.740
2025-02-01 02:53:28,308 (Training) Epoch: 2. Total loss: 1.643. Value loss: 0.447. Policy accuracy: 0.741
2025-02-01 02:56:23,844 (Training) Epoch: 3. Total loss: 1.635. Value loss: 0.441. Policy accuracy: 0.742
2025-02-01 02:59:19,037 (Training) Epoch: 4. Total loss: 1.633. Value loss: 0.440. Policy accuracy: 0.742
2025-02-01 03:02:13,856 (Training) Epoch: 5. Total loss: 1.631. Value loss: 0.438. Policy accuracy: 0.743
2025-02-01 03:05:09,578 (Training) Epoch: 6. Total loss: 1.625. Value loss: 0.435. Policy accuracy: 0.745
2025-02-01 03:08:05,307 (Training) Epoch: 7. Total loss: 1.623. Value loss: 0.433. Policy accuracy: 0.744
2025-02-01 03:11:01,692 (Training) Epoch: 8. Total loss: 1.622. Value loss: 0.432. Policy accuracy: 0.745
2025-02-01 03:13:57,170 (Training) Epoch: 9. Total loss: 1.617. Value loss: 0.430. Policy accuracy: 0.745
2025-02-01 03:16:53,055 (Training) Epoch: 10. Total loss: 1.618. Value loss: 0.430. Policy accuracy: 0.745
2025-02-01 03:26:41,261 (Evaluation) Win rate: 0.18
2025-02-01 03:26:41,261 (Evaluation) Model 1 wins: 12. Draws: 29. Model 2 wins: 9
2025-02-01 03:26:41,601 (Evaluation) Rejecting new model...
2025-02-01 03:28:41,162 (Experiment) Win rate (random): 1.0
2025-02-01 03:33:29,857 (Experiment) Win rate (alpha-beta pruning with depth 5): 0.96
2025-02-01 03:33:29,857 

2025-02-01 03:33:29,858 Iteration 64
2025-02-01 04:12:13,921 (Self-play) Number of new training examples: 14042
2025-02-01 04:12:13,922 (Self-play) Number of total training examples: 835520
2025-02-01 04:12:13,922 (Self-play) Number of total unique state-reward pairs: 584969
2025-02-01 04:12:13,922 (Self-play) Model 1 wins: 119. Draws: 20. Model 2 wins: 111
2025-02-01 04:17:35,701 (Training) Epoch: 1. Total loss: 1.652. Value loss: 0.455. Policy accuracy: 0.741
2025-02-01 04:20:32,971 (Training) Epoch: 2. Total loss: 1.641. Value loss: 0.447. Policy accuracy: 0.741
2025-02-01 04:23:31,094 (Training) Epoch: 3. Total loss: 1.637. Value loss: 0.444. Policy accuracy: 0.743
2025-02-01 04:26:28,461 (Training) Epoch: 4. Total loss: 1.634. Value loss: 0.441. Policy accuracy: 0.743
2025-02-01 04:29:27,233 (Training) Epoch: 5. Total loss: 1.627. Value loss: 0.438. Policy accuracy: 0.744
2025-02-01 04:32:25,435 (Training) Epoch: 6. Total loss: 1.628. Value loss: 0.437. Policy accuracy: 0.744
2025-02-01 04:35:23,925 (Training) Epoch: 7. Total loss: 1.623. Value loss: 0.434. Policy accuracy: 0.745
2025-02-01 04:38:22,239 (Training) Epoch: 8. Total loss: 1.620. Value loss: 0.432. Policy accuracy: 0.746
2025-02-01 04:41:20,295 (Training) Epoch: 9. Total loss: 1.618. Value loss: 0.432. Policy accuracy: 0.746
2025-02-01 04:44:18,744 (Training) Epoch: 10. Total loss: 1.619. Value loss: 0.431. Policy accuracy: 0.745
2025-02-01 04:53:26,720 (Evaluation) Win rate: 0.22
2025-02-01 04:53:26,720 (Evaluation) Model 1 wins: 22. Draws: 17. Model 2 wins: 11
2025-02-01 04:53:27,096 (Evaluation) Rejecting new model...
2025-02-01 04:55:11,926 (Experiment) Win rate (random): 1.0
2025-02-01 05:00:07,877 (Experiment) Win rate (alpha-beta pruning with depth 5): 1.0
2025-02-01 05:00:07,877 

2025-02-01 05:00:07,878 Iteration 65
2025-02-01 05:37:52,197 (Self-play) Number of new training examples: 13430
2025-02-01 05:37:52,197 (Self-play) Number of total training examples: 848950
2025-02-01 05:37:52,197 (Self-play) Number of total unique state-reward pairs: 593863
2025-02-01 05:37:52,198 (Self-play) Model 1 wins: 123. Draws: 24. Model 2 wins: 103
2025-02-01 05:43:20,235 (Training) Epoch: 1. Total loss: 1.654. Value loss: 0.457. Policy accuracy: 0.741
2025-02-01 05:46:21,264 (Training) Epoch: 2. Total loss: 1.646. Value loss: 0.450. Policy accuracy: 0.741
2025-02-01 05:49:22,793 (Training) Epoch: 3. Total loss: 1.637. Value loss: 0.445. Policy accuracy: 0.742
2025-02-01 05:52:22,534 (Training) Epoch: 4. Total loss: 1.634. Value loss: 0.442. Policy accuracy: 0.743
2025-02-01 05:55:23,734 (Training) Epoch: 5. Total loss: 1.630. Value loss: 0.439. Policy accuracy: 0.744
2025-02-01 05:58:26,114 (Training) Epoch: 6. Total loss: 1.625. Value loss: 0.437. Policy accuracy: 0.745
2025-02-01 06:01:27,392 (Training) Epoch: 7. Total loss: 1.623. Value loss: 0.436. Policy accuracy: 0.744
2025-02-01 06:04:28,619 (Training) Epoch: 8. Total loss: 1.624. Value loss: 0.436. Policy accuracy: 0.746
2025-02-01 06:07:29,531 (Training) Epoch: 9. Total loss: 1.620. Value loss: 0.433. Policy accuracy: 0.745
2025-02-01 06:10:29,896 (Training) Epoch: 10. Total loss: 1.616. Value loss: 0.431. Policy accuracy: 0.746
2025-02-01 06:20:23,705 (Evaluation) Win rate: 0.22
2025-02-01 06:20:23,705 (Evaluation) Model 1 wins: 15. Draws: 24. Model 2 wins: 11
2025-02-01 06:20:24,044 (Evaluation) Rejecting new model...
2025-02-01 06:22:13,653 (Experiment) Win rate (random): 1.0
2025-02-01 06:27:02,972 (Experiment) Win rate (alpha-beta pruning with depth 5): 1.0
2025-02-01 06:27:02,973 

2025-02-01 06:27:02,974 Iteration 66
2025-02-01 07:05:19,506 (Self-play) Number of new training examples: 13746
2025-02-01 07:05:19,506 (Self-play) Number of total training examples: 862696
2025-02-01 07:05:19,507 (Self-play) Number of total unique state-reward pairs: 603214
2025-02-01 07:05:19,507 (Self-play) Model 1 wins: 123. Draws: 21. Model 2 wins: 106
2025-02-01 07:10:40,197 (Training) Epoch: 1. Total loss: 1.655. Value loss: 0.459. Policy accuracy: 0.741
2025-02-01 07:13:44,250 (Training) Epoch: 2. Total loss: 1.647. Value loss: 0.451. Policy accuracy: 0.741
2025-02-01 07:16:49,566 (Training) Epoch: 3. Total loss: 1.638. Value loss: 0.446. Policy accuracy: 0.743
2025-02-01 07:19:54,765 (Training) Epoch: 4. Total loss: 1.635. Value loss: 0.444. Policy accuracy: 0.743
2025-02-01 07:22:59,858 (Training) Epoch: 5. Total loss: 1.632. Value loss: 0.441. Policy accuracy: 0.744
2025-02-01 07:26:04,409 (Training) Epoch: 6. Total loss: 1.629. Value loss: 0.440. Policy accuracy: 0.745
2025-02-01 07:29:08,917 (Training) Epoch: 7. Total loss: 1.625. Value loss: 0.438. Policy accuracy: 0.745
2025-02-01 07:32:12,859 (Training) Epoch: 8. Total loss: 1.624. Value loss: 0.437. Policy accuracy: 0.746
2025-02-01 07:35:17,499 (Training) Epoch: 9. Total loss: 1.619. Value loss: 0.433. Policy accuracy: 0.746
2025-02-01 07:38:22,148 (Training) Epoch: 10. Total loss: 1.619. Value loss: 0.433. Policy accuracy: 0.746
2025-02-01 07:47:04,176 (Evaluation) Win rate: 0.46
2025-02-01 07:47:04,177 (Evaluation) Model 1 wins: 22. Draws: 5. Model 2 wins: 23
2025-02-01 07:47:04,568 (Evaluation) Rejecting new model...
2025-02-01 07:48:50,515 (Experiment) Win rate (random): 1.0
2025-02-01 07:54:11,761 (Experiment) Win rate (alpha-beta pruning with depth 5): 0.94
2025-02-01 07:54:11,761 

2025-02-01 07:54:11,762 Iteration 67
2025-02-01 08:32:16,859 (Self-play) Number of new training examples: 13684
2025-02-01 08:32:16,860 (Self-play) Number of total training examples: 876380
2025-02-01 08:32:16,860 (Self-play) Number of total unique state-reward pairs: 612391
2025-02-01 08:32:16,860 (Self-play) Model 1 wins: 118. Draws: 20. Model 2 wins: 112
2025-02-01 08:37:52,549 (Training) Epoch: 1. Total loss: 1.658. Value loss: 0.462. Policy accuracy: 0.741
2025-02-01 08:40:59,994 (Training) Epoch: 2. Total loss: 1.647. Value loss: 0.453. Policy accuracy: 0.742
2025-02-01 08:44:06,931 (Training) Epoch: 3. Total loss: 1.640. Value loss: 0.449. Policy accuracy: 0.744
2025-02-01 08:47:15,568 (Training) Epoch: 4. Total loss: 1.639. Value loss: 0.447. Policy accuracy: 0.744
2025-02-01 08:50:22,234 (Training) Epoch: 5. Total loss: 1.631. Value loss: 0.441. Policy accuracy: 0.744
2025-02-01 08:53:30,321 (Training) Epoch: 6. Total loss: 1.631. Value loss: 0.441. Policy accuracy: 0.745
2025-02-01 08:56:37,899 (Training) Epoch: 7. Total loss: 1.625. Value loss: 0.437. Policy accuracy: 0.745
2025-02-01 08:59:45,376 (Training) Epoch: 8. Total loss: 1.620. Value loss: 0.435. Policy accuracy: 0.746
2025-02-01 09:02:52,942 (Training) Epoch: 9. Total loss: 1.619. Value loss: 0.435. Policy accuracy: 0.746
2025-02-01 09:06:01,054 (Training) Epoch: 10. Total loss: 1.619. Value loss: 0.434. Policy accuracy: 0.747
2025-02-01 09:14:57,998 (Evaluation) Win rate: 0.18
2025-02-01 09:14:57,998 (Evaluation) Model 1 wins: 16. Draws: 25. Model 2 wins: 9
2025-02-01 09:14:58,382 (Evaluation) Rejecting new model...
2025-02-01 09:16:49,381 (Experiment) Win rate (random): 1.0
2025-02-01 09:21:31,919 (Experiment) Win rate (alpha-beta pruning with depth 5): 1.0
2025-02-01 09:21:31,920 

2025-02-01 09:21:31,922 Iteration 68
2025-02-01 10:00:50,516 (Self-play) Number of new training examples: 13994
2025-02-01 10:00:50,516 (Self-play) Number of total training examples: 890374
2025-02-01 10:00:50,516 (Self-play) Number of total unique state-reward pairs: 621870
2025-02-01 10:00:50,516 (Self-play) Model 1 wins: 119. Draws: 22. Model 2 wins: 109
2025-02-01 10:06:36,368 (Training) Epoch: 1. Total loss: 1.657. Value loss: 0.462. Policy accuracy: 0.741
2025-02-01 10:09:46,780 (Training) Epoch: 2. Total loss: 1.644. Value loss: 0.453. Policy accuracy: 0.744
2025-02-01 10:12:58,320 (Training) Epoch: 3. Total loss: 1.641. Value loss: 0.450. Policy accuracy: 0.744
2025-02-01 10:16:09,350 (Training) Epoch: 4. Total loss: 1.636. Value loss: 0.446. Policy accuracy: 0.743
2025-02-01 10:19:20,644 (Training) Epoch: 5. Total loss: 1.632. Value loss: 0.443. Policy accuracy: 0.744
2025-02-01 10:22:30,837 (Training) Epoch: 6. Total loss: 1.627. Value loss: 0.440. Policy accuracy: 0.745
2025-02-01 10:25:40,911 (Training) Epoch: 7. Total loss: 1.623. Value loss: 0.439. Policy accuracy: 0.747
2025-02-01 10:28:50,792 (Training) Epoch: 8. Total loss: 1.623. Value loss: 0.438. Policy accuracy: 0.746
2025-02-01 10:32:01,599 (Training) Epoch: 9. Total loss: 1.620. Value loss: 0.436. Policy accuracy: 0.747
2025-02-01 10:35:11,199 (Training) Epoch: 10. Total loss: 1.617. Value loss: 0.433. Policy accuracy: 0.747
2025-02-01 10:44:53,612 (Evaluation) Win rate: 0.34
2025-02-01 10:44:53,612 (Evaluation) Model 1 wins: 11. Draws: 22. Model 2 wins: 17
2025-02-01 10:44:54,000 (Evaluation) Rejecting new model...
2025-02-01 10:46:44,351 (Experiment) Win rate (random): 1.0
2025-02-01 10:51:41,835 (Experiment) Win rate (alpha-beta pruning with depth 5): 0.96
2025-02-01 10:51:41,835 

2025-02-01 10:51:41,836 Iteration 69
2025-02-01 11:30:48,051 (Self-play) Number of new training examples: 13704
2025-02-01 11:30:48,051 (Self-play) Number of total training examples: 904078
2025-02-01 11:30:48,051 (Self-play) Number of total unique state-reward pairs: 631060
2025-02-01 11:30:48,051 (Self-play) Model 1 wins: 106. Draws: 18. Model 2 wins: 126
2025-02-01 11:36:19,759 (Training) Epoch: 1. Total loss: 1.660. Value loss: 0.464. Policy accuracy: 0.741
2025-02-01 11:39:31,393 (Training) Epoch: 2. Total loss: 1.648. Value loss: 0.455. Policy accuracy: 0.743
2025-02-01 11:42:42,896 (Training) Epoch: 3. Total loss: 1.642. Value loss: 0.451. Policy accuracy: 0.743
2025-02-01 11:45:55,155 (Training) Epoch: 4. Total loss: 1.639. Value loss: 0.447. Policy accuracy: 0.744
2025-02-01 11:49:07,659 (Training) Epoch: 5. Total loss: 1.631. Value loss: 0.444. Policy accuracy: 0.746
2025-02-01 11:52:20,317 (Training) Epoch: 6. Total loss: 1.627. Value loss: 0.440. Policy accuracy: 0.745
2025-02-01 11:55:32,326 (Training) Epoch: 7. Total loss: 1.625. Value loss: 0.439. Policy accuracy: 0.746
2025-02-01 11:58:45,317 (Training) Epoch: 8. Total loss: 1.622. Value loss: 0.437. Policy accuracy: 0.747
2025-02-01 12:01:56,991 (Training) Epoch: 9. Total loss: 1.621. Value loss: 0.436. Policy accuracy: 0.747
2025-02-01 12:05:09,693 (Training) Epoch: 10. Total loss: 1.616. Value loss: 0.434. Policy accuracy: 0.748
2025-02-01 12:14:32,417 (Evaluation) Win rate: 0.26
2025-02-01 12:14:32,418 (Evaluation) Model 1 wins: 11. Draws: 26. Model 2 wins: 13
2025-02-01 12:14:32,799 (Evaluation) Rejecting new model...
2025-02-01 12:16:25,854 (Experiment) Win rate (random): 1.0
2025-02-01 12:21:46,112 (Experiment) Win rate (alpha-beta pruning with depth 5): 1.0
2025-02-01 12:21:46,113 

2025-02-01 12:21:46,114 Iteration 70
2025-02-01 13:01:18,391 (Self-play) Number of new training examples: 14092
2025-02-01 13:01:18,391 (Self-play) Number of total training examples: 918170
2025-02-01 13:01:18,391 (Self-play) Number of total unique state-reward pairs: 640493
2025-02-01 13:01:18,391 (Self-play) Model 1 wins: 128. Draws: 20. Model 2 wins: 102
2025-02-01 13:07:11,767 (Training) Epoch: 1. Total loss: 1.660. Value loss: 0.466. Policy accuracy: 0.741
2025-02-01 13:10:27,397 (Training) Epoch: 2. Total loss: 1.649. Value loss: 0.456. Policy accuracy: 0.742
2025-02-01 13:13:46,071 (Training) Epoch: 3. Total loss: 1.643. Value loss: 0.451. Policy accuracy: 0.743
2025-02-01 13:17:04,484 (Training) Epoch: 4. Total loss: 1.638. Value loss: 0.449. Policy accuracy: 0.745
2025-02-01 13:20:21,095 (Training) Epoch: 5. Total loss: 1.634. Value loss: 0.445. Policy accuracy: 0.744
2025-02-01 13:23:38,130 (Training) Epoch: 6. Total loss: 1.628. Value loss: 0.443. Policy accuracy: 0.745
2025-02-01 13:26:55,143 (Training) Epoch: 7. Total loss: 1.625. Value loss: 0.440. Policy accuracy: 0.746
2025-02-01 13:30:12,195 (Training) Epoch: 8. Total loss: 1.622. Value loss: 0.438. Policy accuracy: 0.747
2025-02-01 13:33:29,424 (Training) Epoch: 9. Total loss: 1.618. Value loss: 0.436. Policy accuracy: 0.747
2025-02-01 13:36:44,907 (Training) Epoch: 10. Total loss: 1.618. Value loss: 0.436. Policy accuracy: 0.748
2025-02-01 13:45:52,035 (Evaluation) Win rate: 0.5
2025-02-01 13:45:52,035 (Evaluation) Model 1 wins: 5. Draws: 20. Model 2 wins: 25
2025-02-01 13:45:52,400 (Evaluation) Rejecting new model...
2025-02-01 13:47:42,167 (Experiment) Win rate (random): 1.0
2025-02-01 13:52:43,103 (Experiment) Win rate (alpha-beta pruning with depth 5): 1.0
2025-02-01 13:52:43,103 

2025-02-01 13:52:43,105 Iteration 71
2025-02-01 14:31:58,921 (Self-play) Number of new training examples: 13780
2025-02-01 14:31:58,921 (Self-play) Number of total training examples: 931950
2025-02-01 14:31:58,921 (Self-play) Number of total unique state-reward pairs: 649520
2025-02-01 14:31:58,921 (Self-play) Model 1 wins: 132. Draws: 21. Model 2 wins: 97
2025-02-01 14:37:46,758 (Training) Epoch: 1. Total loss: 1.661. Value loss: 0.468. Policy accuracy: 0.742
2025-02-01 14:41:24,751 (Training) Epoch: 2. Total loss: 1.650. Value loss: 0.459. Policy accuracy: 0.743
2025-02-01 14:44:57,732 (Training) Epoch: 3. Total loss: 1.641. Value loss: 0.452. Policy accuracy: 0.745
2025-02-01 14:48:28,624 (Training) Epoch: 4. Total loss: 1.639. Value loss: 0.449. Policy accuracy: 0.745
2025-02-01 14:52:00,002 (Training) Epoch: 5. Total loss: 1.631. Value loss: 0.445. Policy accuracy: 0.745
2025-02-01 14:55:29,606 (Training) Epoch: 6. Total loss: 1.628. Value loss: 0.443. Policy accuracy: 0.747
2025-02-01 14:58:55,132 (Training) Epoch: 7. Total loss: 1.626. Value loss: 0.442. Policy accuracy: 0.747
2025-02-01 15:02:19,930 (Training) Epoch: 8. Total loss: 1.622. Value loss: 0.439. Policy accuracy: 0.747
2025-02-01 15:05:45,056 (Training) Epoch: 9. Total loss: 1.619. Value loss: 0.437. Policy accuracy: 0.748
2025-02-01 15:09:10,411 (Training) Epoch: 10. Total loss: 1.617. Value loss: 0.436. Policy accuracy: 0.748
2025-02-01 15:18:34,188 (Evaluation) Win rate: 0.24
2025-02-01 15:18:34,188 (Evaluation) Model 1 wins: 18. Draws: 20. Model 2 wins: 12
2025-02-01 15:18:34,895 (Evaluation) Rejecting new model...
2025-02-01 15:20:22,799 (Experiment) Win rate (random): 1.0
2025-02-01 15:25:30,511 (Experiment) Win rate (alpha-beta pruning with depth 5): 0.96
2025-02-01 15:25:30,511 

2025-02-01 15:25:30,551 Iteration 72
2025-02-01 16:05:17,016 (Self-play) Number of new training examples: 13914
2025-02-01 16:05:17,023 (Self-play) Number of total training examples: 945864
2025-02-01 16:05:17,023 (Self-play) Number of total unique state-reward pairs: 658945
2025-02-01 16:05:17,023 (Self-play) Model 1 wins: 111. Draws: 16. Model 2 wins: 123
2025-02-01 16:11:45,372 (Training) Epoch: 1. Total loss: 1.664. Value loss: 0.470. Policy accuracy: 0.742
2025-02-01 16:15:20,396 (Training) Epoch: 2. Total loss: 1.649. Value loss: 0.459. Policy accuracy: 0.743
2025-02-01 16:18:55,568 (Training) Epoch: 3. Total loss: 1.643. Value loss: 0.454. Policy accuracy: 0.744
2025-02-01 16:22:32,890 (Training) Epoch: 4. Total loss: 1.635. Value loss: 0.448. Policy accuracy: 0.745
2025-02-01 16:26:07,712 (Training) Epoch: 5. Total loss: 1.634. Value loss: 0.447. Policy accuracy: 0.746
2025-02-01 16:29:41,324 (Training) Epoch: 6. Total loss: 1.631. Value loss: 0.445. Policy accuracy: 0.747
2025-02-01 16:33:18,829 (Training) Epoch: 7. Total loss: 1.626. Value loss: 0.442. Policy accuracy: 0.747
2025-02-01 16:36:53,176 (Training) Epoch: 8. Total loss: 1.624. Value loss: 0.441. Policy accuracy: 0.747
2025-02-01 16:40:19,820 (Training) Epoch: 9. Total loss: 1.620. Value loss: 0.438. Policy accuracy: 0.748
2025-02-01 16:43:45,097 (Training) Epoch: 10. Total loss: 1.615. Value loss: 0.436. Policy accuracy: 0.749
2025-02-01 16:53:25,854 (Evaluation) Win rate: 0.1
2025-02-01 16:53:25,854 (Evaluation) Model 1 wins: 22. Draws: 23. Model 2 wins: 5
2025-02-01 16:53:26,752 (Evaluation) Rejecting new model...
2025-02-01 16:55:39,771 (Experiment) Win rate (random): 1.0
2025-02-01 17:00:49,622 (Experiment) Win rate (alpha-beta pruning with depth 5): 0.98
2025-02-01 17:00:49,623 

2025-02-01 17:00:49,637 Iteration 73
2025-02-01 17:43:48,460 (Self-play) Number of new training examples: 14604
2025-02-01 17:43:48,460 (Self-play) Number of total training examples: 960468
2025-02-01 17:43:48,460 (Self-play) Number of total unique state-reward pairs: 668901
2025-02-01 17:43:48,460 (Self-play) Model 1 wins: 106. Draws: 27. Model 2 wins: 117
2025-02-01 17:50:02,651 (Training) Epoch: 1. Total loss: 1.663. Value loss: 0.470. Policy accuracy: 0.741
2025-02-01 17:53:36,002 (Training) Epoch: 2. Total loss: 1.648. Value loss: 0.459. Policy accuracy: 0.744
2025-02-01 17:57:04,980 (Training) Epoch: 3. Total loss: 1.642. Value loss: 0.454. Policy accuracy: 0.745
2025-02-01 18:00:33,887 (Training) Epoch: 4. Total loss: 1.636. Value loss: 0.450. Policy accuracy: 0.746
2025-02-01 18:04:03,217 (Training) Epoch: 5. Total loss: 1.633. Value loss: 0.447. Policy accuracy: 0.746
2025-02-01 18:07:30,735 (Training) Epoch: 6. Total loss: 1.626. Value loss: 0.444. Policy accuracy: 0.748
2025-02-01 18:10:59,179 (Training) Epoch: 7. Total loss: 1.625. Value loss: 0.442. Policy accuracy: 0.748
2025-02-01 18:14:40,308 (Training) Epoch: 8. Total loss: 1.621. Value loss: 0.440. Policy accuracy: 0.748
2025-02-01 18:18:20,993 (Training) Epoch: 9. Total loss: 1.619. Value loss: 0.439. Policy accuracy: 0.748
2025-02-01 18:22:01,969 (Training) Epoch: 10. Total loss: 1.616. Value loss: 0.437. Policy accuracy: 0.749
2025-02-01 18:32:10,880 (Evaluation) Win rate: 0.32
2025-02-01 18:32:10,880 (Evaluation) Model 1 wins: 16. Draws: 18. Model 2 wins: 16
2025-02-01 18:32:31,554 (Evaluation) Rejecting new model...
2025-02-01 18:34:32,913 (Experiment) Win rate (random): 1.0
2025-02-01 18:39:40,196 (Experiment) Win rate (alpha-beta pruning with depth 5): 1.0
2025-02-01 18:39:40,197 

2025-02-01 18:39:40,548 Iteration 74
2025-02-01 19:23:36,004 (Self-play) Number of new training examples: 13818
2025-02-01 19:23:36,004 (Self-play) Number of total training examples: 974286
2025-02-01 19:23:36,004 (Self-play) Number of total unique state-reward pairs: 678056
2025-02-01 19:23:36,005 (Self-play) Model 1 wins: 129. Draws: 27. Model 2 wins: 94
2025-02-01 19:30:11,429 (Training) Epoch: 1. Total loss: 1.664. Value loss: 0.473. Policy accuracy: 0.743
2025-02-01 19:33:43,717 (Training) Epoch: 2. Total loss: 1.648. Value loss: 0.460. Policy accuracy: 0.745
2025-02-01 19:37:16,302 (Training) Epoch: 3. Total loss: 1.644. Value loss: 0.455. Policy accuracy: 0.744
2025-02-01 19:40:49,955 (Training) Epoch: 4. Total loss: 1.635. Value loss: 0.450. Policy accuracy: 0.746
2025-02-01 19:44:22,609 (Training) Epoch: 5. Total loss: 1.632. Value loss: 0.447. Policy accuracy: 0.747
2025-02-01 19:47:54,388 (Training) Epoch: 6. Total loss: 1.629. Value loss: 0.445. Policy accuracy: 0.748
2025-02-01 19:51:34,008 (Training) Epoch: 7. Total loss: 1.624. Value loss: 0.442. Policy accuracy: 0.748
2025-02-01 19:55:11,840 (Training) Epoch: 8. Total loss: 1.622. Value loss: 0.440. Policy accuracy: 0.748
2025-02-01 19:58:50,617 (Training) Epoch: 9. Total loss: 1.618. Value loss: 0.439. Policy accuracy: 0.749
2025-02-01 20:02:30,195 (Training) Epoch: 10. Total loss: 1.616. Value loss: 0.437. Policy accuracy: 0.750
2025-02-01 20:12:04,546 (Evaluation) Win rate: 0.54
2025-02-01 20:12:04,546 (Evaluation) Model 1 wins: 8. Draws: 15. Model 2 wins: 27
2025-02-01 20:12:06,049 (Evaluation) Rejecting new model...
2025-02-01 20:13:54,943 (Experiment) Win rate (random): 1.0
2025-02-01 20:18:57,503 (Experiment) Win rate (alpha-beta pruning with depth 5): 0.98
2025-02-01 20:18:57,503 

2025-02-01 20:18:57,552 Iteration 75
2025-02-01 21:01:03,139 (Self-play) Number of new training examples: 14726
2025-02-01 21:01:03,139 (Self-play) Number of total training examples: 989012
2025-02-01 21:01:03,139 (Self-play) Number of total unique state-reward pairs: 688125
2025-02-01 21:01:03,139 (Self-play) Model 1 wins: 121. Draws: 21. Model 2 wins: 108
2025-02-01 21:07:45,218 (Training) Epoch: 1. Total loss: 1.665. Value loss: 0.474. Policy accuracy: 0.742
2025-02-01 21:11:26,011 (Training) Epoch: 2. Total loss: 1.650. Value loss: 0.462. Policy accuracy: 0.743
2025-02-01 21:15:07,528 (Training) Epoch: 3. Total loss: 1.643. Value loss: 0.456. Policy accuracy: 0.745
